<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.2.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.33/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"echoxc.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":300,"display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/./public/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Pytorch神经网络基础 自动求导">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning 学习笔记">
<meta property="og:url" content="https://echoxc.github.io/2024/05/08/Deep-Learning/index.html">
<meta property="og:site_name" content="LemonTree的小站">
<meta property="og:description" content="Pytorch神经网络基础 自动求导">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://echoxc.github.io/2024/05/08/Deep-Learning/Data-flow.png">
<meta property="og:image" content="https://echoxc.github.io/2024/05/08/Deep-Learning/Backward-propagation-example.png">
<meta property="og:image" content="https://echoxc.github.io/2024/05/08/Deep-Learning/learning-rate-effect.png">
<meta property="og:image" content="https://echoxc.github.io/2024/05/08/Deep-Learning/convolution-picture1.png">
<meta property="og:image" content="https://echoxc.github.io/2024/05/08/Deep-Learning/convolution-picture2.png">
<meta property="og:image" content="https://echoxc.github.io/2024/05/08/Deep-Learning/convolution-picture3.png">
<meta property="og:image" content="https://echoxc.github.io/2024/05/08/Deep-Learning/convolution-picture4.png">
<meta property="og:image" content="https://echoxc.github.io/2024/05/08/Deep-Learning/convolution-picture5.png">
<meta property="og:image" content="https://echoxc.github.io/2024/05/08/Deep-Learning/convolution-picture6.png">
<meta property="og:image" content="https://echoxc.github.io/2024/05/08/Deep-Learning/convolution-picture7.png">
<meta property="og:image" content="https://echoxc.github.io/2024/05/08/Deep-Learning/convolution-picture8.png">
<meta property="og:image" content="https://echoxc.github.io/2024/05/08/Deep-Learning/convolution-picture9.png">
<meta property="og:image" content="https://echoxc.github.io/2024/05/08/Deep-Learning/2D_convolution.png">
<meta property="og:image" content="https://echoxc.github.io/2024/05/08/Deep-Learning/convolution-fill.png">
<meta property="og:image" content="https://echoxc.github.io/2024/05/08/Deep-Learning/cat.png">
<meta property="og:image" content="https://echoxc.github.io/2024/05/08/Deep-Learning/LeNet.png">
<meta property="og:image" content="https://echoxc.github.io/2024/05/08/Deep-Learning/1-1-convolution.png">
<meta property="og:image" content="https://echoxc.github.io/2024/05/08/Deep-Learning/NiN-block.png">
<meta property="og:image" content="https://echoxc.github.io/2024/05/08/Deep-Learning/NiN-Networks.png">
<meta property="article:published_time" content="2024-05-08T12:34:37.000Z">
<meta property="article:modified_time" content="2024-05-30T13:49:13.909Z">
<meta property="article:author" content="Lemon Tree">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://echoxc.github.io/2024/05/08/Deep-Learning/Data-flow.png">


<link rel="canonical" href="https://echoxc.github.io/2024/05/08/Deep-Learning/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://echoxc.github.io/2024/05/08/Deep-Learning/","path":"2024/05/08/Deep-Learning/","title":"Deep Learning 学习笔记"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Deep Learning 学习笔记 | LemonTree的小站</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">LemonTree的小站</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">没有星星的夜里 我用泪光吸引你</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于我</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">8</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">5</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">9</span></a></li><li class="menu-item menu-item-guestbook"><a href="/guestbook/" rel="section"><i class="fa fa-book fa-fw"></i>留言板</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#pytorch%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80"><span class="nav-number">1.</span> <span class="nav-text">Pytorch神经网络基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC"><span class="nav-number">1.1.</span> <span class="nav-text">自动求导</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%ADforward-propagation"><span class="nav-number">1.1.1.</span> <span class="nav-text">正向传播（Forward
Propagation）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADbackward-propagation"><span class="nav-number">1.1.2.</span> <span class="nav-text">反向传播（Backward
Propagation）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%84%E9%80%A0"><span class="nav-number">1.2.</span> <span class="nav-text">模型构造</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%82%E5%92%8C%E5%9D%97"><span class="nav-number">1.2.1.</span> <span class="nav-text">层和块</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E7%AE%A1%E7%90%86"><span class="nav-number">1.3.</span> <span class="nav-text">参数管理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">1.4.</span> <span class="nav-text">初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E8%88%AC%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95"><span class="nav-number">1.4.1.</span> <span class="nav-text">一般初始化方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E6%9F%90%E4%BA%9B%E5%9D%97%E5%BA%94%E7%94%A8%E4%B8%8D%E5%90%8C%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95"><span class="nav-number">1.4.2.</span> <span class="nav-text">对某些块应用不同的初始化方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">1.4.3.</span> <span class="nav-text">自定义初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9B%B4%E6%9A%B4%E5%8A%9B%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">1.4.4.</span> <span class="nav-text">更暴力的方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E7%BB%91%E5%AE%9A"><span class="nav-number">1.4.5.</span> <span class="nav-text">参数绑定</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82"><span class="nav-number">1.5.</span> <span class="nav-text">自定义层</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%84%E9%80%A0%E4%B8%80%E4%B8%AA%E6%B2%A1%E6%9C%89%E4%BB%BB%E4%BD%95%E5%8F%82%E6%95%B0%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82"><span class="nav-number">1.5.1.</span> <span class="nav-text">构造一个没有任何参数的自定义层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B8%A6%E5%8F%82%E6%95%B0%E7%9A%84%E5%B1%82"><span class="nav-number">1.5.2.</span> <span class="nav-text">自定义带参数的层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.5.3.</span> <span class="nav-text">使用自定义层构建模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6"><span class="nav-number">1.6.</span> <span class="nav-text">读写文件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BF%9D%E5%AD%98%E5%BC%A0%E9%87%8F"><span class="nav-number">1.6.1.</span> <span class="nav-text">加载和保存张量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="nav-number">1.6.2.</span> <span class="nav-text">加载和保存模型参数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E4%BF%9D%E5%AD%98"><span class="nav-number">1.6.2.1.</span> <span class="nav-text">模型参数保存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E8%AF%BB%E5%8F%96"><span class="nav-number">1.6.2.2.</span> <span class="nav-text">模型参数读取</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%AD%A5%E9%AA%A4"><span class="nav-number">1.7.</span> <span class="nav-text">模型训练步骤</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%BA%93%E5%AF%BC%E5%85%A5"><span class="nav-number">1.7.1.</span> <span class="nav-text">基本库导入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dataset%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E5%BB%BA"><span class="nav-number">1.7.2.</span> <span class="nav-text">Dataset数据集构建</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E9%BB%98%E8%AE%A4dataset%E5%BD%A2%E5%BC%8F"><span class="nav-number">1.7.2.1.</span> <span class="nav-text">使用默认Dataset形式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89dataset%E5%BD%A2%E5%BC%8F"><span class="nav-number">1.7.2.2.</span> <span class="nav-text">使用自定义Dataset形式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dataset%E6%95%B0%E6%8D%AE%E6%9F%A5%E7%9C%8B"><span class="nav-number">1.7.2.3.</span> <span class="nav-text">Dataset数据查看</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">1.7.3.</span> <span class="nav-text">数据预处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%92%E5%88%86%E8%AE%AD%E7%BB%83%E9%9B%86%E6%B5%8B%E8%AF%95%E9%9B%86%E9%AA%8C%E8%AF%81%E9%9B%86"><span class="nav-number">1.7.4.</span> <span class="nav-text">划分训练集、测试集、验证集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dataloader%E6%9E%84%E5%BB%BA"><span class="nav-number">1.7.5.</span> <span class="nav-text">DataLoader构建</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#dataset%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BD%AC%E6%8D%A2%E4%B8%BAdataloader"><span class="nav-number">1.7.5.1.</span> <span class="nav-text">Dataset数据集转换为DataLoader</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dataloader%E8%AE%BF%E9%97%AE%E6%96%B9%E5%BC%8F"><span class="nav-number">1.7.5.2.</span> <span class="nav-text">DataLoader访问方式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="nav-number">1.7.6.</span> <span class="nav-text">模型训练</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%8F%82%E6%95%B0%E5%AE%9A%E4%B9%89"><span class="nav-number">1.7.6.1.</span> <span class="nav-text">基本参数定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-number">1.7.6.2.</span> <span class="nav-text">训练过程</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82"><span class="nav-number">1.8.</span> <span class="nav-text">模型调参</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="nav-number">1.8.1.</span> <span class="nav-text">学习率</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#gpu%E9%83%A8%E7%BD%B2"><span class="nav-number">2.</span> <span class="nav-text">GPU部署</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#gpu-%E5%8F%AF%E7%94%A8%E6%80%A7%E6%A3%80%E6%9F%A5"><span class="nav-number">2.1.</span> <span class="nav-text">GPU 可用性检查</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8C%87%E5%AE%9Agpu%E8%AE%BE%E5%A4%87"><span class="nav-number">2.2.</span> <span class="nav-text">指定GPU设备</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#gpu%E4%B8%8A%E7%9A%84%E5%BC%A0%E9%87%8F%E8%BF%90%E7%AE%97"><span class="nav-number">2.2.1.</span> <span class="nav-text">GPU上的张量运算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8Egpu"><span class="nav-number">2.2.2.</span> <span class="nav-text">神经网络与GPU</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#standard-examples"><span class="nav-number">3.</span> <span class="nav-text">Standard Examples</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#dl-model-based-on-gpu"><span class="nav-number">3.1.</span> <span class="nav-text">DL Model based on GPU</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8Bgpu%E8%AE%BE%E5%A4%87%E4%BF%A1%E6%81%AF%E5%B9%B6%E6%8C%87%E5%AE%9A%E6%89%80%E7%94%A8gpu"><span class="nav-number">3.1.1.</span> <span class="nav-text">查看GPU设备信息并指定所用GPU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="nav-number">3.1.2.</span> <span class="nav-text">数据加载</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%A0%87%E5%87%86%E5%8C%96"><span class="nav-number">3.1.3.</span> <span class="nav-text">数标准化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%84%E5%BB%BAdataset%E5%92%8Cdataloader"><span class="nav-number">3.1.4.</span> <span class="nav-text">构建Dataset和Dataloader</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%BF%9B%E8%A1%8C%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">3.1.5.</span> <span class="nav-text">构建模型并进行初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%A8gpu%E4%B8%8A%E8%BF%9B%E8%A1%8C%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="nav-number">3.1.6.</span> <span class="nav-text">在GPU上进行模型训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%A2%84%E6%B5%8B"><span class="nav-number">3.1.7.</span> <span class="nav-text">模型评估与预测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="nav-number">3.1.8.</span> <span class="nav-text">数据可视化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#high-dimensional-linear-regression"><span class="nav-number">3.2.</span> <span class="nav-text">High Dimensional Linear
Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E4%BA%A7%E7%94%9F"><span class="nav-number">3.2.1.</span> <span class="nav-text">数据产生</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="nav-number">3.2.2.</span> <span class="nav-text">回归模型训练</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E7%B1%BB%E5%9E%8B"><span class="nav-number">4.</span> <span class="nav-text">数据集类型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#dataframe%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">4.1.</span> <span class="nav-text">DataFrame数据集</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#cnn-convolutional-neural-network"><span class="nav-number">5.</span> <span class="nav-text">CNN (Convolutional Neural
Network)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%8D%B7%E7%A7%AF"><span class="nav-number">5.1.</span> <span class="nav-text">什么是卷积？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%8E%E8%9D%B4%E8%9D%B6%E6%95%88%E5%BA%94%E8%AF%B4%E8%B5%B7"><span class="nav-number">5.1.1.</span> <span class="nav-text">从蝴蝶效应说起</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%8D%B7%E7%A7%AF-%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AB%E5%8D%B7%E7%A7%AF"><span class="nav-number">5.1.2.</span> <span class="nav-text">卷积、卷积 为什么叫“卷积”？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%9B%BE%E5%83%8F%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C"><span class="nav-number">5.1.3.</span> <span class="nav-text">什么是图像的卷积操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%8D%B7%E7%A7%AF"><span class="nav-number">5.1.4.</span> <span class="nav-text">卷积神经网络与卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="nav-number">5.1.5.</span> <span class="nav-text">参考链接</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%8D%E8%B0%88%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E4%B8%8E%E5%8D%B7%E7%A7%AF"><span class="nav-number">5.2.</span> <span class="nav-text">再谈全连接层与卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%B3%E7%A7%BB%E4%B8%8D%E5%8F%98%E6%80%A7"><span class="nav-number">5.2.1.</span> <span class="nav-text">平移不变性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82-vs-%E5%8D%B7%E7%A7%AF"><span class="nav-number">5.2.2.</span> <span class="nav-text">全连接层 VS 卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%80%E9%83%A8%E6%80%A7"><span class="nav-number">5.2.3.</span> <span class="nav-text">局部性</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-number">5.3.</span> <span class="nav-text">卷积层</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E7%BB%B4%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-number">5.3.1.</span> <span class="nav-text">二维卷积层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%9B%B8%E5%85%B3-vs-%E5%8D%B7%E7%A7%AF"><span class="nav-number">5.3.2.</span> <span class="nav-text">交叉相关 VS 卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E5%A1%AB%E5%85%85padding%E4%B8%8E%E6%AD%A5%E5%B9%85stride"><span class="nav-number">5.3.3.</span> <span class="nav-text">卷积层的填充padding与步幅stride</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A1%AB%E5%85%85"><span class="nav-number">5.3.3.1.</span> <span class="nav-text">填充</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A5%E5%B9%85"><span class="nav-number">5.3.3.2.</span> <span class="nav-text">步幅</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5%E5%92%8C%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="nav-number">5.3.4.</span> <span class="nav-text">多输入和多输出通道</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93"><span class="nav-number">5.3.4.1.</span> <span class="nav-text">多输入通道</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="nav-number">5.3.4.2.</span> <span class="nav-text">多输出通道</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E5%A4%9A%E8%BE%93%E5%85%A5%E5%92%8C%E5%A4%9A%E8%BE%93%E5%87%BA"><span class="nav-number">5.3.4.3.</span> <span class="nav-text">为什么使用多输入和多输出</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#times-1-%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-number">5.3.4.4.</span> <span class="nav-text">\(1 \times 1\)
卷积层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5%E5%92%8C%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93%E6%80%BB%E7%BB%93"><span class="nav-number">5.3.4.5.</span> <span class="nav-text">多输入和多输出通道总结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82-pooling"><span class="nav-number">5.4.</span> <span class="nav-text">池化层 Pooling</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A1%AB%E5%85%85%E6%AD%A5%E5%B9%85%E5%92%8C%E5%A4%9A%E9%80%9A%E9%81%93"><span class="nav-number">5.4.1.</span> <span class="nav-text">填充、步幅和多通道</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="nav-number">5.4.2.</span> <span class="nav-text">最大池化层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%B3%E5%9D%87%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="nav-number">5.4.3.</span> <span class="nav-text">平均池化层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lenet-%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">5.5.</span> <span class="nav-text">LeNet 经典卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#lenet-pytorch%E5%AE%9E%E7%8E%B0"><span class="nav-number">5.5.1.</span> <span class="nav-text">LeNet Pytorch实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#vgg%E5%9D%97"><span class="nav-number">5.6.</span> <span class="nav-text">VGG块</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nin%E7%BD%91%E7%BB%9C"><span class="nav-number">5.7.</span> <span class="nav-text">NiN网络</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Lemon Tree"
      src="/images/picture3.jpg">
  <p class="site-author-name" itemprop="name">Lemon Tree</p>
  <div class="site-description" itemprop="description">最好的自己，永远留给台下的你！</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
        <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=280 height=110 src="//music.163.com/outchain/player?type=0&id=2844914148&auto=1&height=90"></iframe>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://echoxc.github.io/2024/05/08/Deep-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/picture3.jpg">
      <meta itemprop="name" content="Lemon Tree">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LemonTree的小站">
      <meta itemprop="description" content="最好的自己，永远留给台下的你！">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Deep Learning 学习笔记 | LemonTree的小站">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Deep Learning 学习笔记
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">

  
  <div style="padding-left: 8px;">
    <span class="post-meta-divider" style="padding-right: 8px;">|</span>
    <i class="fa fa-thumb-tack" style="color: #EB6D39"></i>
    <font color=EB6D39>置顶</font>
  </div>
  
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-05-08 20:34:37" itemprop="dateCreated datePublished" datetime="2024-05-08T20:34:37+08:00">2024-05-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-05-30 21:49:13" itemprop="dateModified" datetime="2024-05-30T21:49:13+08:00">2024-05-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Math/" itemprop="url" rel="index"><span itemprop="name">Math</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep-Learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>31k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>56 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="pytorch神经网络基础">Pytorch神经网络基础</h1>
<h2 id="自动求导">自动求导</h2>
<span id="more"></span>
<p><img src="Data-flow.png" width="50%" height="50%" title="数据流" alt="错误无法显示"/></p>
<p>在如图所示的数据流下，要计算<span
class="math inline">\(y\)</span>关于<span
class="math inline">\(x\)</span>的梯度，可以采样两种方法</p>
<h3 id="正向传播forward-propagation">正向传播（Forward
Propagation）</h3>
<p>梯度计算方向和数据流方向相同：<span class="math inline">\(\frac{ dy }
{ dx } = \frac{ {dy} } { {db} } \left( {\frac{ {db} } { {da} }\left(
{\frac{ {da} } { {dx} } } \right)}
\right)\)</span>，称为正向传播模式</p>
<h3 id="反向传播backward-propagation">反向传播（Backward
Propagation）</h3>
<p>梯度计算方向和数据流方向相反：<span class="math inline">\(\frac{ dy }
{ dx } = \frac{ {da} } { {dx} } \left( {\frac{ {db} } { {da} }\left(
{\frac{ {dy} } { {db} } } \right)}
\right)\)</span>，称为反向传播模式</p>
<p>Back
Propagation算法是多层神经网络的训练中举足轻重的算法。简单的理解，它的确就是复合函数的链式法则，但其在实际运算中的意义比链式法则要大的多。</p>
<p><img src="Backward-propagation-example.png" width="50%" height="50%" title="反向传播示例" alt="错误无法显示"/></p>
<p>以上图为例子，我们相求<span class="math inline">\(e\)</span>关于<span
class="math inline">\(a\)</span>和<span
class="math inline">\(b\)</span>的导数，那么我们有<span
class="math inline">\(\frac{ {de} }{ {da} } = \frac{ {de} }{ {dc}
}\frac{ {dc} }{ {da} }\)</span> 和 <span class="math inline">\(\frac{
{de} }{ {db} } = \frac{ {de} }{ {dc} }\frac{ {dc} }{ {db} } + \frac{
{de} }{ {dd} }\frac{ {dd} }{ {db} }\)</span></p>
<p>如果采用Forward
Propagation，我们会发现这样做是十分冗余的，因为很多路径被重复访问了。比如图中的a-c-e和b-c-e就都走了路径c-e。对于权值动则数万的深度模型中的神经网络，这样的冗余所导致的计算量是相当大的。</p>
<p>Backward
Propagation算法则机智地避开了这种冗余，它对于每一个路径只访问一次就能求顶点对所有下层节点的偏导值。正如反向传播(BP)算法的名字说的那样，BP算法是反向(自上往下)来寻找路径的，从最上层的节点e开始，初始值为1，以层为单位进行处理。对于e的下一层的所有子节点，将1乘以e到某个节点路径上的偏导值，并将结果“堆放”在该子节点中。等e所在的层按照这样传播完毕后，第二层的每一个节点都“堆放"些值，然后我们针对每个节点，把它里面所有“堆放”的值求和，就得到了顶点e对该节点的偏导。然后将这些第二层的节点各自作为起始顶点，初始值设为顶点e对它们的偏导值，以"层"为单位重复上述传播过程，即可求出顶点e对每一层节点的偏导数。</p>
<p>而神经网络正是需要对每一层求梯度，因此BP算法恰好契合了神经网络的需要。</p>
<h2 id="模型构造">模型构造</h2>
<h3 id="层和块">层和块</h3>
<p>Pytoch中Module是一个很重要的概念，Module可以认为是任何一个层和一个神经网络它都属于Module的一个子类.在PyTorch中，nn.Module类的子类可以像函数一样被调用，这是因为在nn.Module的实现中，__call__方法被重写了，允许你像调用函数一样调用它们。当你调用一个继承自nn.Module的类的实例时，PyTorch会自动调用forward方法，这个方法定义了这个模型的前向传播逻辑。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">X = torch.rand(<span class="number">2</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        self.out = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># nn.Relu()是构造了一个ReLU对象，并不是函数调用，而F.ReLU()是函数调用</span></span><br><span class="line">        <span class="keyword">return</span> self.out(F.relu(self.hidden(X)))</span><br><span class="line"></span><br><span class="line">net1 = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line">net2 = MLP()</span><br><span class="line"><span class="built_in">print</span>(net1(X))</span><br><span class="line"><span class="built_in">print</span>(net2(X))</span><br></pre></td></tr></table></figure>
<p>这个例子中我们通过自定义继承nn.Module这个类来实现了特定功能的函数，我们可以通过继承nn.Module这个类可以比Sequential去更灵活的去定义我们的参数是什么样子以及如何做前向计算。</p>
<p>比如一个混合搭配各种混合块的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NestMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                                 nn.Linear(<span class="number">64</span>, <span class="number">32</span>), nn.ReLU())</span><br><span class="line">        self.linear = nn.Linear(<span class="number">32</span>, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(self.net(X))</span><br><span class="line"></span><br><span class="line">chimera = nn.Sequential(NestMLP(), nn.Linear(<span class="number">16</span>, <span class="number">20</span>))</span><br><span class="line"><span class="built_in">print</span>(chimera(X))</span><br></pre></td></tr></table></figure>
<p>因此通过这种方法我们可以进行更加灵活的定义。</p>
<h2 id="参数管理">参数管理</h2>
<ul>
<li>访问某一层的参数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">X = torch.rand(size=(<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].state_dict())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net[<span class="number">2</span>].bias))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.data) <span class="comment"># 访问对应的值</span></span><br><span class="line"><span class="built_in">print</span>(net.state_dict()[<span class="string">&#x27;2.bias&#x27;</span>].data)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.grad) <span class="comment"># 这里还没有做反向计算，所以梯度为None</span></span><br></pre></td></tr></table></figure>
<ul>
<li>一次性访问所有参数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters()]) <span class="comment"># *[]序列解释包，将这个列表解包成单独的元组</span></span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()])</span><br></pre></td></tr></table></figure>
<h2 id="初始化">初始化</h2>
<p>初始化的目的是让模型在一开始的时候使得每一层的输入输出大小在一个尺度上面，不要然它出现越往后面越大或者越往后面越小的情况，使模型爆掉了。只要初始化开始时不出问题，不同的初始化方法对精度的影响其实差不多。</p>
<h3 id="一般初始化方法">一般初始化方法</h3>
<p>遍历所有Module进行初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_normal</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line"></span><br><span class="line">net.apply(init_normal)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.data, net[<span class="number">0</span>].bias.data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_constant</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line"></span><br><span class="line">net.apply(init_constant)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.data, net[<span class="number">0</span>].bias.data)</span><br></pre></td></tr></table></figure>
<h3 id="对某些块应用不同的初始化方法">对某些块应用不同的初始化方法</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">xavier</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_42</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">net[<span class="number">0</span>].apply(xavier)</span><br><span class="line">net[<span class="number">2</span>].apply(init_42)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.data[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data)</span><br></pre></td></tr></table></figure>
<h3 id="自定义初始化">自定义初始化</h3>
<p>也可以自定义初始化方法，如初始化保留绝对值大于等于5的权重</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_init</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">&quot;Init&quot;</span>,</span><br><span class="line">            *[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> m.named_parameters()][<span class="number">0</span>]</span><br><span class="line">        )</span><br><span class="line">        nn.init.uniform_(m.weight, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        m.weight.data *= m.weight.data.<span class="built_in">abs</span>() &gt;= <span class="number">5</span></span><br><span class="line"></span><br><span class="line">net.apply(my_init)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight[:<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<h3 id="更暴力的方法">更暴力的方法</h3>
<p>还可以直接把值拿出来做替换，如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.data[:] += <span class="number">1</span></span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] += <span class="number">42</span></span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<h3 id="参数绑定">参数绑定</h3>
<p>当有一些layer想要sharing某些weight时，可以进行参数绑定，也就是在构建net时其指向同一个类，这是不用网络直接共享权重的一个方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">shared = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), shared, nn.ReLU(), shared, nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line">net[<span class="number">2</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<h2 id="自定义层">自定义层</h2>
<p>本质上讲，自定义层和自定义网络没什么本质区别，因层也是nn.Module的一个子类</p>
<h3
id="构造一个没有任何参数的自定义层">构造一个没有任何参数的自定义层</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CenteredLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> X - X.mean()</span><br><span class="line"></span><br><span class="line">layer = CenteredLayer()</span><br><span class="line"><span class="built_in">print</span>(layer(torch.FloatTensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])))</span><br></pre></td></tr></table></figure>
<p>进而可以将层作为组件合并到构建更复杂的模型中 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">8</span>, <span class="number">128</span>), CenteredLayer())</span><br><span class="line">Y = net(torch.rand(<span class="number">4</span>, <span class="number">8</span>))</span><br><span class="line"><span class="built_in">print</span>(Y)</span><br></pre></td></tr></table></figure></p>
<h3 id="自定义带参数的层">自定义带参数的层</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_units, units</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(in_units, units))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(units, ))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        linear = torch.matmul(X, self.weight.data) + self.bias.data</span><br><span class="line">        <span class="keyword">return</span> F.relu(linear)</span><br><span class="line"></span><br><span class="line">dense = MyLinear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(dense.weight)</span><br><span class="line"><span class="built_in">print</span>(torch.rand(<span class="number">2</span>, <span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<h3 id="使用自定义层构建模型">使用自定义层构建模型</h3>
<p>同样可以使用自定义层构建模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(MyLinear(<span class="number">64</span>, <span class="number">8</span>), MyLinear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(net(torch.rand(<span class="number">2</span>, <span class="number">64</span>)))</span><br></pre></td></tr></table></figure>
<h2 id="读写文件">读写文件</h2>
<h3 id="加载和保存张量">加载和保存张量</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 储存一个张量列表，然后把它们读回内存</span></span><br><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line">y = torch.zeros(<span class="number">4</span>)</span><br><span class="line">torch.save([x, y], <span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line">x2, y2 = torch.load(<span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>((x2, y2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入或读取从字符串映射到张量的字典</span></span><br><span class="line">mydict = &#123;<span class="string">&#x27;x&#x27;</span>: x, <span class="string">&#x27;y&#x27;</span>: y&#125;</span><br><span class="line">torch.save(mydict, <span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line">mydict2 = torch.load(<span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(mydict2)</span><br></pre></td></tr></table></figure>
<h3 id="加载和保存模型参数">加载和保存模型参数</h3>
<p>我们可以通过state_dict()来得到所有的Parameter中字符串到Parameter值的一个映射，并将其保存下来实例化一个模型的备份。</p>
<h4 id="模型参数保存">模型参数保存</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.output(F.relu(self.hidden(x)))</span><br><span class="line"></span><br><span class="line">net = MLP()</span><br><span class="line">X = torch.randn(size=(<span class="number">2</span>, <span class="number">20</span>))</span><br><span class="line">Y = net(X)</span><br><span class="line">torch.save(net.state_dict(), <span class="string">&#x27;mlp.params&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="模型参数读取">模型参数读取</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">clone = MLP()</span><br><span class="line">clone.load_state_dict(torch.load(<span class="string">&#x27;mlp.params&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(clone.<span class="built_in">eval</span>())</span><br><span class="line">Y_clone = clone(X)</span><br><span class="line"><span class="built_in">print</span>(Y_clone == Y)</span><br></pre></td></tr></table></figure>
<h2 id="模型训练步骤">模型训练步骤</h2>
<h3 id="基本库导入">基本库导入</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset, Dataset, DataLoader</span><br></pre></td></tr></table></figure>
<h3 id="dataset数据集构建">Dataset数据集构建</h3>
<p>在拿到一个张量数据后，首先要将其整理成Dataset的形式，首先要划分输入数据(features)和输出数据(labels)，然后将其整理为Dataset的形式，Dataset本身不提供数据的批处理或迭代功能。</p>
<h4 id="使用默认dataset形式">使用默认Dataset形式</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset = TensorDataset(features, labels)</span><br></pre></td></tr></table></figure>
<h4 id="使用自定义dataset形式">使用自定义Dataset形式</h4>
<p>除了使用内置的数据集，我们也可以自定义数据集。自定义数据集需要继承Dataset类，并实现__len__和__getitem__两个方法。在实际应用中，self.data
可以是任何类型的数据结构，只要能够按照索引获取样本即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, labels</span>):</span><br><span class="line">        self.features = features</span><br><span class="line">        self.labels = labels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.features)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># 返回一个包含特征和标签的元组</span></span><br><span class="line">        <span class="keyword">return</span> self.features[idx], self.labels[idx]</span><br></pre></td></tr></table></figure>
<h4 id="dataset数据查看">Dataset数据查看</h4>
<p>Dataset的访问方法为按照样本的索引访问单个样本，常用的操作为</p>
<ul>
<li><p>访问第<span
class="math inline">\(i\)</span>个样本的features和labels<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset[i]</span><br></pre></td></tr></table></figure></p></li>
<li><p>访问第<span
class="math inline">\(i\)</span>个样本的features或labels<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataset[i][<span class="number">0</span>]</span><br><span class="line">dataset[i][<span class="number">1</span>]</span><br></pre></td></tr></table></figure></p></li>
<li><p>访问Dataset前<span
class="math inline">\(m\)</span>个样本的的features和labels<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataset):</span><br><span class="line">    <span class="keyword">if</span> i &gt;= m:  <span class="comment"># 如果已经打印了100个样本，跳出循环</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Sample <span class="subst">&#123;i&#125;</span>: <span class="subst">&#123;data&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></p></li>
</ul>
<h3 id="数据预处理">数据预处理</h3>
<p>自定义数据集类通常还需要进行数据预处理，例如归一化、编码、格式化等，以确保数据适合模型训练。可通过sklearn中的方法对数据进行标准化处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line"><span class="comment"># fit_transform按数据特征列进行标准化</span></span><br><span class="line">data_normal = scaler.fit_transform(data)</span><br><span class="line"><span class="comment"># transform按数据特征列进行标准化</span></span><br><span class="line">test_data_normal = scaler.transform(testdata)</span><br><span class="line"><span class="comment"># inverse_transform逆标准化还原数据</span></span><br><span class="line">data_row = scaler.inverse_transform(data_normal)</span><br></pre></td></tr></table></figure>
<h3 id="划分训练集测试集验证集">划分训练集、测试集、验证集</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_size = <span class="built_in">int</span>(<span class="number">0.8</span> * <span class="built_in">len</span>(dataset))</span><br><span class="line">val_size = <span class="built_in">int</span>(<span class="number">0.1</span> * <span class="built_in">len</span>(dataset))</span><br><span class="line">test_size = <span class="built_in">len</span>(dataset) - train_size - val_size</span><br><span class="line">train_dataset, val_dataset, test_dataset = \</span><br><span class="line">    torch.utils.data.random_split(dataset, [train_size, val_size, test_size])</span><br></pre></td></tr></table></figure>
<h3 id="dataloader构建">DataLoader构建</h3>
<p>DataLoader提供了一种便捷的方式来以批次的形式访问数据，它在内部实现了数据的迭代，可以按批次返回数据，同时还可以进行数据打乱和多线程数据加载。</p>
<h4
id="dataset数据集转换为dataloader">Dataset数据集转换为DataLoader</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>DataLoader 中，shuffle 参数指定了是否在每个
epoch（数据加载周期）开始时对数据集进行打乱。具体来说，当 shuffle=True
时，整个数据集的顺序会被随机打乱，然后再划分为批次。</p>
<h4 id="dataloader访问方式">DataLoader访问方式</h4>
<ul>
<li><p>迭代访问，DataLoader
本身是一个迭代器，可以直接在它上面进行迭代，以按批次获取数据。每次迭代返回的是一个数据批次，通常是一个包含特征和标签的元组。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> batch_index, (features, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">    <span class="comment"># 在这里使用 features 和 labels 进行模型训练或评估</span></span><br><span class="line"><span class="keyword">for</span> features, labels <span class="keyword">in</span> data_loader:</span><br></pre></td></tr></table></figure></p></li>
<li><p>按索引访问<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">single_batch = <span class="built_in">next</span>(<span class="built_in">iter</span>(data_loader)) </span><br></pre></td></tr></table></figure></p></li>
<li><p>使用 len() 函数，可以使用内置的 len() 函数来获取 DataLoader
中的批次总数。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">num_batches = <span class="built_in">len</span>(data_loader)</span><br></pre></td></tr></table></figure></p></li>
<li><p>结合 Subset
使用，当需要从一个完整的数据集中选择一个子集进行训练或验证时，可以使用
Subset 随机选择或指定一系列索引。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Subset</span><br><span class="line">indices = [...]  <span class="comment"># 指定的索引列表</span></span><br><span class="line">subset = Subset(full_dataset, indices)</span><br><span class="line">data_loader = DataLoader(subset, ...)</span><br></pre></td></tr></table></figure></p></li>
</ul>
<h3 id="模型训练">模型训练</h3>
<p>在每个迭代周期里，我们将完整遍历一次数据集（train_data），不停地从中获取一个小批量的输入和相应的标签。对于每⼀个小批量，我们会进⾏以下步骤:</p>
<ul>
<li>通过调用net(X)生成预测并计算损失Loss（前向传播）。</li>
<li>通过进行反向传播来计算梯度。</li>
<li>通过调用优化器来更新模型参数。</li>
</ul>
<h4 id="基本参数定义">基本参数定义</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">10</span>,  <span class="number">1e-2</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line"><span class="comment"># nn.MSELoss()默认关键字reduction=&quot;mean&quot;，求均方误差，返回一个标量</span></span><br><span class="line"><span class="comment"># reduction=&quot;none&quot;：求所有对应位置的差的平方，返回的仍然是一个和原来维度一样的tensor。</span></span><br><span class="line"><span class="comment"># reduction=&quot;sum&quot;：求所有对应位置差的平方的和，返回的是一个标量。</span></span><br></pre></td></tr></table></figure>
<h4 id="训练过程">训练过程</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_loader:</span><br><span class="line">        l = loss(net(X), y)</span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step()</span><br><span class="line">    <span class="comment"># 打印训练信息</span></span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span> Loss: <span class="subst">&#123;l.item()&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="模型调参">模型调参</h2>
<h3 id="学习率">学习率</h3>
<p>学习率决定了在每步参数更新中，模型参数有多大程度（或多快、多大步长）的调整。学习率是一个超参数。不同学习率的影响可以用下图表示</p>
<p><img src="learning-rate-effect.png" width="50%" height="50%" title="学习率影响" alt="错误无法显示"/></p>
<p>学习率还会跟优化过程的其他方面相互作用，这个相互作用可能是非线性的。小的batch
size最好搭配小的学习率，因为batch
size越小也可能有噪音，这时候就需要小心翼翼地调整参数。</p>
<h1 id="gpu部署">GPU部署</h1>
<h2 id="gpu-可用性检查">GPU 可用性检查</h2>
<ul>
<li><p>shell中查看GPU使用率 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure></p></li>
<li><p>检查GPU是否可用 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br></pre></td></tr></table></figure></p></li>
</ul>
<h2 id="指定gpu设备">指定GPU设备</h2>
<p>深度学习的所有框架都是默认在CPU上做运算的，使用GPU的话需要先指定GPU</p>
<h3 id="gpu上的张量运算">GPU上的张量运算</h3>
<ul>
<li><p>查看张量所在的设备 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(x.device)</span><br></pre></td></tr></table></figure></p></li>
<li><p>数据在GPU上储存<br />
可以采用两种方法将数据储存在GPU上 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = x.cuda()</span><br><span class="line"><span class="built_in">print</span>(x.device)</span><br><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(X.device)</span><br><span class="line"><span class="built_in">print</span>(x+X)</span><br></pre></td></tr></table></figure>
值得注意的是在GPU上做运算必须要求数据在同一个设备(GPU)上</p></li>
</ul>
<h3 id="神经网络与gpu">神经网络与GPU</h3>
<p>同样可以将NN部署在GPU上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">net = net.to(device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net(X))</span><br><span class="line"><span class="comment"># 确认模型参数储存在同一个GPU上</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.data.device)</span><br></pre></td></tr></table></figure>
<h1 id="standard-examples">Standard Examples</h1>
<h2 id="dl-model-based-on-gpu">DL Model based on GPU</h2>
<h3 id="查看gpu设备信息并指定所用gpu">查看GPU设备信息并指定所用GPU</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset, Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> mypackage <span class="keyword">import</span> mydl</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">check_gpu_info</span>():</span><br><span class="line">    <span class="string">&#x27;显示 GPU 版本和内存信息&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> torch.cuda.is_available():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;No GPU found. CPU will be used.&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用 nvidia-smi 命令获取 GPU 版本信息</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        result = subprocess.run([<span class="string">&#x27;nvidia-smi&#x27;</span>], capture_output=<span class="literal">True</span>, text=<span class="literal">True</span>, check=<span class="literal">True</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;GPU Version Info:&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(result.stdout)</span><br><span class="line">    <span class="keyword">except</span> subprocess.CalledProcessError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Failed to run nvidia-smi command:&quot;</span>, e)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用 PyTorch API 获取 GPU 内存信息</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nGPU Memory Info:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;GPU Total Numbers: <span class="subst">&#123;torch.cuda.device_count()&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(torch.cuda.device_count()):</span><br><span class="line">        properties = torch.cuda.get_device_properties(i)</span><br><span class="line">        total_memory_gb = properties.total_memory / (<span class="number">1024</span> ** <span class="number">3</span>)</span><br><span class="line">        used_memory_gb = torch.cuda.memory_allocated(i) / (<span class="number">1024</span> ** <span class="number">3</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;GPU <span class="subst">&#123;i&#125;</span>:&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;\tName: <span class="subst">&#123;properties.name&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;\tTotal Memory: <span class="subst">&#123;total_memory_gb:<span class="number">.2</span>f&#125;</span> GB&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;\tUsed Memory: <span class="subst">&#123;used_memory_gb:<span class="number">.2</span>f&#125;</span> GB&quot;</span>)</span><br><span class="line">check_gpu_info()</span><br><span class="line">device = torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;<span class="number">0</span>&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="数据加载">数据加载</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                            Data Preprocessing</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line">filename = <span class="string">rf&#x27;.\Dataset\Traindata(r-u_to_k).xlsx&#x27;</span></span><br><span class="line">data = pd.read_excel(filename, sheet_name=<span class="string">&quot;Sheet1&quot;</span>, header=<span class="literal">None</span>)</span><br><span class="line">data = torch.tensor(data[<span class="number">1</span>:].values.astype(np.float32))</span><br><span class="line">data = data[:, :<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<h3 id="数标准化">数标准化</h3>
<p>数据标准化是为了使得模型更快收敛，通常用sklearn中的方法进行数据标准化，sklearn处理的对象是numpy数组，因此在使用前要注意将数据类型转换为numpy</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                            Data Standardization</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">data = torch.tensor(scaler.fit_transform(data.numpy()), dtype=torch.float32)</span><br></pre></td></tr></table></figure>
<h3 id="构建dataset和dataloader">构建Dataset和Dataloader</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#               Building corresponding Dataset and Dataloader</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment"># Building Dataset</span></span><br><span class="line">dataset_st = TensorDataset(data_st[:, <span class="number">0</span>:<span class="number">2</span>], data_st[:, [<span class="number">2</span>]])</span><br><span class="line">train_size = <span class="built_in">int</span>(<span class="number">0.8</span> * <span class="built_in">len</span>(dataset_st))</span><br><span class="line">val_size = <span class="built_in">int</span>(<span class="number">0.1</span> * <span class="built_in">len</span>(dataset_st))</span><br><span class="line">test_size = <span class="built_in">len</span>(dataset_st) - train_size - val_size</span><br><span class="line">train_dataset_st, val_dataset_st, test_dataset_st = torch.utils.data.random_split(dataset_st, [train_size, val_size, test_size])</span><br><span class="line"><span class="comment"># Building Dataloader</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">train_loader = DataLoader(train_dataset_st, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">val_loader = DataLoader(val_dataset_st, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset_st, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="构建模型并进行初始化">构建模型并进行初始化</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                      Building Model and Inilization</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">2</span>, <span class="number">128</span>), nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">128</span>, <span class="number">32</span>), nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">32</span>, <span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">xavier</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line">net.apply(xavier)</span><br></pre></td></tr></table></figure>
<h3 id="在gpu上进行模型训练">在GPU上进行模型训练</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                            Model Training</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line">num_epochs, lr = <span class="number">50</span>, <span class="number">1e-2</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line">net.to(device=device)</span><br><span class="line">net.train()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line">        l = loss(net(X), y)</span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span> Loss: <span class="subst">&#123;l.item()&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="模型评估与预测">模型评估与预测</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                            Model Evaluation</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line">net.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    y_pred = net(test_dataset_st[:][<span class="number">0</span>].cuda())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pred_test_data_st = torch.cat([test_dataset_st[:][<span class="number">0</span>], y_pred.cpu()], dim=<span class="number">1</span>)</span><br><span class="line">test_data_st = torch.cat([test_dataset_st[:][<span class="number">0</span>], test_dataset_st[:][<span class="number">1</span>]], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="数据可视化">数据可视化</h3>
<p>matplotlib函数基于numpy数组进行处理，当有tensor会将自动转换为numpy格式，但是他只能处理cpu的数据，因此模型预测的数据结果需要移动到cpu上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                            Figure Plotting</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line">Number = np.arange(<span class="built_in">len</span>(y_pred)) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">1</span>, figsize=(<span class="number">7</span>, <span class="number">5</span>))</span><br><span class="line">ax.scatter(Number,test_data_st[:, <span class="number">2</span>], label=<span class="string">&quot;Ture data&quot;</span>, c=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">ax.scatter(Number, pred_test_data_st[:, <span class="number">2</span>], label=<span class="string">&quot;DL predictions&quot;</span>, c=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&quot;u235(Standarization)&quot;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&quot;keff(Standarization)&quot;</span>)</span><br><span class="line">ax.legend()</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">1</span>, figsize=(<span class="number">7</span>, <span class="number">5</span>))</span><br><span class="line">ax.scatter(Number, scaler.inverse_transform(test_data_st.numpy())[:, <span class="number">2</span>], label=<span class="string">&quot;Ture data&quot;</span>, c=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">ax.scatter(Number, scaler.inverse_transform(pred_test_data_st.numpy())[:, <span class="number">2</span>], label=<span class="string">&quot;DL predictions&quot;</span>, c=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&quot;u235&quot;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&quot;keff&quot;</span>)</span><br><span class="line">ax.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="high-dimensional-linear-regression">High Dimensional Linear
Regression</h2>
<h3 id="数据产生">数据产生</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机种子以获得可重复的结果</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">num_inputs = <span class="number">3</span></span><br><span class="line">num_samples = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成输入数据，这里我们假设每个输入变量的范围是0到10</span></span><br><span class="line">X1 = np.random.uniform(low=<span class="number">0</span>, high=<span class="number">4</span>, size=num_samples)</span><br><span class="line">X2 = np.random.uniform(low=<span class="number">0</span>, high=<span class="number">4</span>, size=num_samples)</span><br><span class="line">X3 = np.random.uniform(low=<span class="number">0</span>, high=<span class="number">4</span>, size=num_samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成噪声项，这里我们假设噪声项是正态分布的</span></span><br><span class="line">epsilon = np.random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=num_samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算输出变量Y，包含非线性项</span></span><br><span class="line">Y = X1**<span class="number">2</span> + <span class="number">5</span>*np.sin(X2) - X3**<span class="number">3</span> + epsilon</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个DataFrame</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;X1&#x27;</span>: X1,</span><br><span class="line">    <span class="string">&#x27;X2&#x27;</span>: X2,</span><br><span class="line">    <span class="string">&#x27;X3&#x27;</span>: X3,</span><br><span class="line">    <span class="string">&#x27;Y&#x27;</span>: Y</span><br><span class="line">&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line">df.to_csv(<span class="string">&#x27;./Dataset/Traindata(r1-r3-u_to_k).csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h3 id="回归模型训练">回归模型训练</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset, Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> mypackage <span class="keyword">import</span> mydl</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                      Building Model and Inilization</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">3</span>, <span class="number">128</span>), nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">128</span>, <span class="number">32</span>), nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">32</span>, <span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line">mydl.init_cnn(net)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                            Data Preprocessing</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line">filename = <span class="string">rf&#x27;./Dataset/Traindata(r1-r3-u_to_k).csv&#x27;</span></span><br><span class="line">data = mydl.read_data(filename, use_header=<span class="number">0</span>)</span><br><span class="line">data= torch.tensor(data.to_numpy(dtype=np.float32))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                            Data Standardization</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line">normalizer = mydl.Normalizer()</span><br><span class="line">normalizer.fit(data)</span><br><span class="line">data_st = normalizer.transform(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#               Building corresponding Dataset and Dataloader</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment"># Dataset Building</span></span><br><span class="line">dataset_st = TensorDataset(data_st[:, <span class="number">0</span>:<span class="number">3</span>], data_st[:, [<span class="number">3</span>]])</span><br><span class="line">train_size = <span class="built_in">int</span>(<span class="number">0.8</span> * <span class="built_in">len</span>(dataset_st))</span><br><span class="line">val_size = <span class="built_in">int</span>(<span class="number">0.1</span> * <span class="built_in">len</span>(dataset_st))</span><br><span class="line">test_size = <span class="built_in">len</span>(dataset_st) - train_size - val_size</span><br><span class="line">train_dataset_st, val_dataset_st, test_dataset_st = \</span><br><span class="line">    torch.utils.data.random_split(dataset_st, [train_size, val_size, test_size])</span><br><span class="line"><span class="comment"># Dataloader Building</span></span><br><span class="line">batch_size = <span class="number">200</span></span><br><span class="line">train_loader = DataLoader(train_dataset_st, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">val_loader = DataLoader(val_dataset_st, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset_st, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                            Model Training</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line">device = mydl.try_gpu()</span><br><span class="line">num_epochs, lr = <span class="number">300</span>, <span class="number">1e-3</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line">net.to(device=device)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line"></span><br><span class="line">Epoch, train_losses, val_losses= [], [], []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    net.train()</span><br><span class="line">    <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line">        l = loss(net(X), y)</span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        train_loss = mydl.evaluate_loss(net, train_loader, loss)</span><br><span class="line">        val_loss = mydl.evaluate_loss(net, test_loader, loss)</span><br><span class="line">        Epoch.append(epoch + <span class="number">1</span>)</span><br><span class="line">        train_losses.append(train_loss)</span><br><span class="line">        val_losses.append(val_loss)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span> Loss: <span class="subst">&#123;l.item()&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    test_inputs = test_dataset_st[:][<span class="number">0</span>].to(device)</span><br><span class="line">    test_targets = test_dataset_st[:][<span class="number">1</span>].to(device)</span><br><span class="line">    test_outputs = net(test_inputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                            Data Anti-normalization</span></span><br><span class="line"><span class="comment">############################################################################### </span></span><br><span class="line">test_targets = test_targets * normalizer.std[<span class="number">3</span>] + normalizer.mean[<span class="number">3</span>]</span><br><span class="line">test_outputs = test_outputs * normalizer.std[<span class="number">3</span>] + normalizer.mean[<span class="number">3</span>]</span><br><span class="line"><span class="built_in">print</span>(normalizer.std, normalizer.mean)</span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                               Figure</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment"># Loss in every Eopch</span></span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">1</span>, figsize=(<span class="number">6</span>, <span class="number">5</span>))</span><br><span class="line">ax.plot(Epoch, train_losses, label=<span class="string">&#x27;Train Loss&#x27;</span>)</span><br><span class="line">ax.plot(Epoch, val_losses, label=<span class="string">&#x27;Validation Loss&#x27;</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;MSE Loss&#x27;</span>)</span><br><span class="line">ax.set_title(<span class="string">&#x27;Loss Curves&#x27;</span>)</span><br><span class="line">ax.set_yscale(<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">ax.legend(framealpha=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prediction Accuracy</span></span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">1</span>, figsize=(<span class="number">6</span>, <span class="number">5</span>))</span><br><span class="line">ax.scatter(test_targets.to(mydl.cpu()), test_outputs.to(mydl.cpu()), s=<span class="number">5</span>, c=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">m = torch.<span class="built_in">max</span>(torch.<span class="built_in">abs</span>(test_targets)).to(mydl.cpu())</span><br><span class="line">ax.plot([-m, m], [-m, m])</span><br><span class="line">ax.set_xlim([-m, m])</span><br><span class="line">ax.set_ylim([-m, m])</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;Target Values&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;Prediction Values&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="数据集类型">数据集类型</h1>
<h2 id="dataframe数据集">DataFrame数据集</h2>
<p>pandas 的 read_excel 函数用于读取Excel文件，并将数据加载到一个
DataFrame 对象中。DataFrame 对象本身不会在数据中显示行号和列号，但它们是
DataFrame 的一部分，打印时会看到对应的行号和列号。</p>
<h1 id="cnn-convolutional-neural-network">CNN (Convolutional Neural
Network)</h1>
<p>适合于计算机视觉的神经⽹络架构基于两个原则：</p>
<ol type="1">
<li><p>平移不变性（translation
invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层
应该对相同的图像区域具有相似的反应。</p></li>
<li><p>局部性（locality）：神经网络的前面几层应该只探索输⼊图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</p></li>
</ol>
<h2 id="什么是卷积">什么是卷积？</h2>
<h3 id="从蝴蝶效应说起">从蝴蝶效应说起</h3>
<div class="group-picture"><div class="group-picture-row"><div class="group-picture-column"><img src="convolution-picture1.png" width="40%" title="卷积说明1" alt="错误无法显示"  /></div><div class="group-picture-column"><img src="convolution-picture2.png" width="40%" title="卷积说明2" alt="错误无法显示" /></div></div></div>
<p>为了更好的理解卷积，考虑这样一个例子，有一只蝴蝶不停地在扇动翅膀，不同时刻扇动翅膀的快慢不同，因此其产生的破坏效果也不同，其破坏效果的影响力我们用左图来描述，并且某时刻产生的破坏效果不会马上消失，而是随时间逐渐衰减，其衰减效果如右图所示。</p>
<p>现在解决一个问题，求一下<span
class="math inline">\(t\)</span>时刻感受到的破坏力？这个问题也很简单，把之前所有时刻对<span
class="math inline">\(t\)</span>时刻的影响加起来就行了，本质上也就是求这个式子:</p>
<p><span class="math display">\[
\int_0^t {f\left( x \right)g\left( {t - x} \right)dx}
\]</span></p>
<p>这也就是我们后面要说的卷积。</p>
<h3 id="卷积卷积-为什么叫卷积">卷积、卷积 为什么叫“卷积”？</h3>
<p>我们给出所谓的卷积的定义，也就是这个式子 <span
class="math display">\[
\int_{ - \infty }^\infty {f\left( \tau \right)g\left( {x - \tau}
\right)d\tau}
\]</span></p>
<p>从左边图可以看出，图中每一条连线都对应着一对<span
class="math inline">\(f(x)\)</span>和<span
class="math inline">\(g(t-x)\)</span>的相乘，把所有的值加起来，就得到了我们所谓的卷积。</p>
<div class="group-picture"><div class="group-picture-row"><div class="group-picture-column"><img src="convolution-picture3.png" width="40%" title="卷积说明3" alt="错误无法显示"  /></div><div class="group-picture-column"><img src="convolution-picture4.png" width="40%" title="卷积说明4" alt="错误无法显示" /></div></div></div>
<p>此时如果我们将<span
class="math inline">\(g(t)\)</span>函数翻转一下，会发现卷积实际上就是将函数翻转后对应位置相乘求和，这也就是为什么叫卷积。</p>
<h3 id="什么是图像的卷积操作">什么是图像的卷积操作</h3>
<p>如果我们把视野放得更广一点，在上面蝴蝶效应的例子中，如果影响力的变化不是随时间改变，而是随着空间距离而改变的，也就是说对<span
class="math inline">\(x\)</span>位置产生影响的是其他很多位置，那么回到开始的问题，什么是图像的卷积操作？图像的卷积操作实际上就是去看图像上其他很多像素点对一个像素点是如何产生影响的，举个例子</p>
<p><img src="convolution-picture5.png" width="90%" title="平滑卷积核操作" alt="错误无法显示" /></p>
<p>可以看到这个例子中，卷积核规定了周围的像素点对当前像素点的影响，当前在经过一个与平滑卷积核进行卷积操作后，对图像进行了平滑，也就是说在这个卷积核下考虑周围像素点对某个像素点影响，遍历整个图片后，得到的结果是每个像素点更平滑。</p>
<div class="group-picture"><div class="group-picture-row"><div class="group-picture-column"><img src="convolution-picture6.png" width="40%" title="卷积说明6" alt="错误无法显示" /></div><div class="group-picture-column"><img src="convolution-picture7.png" width="40%" title="卷积说明7" alt="错误无法显示" /></div><div class="group-picture-column"><img src="convolution-picture8.png" width="40%" title="卷积说明8" alt="错误无法显示" /></div></div></div>
<p>我们现在考虑<span
class="math inline">\(g(m,n)\)</span>这个卷积核下，<span
class="math inline">\((x,y)\)</span>周围的像素点对<span
class="math inline">\((x,y)\)</span>这个像素点的影响效果，根据卷积的定义，可以得到</p>
<p><span class="math display">\[
\begin{array}{l}
\begin{aligned}
f\left( {x,y} \right)g\left( {m,n} \right) &amp;= \sum {f\left( {x,y}
\right)g\left( {m - x,n - y} \right)} \\
&amp; = f\left( {x - 1,y - 1} \right)g\left( {1,1} \right) + f\left(
{x,y - 1} \right)g\left( {0,1} \right) + f\left( {x + 1,y - 1}
\right)g\left( { - 1,1} \right)\\
&amp; + f\left( {x - 1,y} \right)g\left( {1,0} \right) + f\left( {x,y}
\right)g\left( {0,0} \right) + f\left( {x + 1,y} \right)g\left( { - 1,0}
\right)\\
&amp; + f\left( {x - 1,y + 1} \right)g\left( {1, - 1} \right) + f\left(
{x,y + 1} \right)g\left( {0, - 1} \right) + f\left( {x + 1,y + 1}
\right)g\left( { - 1, - 1} \right)
\end{aligned}
\end{array}
\]</span></p>
<p><img src="convolution-picture9.png" width="90%" title="卷积说明8" alt="错误无法显示" /></p>
<p>同样我们发现它仍然是卷着乘的，我们我们将它翻转<span
class="math inline">\(180^\circ\)</span>后会发现是对应位置相乘，实际上后来我们CNN中用的卷积核就是翻转后的结果，它可以直接扣在图像上直接相乘再相加，但它本质上仍然是一个卷积运算。</p>
<h3 id="卷积神经网络与卷积">卷积神经网络与卷积</h3>
<p>卷积神经网络主要是用来干图像识别的。</p>
<h3 id="参考链接">参考链接</h3>
<iframe src="//player.bilibili.com/player.html?aid=418492547&amp;bvid=BV1VV411478E&amp;cid=353587154&amp;p=1&amp;autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true">
</iframe>
<h2 id="再谈全连接层与卷积">再谈全连接层与卷积</h2>
<p>在之前处理图片时，我们将一张图片reshape成一个1D向量来处理，现在我们考虑它的空间信息，对于一个图片它都包含一部分空间信息，因此我们选择用矩阵(宽度,
高度)去描述神经网络的输入<span
class="math inline">\(x\)</span>和输出<span
class="math inline">\(h\)</span>，对应地我们可以将我们的权重变为4D张量，此时有对应的变换关系</p>
<p><span class="math display">\[
{ h_{ij} } = \sum\limits_{k,l} { {w_{ijkl} } {x_{kl} } }
\]</span></p>
<p>接下来我们对<span
class="math inline">\(w\)</span>做一个重新的索引，使得<span
class="math inline">\({v_{ijab} } = {w_{ij(i + a)(j + b)}
}\)</span>，此时有</p>
<p><span class="math display">\[
\begin{equation}
{ h_{ij} } = \sum\limits_{k,l} { {w_{ijkl} } {x_{kl} } }  =
\sum\limits_{a,b} { {v_{ijab} } {x_{(i + a)(j + b)} } }
\label{convolution}
\end{equation}
\]</span></p>
<p>这个式子可以看成<span class="math inline">\((i,
j)\)</span>位置的输出<span class="math inline">\(h\)</span>是由<span
class="math inline">\((i, j)\)</span>位置的周边<span
class="math inline">\((i+a, j+b)\)</span>的一些输入<span
class="math inline">\(x\)</span>在权重<span class="math inline">\({\bf
v}\)</span>下所共同影响而得到的。下面我们根据我们的基本原则引出我们的卷积：</p>
<h3 id="平移不变性">平移不变性</h3>
<p>在方程<span
class="math inline">\(\eqref{convolution}\)</span>中，权重<span
class="math inline">\({\bf
v}\)</span>本质上就是一组识别器，而这时如果<span
class="math inline">\((i, j)\)</span>发生变化(对应平移)，这时权重<span
class="math inline">\(v_{ijab}\)</span>也会发生变化，使得输出结果做出对应的改变，而根据平移不变性的要求，不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应。所以我们并不希望发生平移时（也就是改变<span
class="math inline">\(i, j\)</span>时）输出也随<span
class="math inline">\(i, j\)</span>的变化而改变，因此我们说<span
class="math inline">\({\bf v}\)</span>并不该依赖于<span
class="math inline">\((i, j)\)</span>，进而我们有<span
class="math inline">\(v_{ijab}=v_{ab}\)</span>，故而得到：</p>
<p><span class="math display">\[
{h_{ij} } = \sum\limits_{a,b} { {v_{ab} } {x_{(i + a)(j + b)} } }
\]</span></p>
<p>这就是所谓的二维交叉相关。</p>
<h3 id="全连接层-vs-卷积">全连接层 VS 卷积</h3>
<ul>
<li><p>全连接层最大的问题在于全连接层第一层的权重参数矩阵的行取决于输入的维度，特别是在处理图像问题的时候，如果把每个像素点作为一个维度，这个模型会非常大，容易爆掉。</p></li>
<li><p>相比于全连接层，卷积的优势在于不管它的输入是多大，卷积核的大小总是固定的，这样就极大地降低了模型复杂度。</p></li>
</ul>
<h3 id="局部性">局部性</h3>
<p>局部性的意思是说当我们评估<span
class="math inline">\(h_{ij}\)</span>时，我们不应该使用远离<span
class="math inline">\(x_{ij}\)</span>的参数，因此我们选择当<span
class="math inline">\(|a|, |b| &gt; \Delta\)</span>时，使<span
class="math inline">\(v_{ab}=0\)</span>，此时有： <span
class="math display">\[
{h_{ij} } = \sum\limits_{a =  - \Delta }^\Delta  {\sum\limits_{b =  -
\Delta }^\Delta  { {v_{ab} } {x_{(i + a)(j + b)} } } }
\]</span></p>
<p>因此我们对全连接层使用平移不变性和局部性就得到了我们的卷积层，换句话说就是卷积是一个特殊的全连接层。</p>
<h2 id="卷积层">卷积层</h2>
<p>卷积层本质是将输入和核矩阵进行交叉相关，加上偏移后得到输出，核矩阵和偏移都是可学习的参数，核矩阵的大小是超参数。</p>
<h3 id="二维卷积层">二维卷积层</h3>
<p><img src="2D_convolution.png" width="50%" height="50%" title="二维卷积层示例" alt="错误无法显示"/></p>
<p>在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。当卷积窗口滑动到新一个位置时，包含在该窗口中的部分张量与卷积核张量进行按元素相乘，得到的张量再求和得到⼀个单⼀的标量值，由此我们得出了这⼀位置的输出张量值。</p>
<p>卷积核的宽度和高度大于1，而卷积核只与图像中每个大小完全适合的位置进行互相关运算，这一过程的数学表述可以表示为</p>
<p><span class="math display">\[
{\bf Y} = {\bf X} \star {\bf W} + b
\]</span></p>
<ul>
<li>输入<span class="math inline">\({\bf X}\)</span>: <span
class="math inline">\(n_{h} \times n_{w}\)</span></li>
<li>卷积核<span class="math inline">\({\bf W}\)</span>: <span
class="math inline">\(k_{h} \times k_{w}\)</span></li>
<li>偏差: <span class="math inline">\(b \in \mathbb{R}\)</span></li>
<li>输出<span class="math inline">\({\bf Y}\)</span>: <span
class="math inline">\((n_{h} - k_{h} + 1) \times (n_{w} - k_{w} +
1)\)</span></li>
</ul>
<p>其中<span class="math inline">\(\star\)</span>表示交叉相关运算，<span
class="math inline">\({\bf W}\)</span>和<span
class="math inline">\(b\)</span>是可学习的参数。</p>
<h3 id="交叉相关-vs-卷积">交叉相关 VS 卷积</h3>
<ul>
<li>二维交叉相关</li>
</ul>
<p><span class="math display">\[
{h_{ij} } = \sum\limits_{a,b} { {v_{ab} } {x_{(i + a)(j + b)} } }
\]</span></p>
<ul>
<li>二维卷积</li>
</ul>
<p><span class="math display">\[
{h_{ij} } = \sum\limits_{a,b} { {v_{-a, -b} } {x_{(i + a)(j + b)} } }
\]</span></p>
<p>它们唯一的区别是卷积在索引<span
class="math inline">\(w\)</span>的时候是翻转的，相当于先翻转<span
class="math inline">\(180^\circ\)</span>再做交叉相关操作，这也是为什么称之为“卷积”。由于对称性的存在，在实际使用过程中它们没有任何区别，用二维交叉学出来的东西翻转过来就是用二维卷积学出来的东西。</p>
<h3
id="卷积层的填充padding与步幅stride">卷积层的填充padding与步幅stride</h3>
<p>填充和步幅是卷积层的超参数，填充是在输入周围添加额外的行和列，来控制输出形状的减少量，步幅是每次滑动核窗口时的步长，可以成倍的减少输出形状。</p>
<h4 id="填充">填充</h4>
<p>经过一层卷积操作后，<span class="math inline">\(n_{h} \times
n_{w}\)</span>的输入减小为为<span class="math inline">\((n_{h} - k_{h} +
1) \times (n_{w} - k_{w} +
1)\)</span>的输出，如果想做比较深的神经网络，我们就需要对其进行填充</p>
<p><img src="convolution-fill.png" width="40%" height="50%" title="填充操作" alt="错误无法显示"/></p>
<p>通过在输入的四周添加额外的行和和列，可以使得输出形状保持不变</p>
<ul>
<li>填充<span class="math inline">\(p_h\)</span>行和<span
class="math inline">\(p_w\)</span>列，输出形状为<span
class="math inline">\((n_{h} - k_{h} + p_{h} + 1) \times (n_{w} - k_{w}
+p_{w} + 1)\)</span></li>
<li>为了保持形状不变，通常取<span class="math inline">\(p_{h} = k_{h} -
1\)</span>, <span class="math inline">\(p_{w} = k_{w} - 1\)</span>
<ul>
<li>当<span
class="math inline">\(k_{h}\)</span>为奇数时：在上下两侧填充<span
class="math inline">\(p_{h} / 2\)</span></li>
<li>当<span
class="math inline">\(k_{h}\)</span>为偶数时：在上侧填充<span
class="math inline">\(\lceil p_{h} / 2 \rceil\)</span>, 在下侧填充<span
class="math inline">\(\lfloor p_{h} / 2 \rfloor\)</span></li>
</ul></li>
</ul>
<h4 id="步幅">步幅</h4>
<p>当输入一个比较大的图片时，可以通过调整步幅的大小来减小输出</p>
<ul>
<li>给定高度<span class="math inline">\(s_h\)</span>和宽度<span
class="math inline">\(s_w\)</span>的步幅，输出形状为<span
class="math inline">\(\lfloor (n_{h} - k_{h} + p_{h}) / s_{h} + 1
\rfloor \times \lfloor (n_{w} - k_{w} +p_{w}) / s_{h} + 1
\rfloor\)</span></li>
<li>如果<span class="math inline">\(p_{h} = k_{h} - 1\)</span>, <span
class="math inline">\(p_{w} = k_{w} - 1\)</span>，则输出形状为<span
class="math inline">\(\lfloor (n_{h} - 1) / s_{h} + 1 \rfloor \times
\lfloor (n_{w} - 1) / s_{h} + 1 \rfloor\)</span></li>
<li>如果输入高度<span class="math inline">\(n_{h}\)</span>和宽度<span
class="math inline">\(n_{w}\)</span>可以被步幅整除，则输出形状为<span
class="math inline">\((n_{h}/s_{h}) \times (n_{w}/s_{w})\)</span></li>
</ul>
<h3 id="多输入和多输出通道">多输入和多输出通道</h3>
<p>对于彩色图像来讲可能有RGB三个通道，如果直接将其转换为灰度则会丢失信息。</p>
<h4 id="多输入通道">多输入通道</h4>
<ul>
<li>输入<span class="math inline">\({\bf X}\)</span>: <span
class="math inline">\({c_i} \times {n_h} \times {n_w}\)</span></li>
<li>核<span class="math inline">\({\bf W}\)</span>: <span
class="math inline">\({c_i} \times {k_h} \times {k_w}\)</span></li>
<li>输出<span class="math inline">\({\bf Y}\)</span>: <span
class="math inline">\({m_h} \times {m_w}\)</span></li>
</ul>
<p><span class="math display">\[
{\bf Y} = \sum\limits_{i = 0}^{ {c_i} } { { {\bf X}_{i,:,:} } \star {
{\bf W}_{i,:,:} } }
\]</span></p>
<p>多输入通道每个通道都有一个卷积核，结果是所有通道卷积结果的和。</p>
<h4 id="多输出通道">多输出通道</h4>
<ul>
<li>输入<span class="math inline">\({\bf X}\)</span>: <span
class="math inline">\({c_i} \times {n_h} \times {n_w}\)</span></li>
<li>核<span class="math inline">\({\bf W}\)</span>: <span
class="math inline">\({c_o} \times {c_i} \times {k_h} \times
{k_w}\)</span></li>
<li>输出<span class="math inline">\({\bf Y}\)</span>: <span
class="math inline">\({c_o} \times {m_h} \times {m_w}\)</span></li>
</ul>
<p><span class="math display">\[
{\bf Y}_{i,:,:}= {\bf X} \star { {\bf W}_{i,:,:,:} } \quad {\rm for}
\quad i=1,\ldots,c_o
\]</span></p>
<p>无论有多少输入通道，到目前为止我们只用到了单输出通道，但实际上我们可以有多个三维卷积核，每个核都可以生成一个输出通道。</p>
<h4 id="为什么使用多输入和多输出">为什么使用多输入和多输出</h4>
<ul>
<li>对于每一个输出通道，它都有一个卷积核去识别特定的模式，</li>
</ul>
<p><img src="cat.png" width="80%" height="50%" title="猫图像识别" alt="错误无法显示"/></p>
<ul>
<li>输入通道核识别并组合输入中的模式<br />
当把输出通道的结果传给下一次层的输入时，下一通道会进一步进行特征提取并进行组合，得到一个组合的模式识别。</li>
</ul>
<h4 id="times-1-卷积层"><span class="math inline">\(1 \times 1\)</span>
卷积层</h4>
<p><span class="math inline">\(k_h = k_w =
1\)</span>这个卷积层是一个特殊的卷积层，它不识别空间模式，并不会提取空间信息，而只是用来融合通道。本质上它相当于输入形状为<span
class="math inline">\(n_h n_w \times c_i\)</span>的<span
class="math inline">\({\bf X}\)</span>，权重为<span
class="math inline">\(c_i \times c_o\)</span>的<span
class="math inline">\({\bf K}\)</span>的全连接层。</p>
<h4 id="多输入和多输出通道总结">多输入和多输出通道总结</h4>
<ul>
<li>输入<span class="math inline">\({\bf X}\)</span>: <span
class="math inline">\({c_i} \times {n_h} \times {n_w}\)</span></li>
<li>核<span class="math inline">\({\bf W}\)</span>: <span
class="math inline">\({c_i} \times {k_h} \times {k_w}\)</span></li>
<li>偏差<span class="math inline">\({\bf B}\)</span>: <span
class="math inline">\(c_o \times c_i\)</span></li>
<li>输出<span class="math inline">\({\bf Y}\)</span>: <span
class="math inline">\({m_h} \times {m_w}\)</span></li>
<li>计算复杂度: <span class="math inline">\(O (c_i c_o k_h k_w m_h
m_w)\)</span></li>
</ul>
<h2 id="池化层-pooling">池化层 Pooling</h2>
<p>通常当我们处理图像时，我们希望逐渐降低隐藏表示的空间分辨率、聚集信息，这样随着我们在神经网络中
层叠的上升，每个神经元对其敏感的感受野（输入）就越大。</p>
<p>而的机器学习任务通常会跟全局图像的问题有关（例如：图像是否包含⼀只猫呢？），所以我们最后⼀
层的神经元应该对整个输入的全局敏感。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表
示的目标，同时将卷积图层的所有优势保留在中间层。</p>
<p>池化层最终返回窗口中最大或平均值，它同样有窗口大小、填充和步幅作为超参数，能够缓解卷积层对位置的敏感性。</p>
<p>在Pytorch中默认步幅大小与池化窗口相同。</p>
<h3 id="填充步幅和多通道">填充、步幅和多通道</h3>
<ul>
<li>池化层和卷积层类似，都具体填充和步幅</li>
<li>没有可学习的参数，不需要学kernel</li>
<li>在每个输入通道应用池化层以获得相应的输出通道，它只是处理一下数据</li>
<li>输出通道数 = 输入通道数</li>
</ul>
<h3 id="最大池化层">最大池化层</h3>
<p>输出每个信号中最强的模式信号</p>
<h3 id="平均池化层">平均池化层</h3>
<p>将最大池化层中的“最大”操作替换为“平均”</p>
<h2 id="lenet-经典卷积神经网络">LeNet 经典卷积神经网络</h2>
<p><img src="LeNet.png" width="80%" height="50%" title="LeNet神经网络" alt="错误无法显示"/></p>
<p>LeNet是早期成功的神经网络，先使用卷积层来学习图片空间信息，然后使用全连接层转到类别空间。</p>
<h3 id="lenet-pytorch实现">LeNet Pytorch实现</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Reshape</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    </span><br><span class="line">net = torch.nn.Sequential(</span><br><span class="line">    Reshape(), nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.ReLU(), </span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), </span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.ReLU(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), nn.Flatten(), </span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.ReLU(), </span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.ReLU(), </span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;output shape: \t&#x27;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy_gpu</span>(<span class="params">net, data_iter, device=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">            X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            X = X.to(device)</span><br><span class="line">        y = y.to(device)</span><br><span class="line">        metric.add(d2l.accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">0.1</span>, <span class="number">10</span></span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>
<h2 id="vgg块">VGG块</h2>
<p>VGG块的想法是n个卷积层和1一个池化层把封装成块。</p>
<ul>
<li>VGG使用可重复使用的卷积块来构建深度卷积神经网络</li>
<li>不同卷积块的个数和超参数可以得到不同复杂程度的变种</li>
</ul>
<h2 id="nin网络">NiN网络</h2>
<p>无论是 LeNet 还是 AlexNet
网络，在卷积层输出的最后，其最后都通过一个Flatten层来展平卷积层的输出，然后再加两个全连接层进行分类预测，但实际是这个全连接层是十分占内存的。我们知道一个
<span class="math inline">\(1 \times 1\)</span>
的卷积层可以等效为一个全连接层</p>
<p><img src="1-1-convolution.png" width="80%" title="1乘1卷积" alt="错误无法显示"  />
<img src="NiN-block.png" width="30%" title="NiN架构" alt="错误无法显示" /></p>
<p>通过用 <span class="math inline">\(1 \times 1\)</span>
的卷积层代替全连接层，减少了模型大小，最终得到NiN的架构为</p>
<p><img src="NiN-Networks.png" width="80%" title="NiN Networks" alt="错误无法显示"  /></p>
<ul>
<li>无全连接层</li>
<li>交替使用NiN块和步幅为2的最大池化层逐步减小高宽
<ul>
<li>增大通道数</li>
</ul></li>
<li>最后使用全局平均池化层得到输出
<ul>
<li>其输入通道数是类别数</li>
</ul></li>
<li>NiN块使用卷积层加两个1x1卷积层
<ul>
<li>后者对每个像素增加了非线性性</li>
</ul></li>
<li>NiN使用全局平均池化层来替代VGG和AlexNet中的全连接层
<ul>
<li>不容易过拟合，更少的参数个数</li>
</ul></li>
</ul>

    </div>

    
        <div style="text-align:center;color: #ccc;font-size:14px;">
              -------------本文结束  <i class="fa fa-paw"></i>  感谢您的阅读-------------
        </div>
    

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Lemon Tree
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://echoxc.github.io/2024/05/08/Deep-Learning/" title="Deep Learning 学习笔记">https://echoxc.github.io/2024/05/08/Deep-Learning/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/05/06/Kalman-Filter/" rel="prev" title="Kalman Filter">
                  <i class="fa fa-angle-left"></i> Kalman Filter
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/06/14/Openmc/" rel="next" title="Openmc">
                  Openmc <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="lv-container" data-id="city" data-uid="MTAyMC81OTgwNS8zNjI2Nw=="></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Lemon Tree</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">73k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">2:12</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>



<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("04/29/2024 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>
    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/echoxc" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="100" alpha="0.4" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.33/fancybox/fancybox.umd.js" integrity="sha256-+2+qOqR8CKoHh/AsVR9k2qaDBKWjYNC2nozhYmv5j9k=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/fancybox.js"></script>



  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script src="/js/third-party/comments/livere.js"></script>

</body>
</html>
