<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>中子物理学</title>
    <url>/2024/04/30/Reactor-neutron-physics/</url>
    <content><![CDATA[<h1 id="中子与原子核的作用">中子与原子核的作用</h1>
<h2 id="中子分类">中子分类</h2>
<p>反应堆核物理，实际上就是堆内中子的物理学。中子是组成原子核的粒子之一，其静止质量稍大于质子的静止质量。通常在工程上取中子的静止质量为1u,
中子不带电荷。</p>
<span id="more"></span>
<p>在反应堆物理分析中，中子一般按照能量分成三类：</p>
<ul>
<li><strong>热中子</strong>：能量小于1eV</li>
<li><strong>中能中子</strong>：能量在1eV <span
class="math inline">\(\sim\)</span> 0.1MeV之间</li>
<li><strong>快中子</strong>：能量大于0.1MeV</li>
</ul>
<h2 id="中子与原子核反应">中子与原子核反应</h2>
<p>概况的讲，堆内中子与原子核的相互作用方式有：<strong>势散射</strong>、<strong>直接相互作用</strong>和<strong>复合核的形成</strong>。</p>
<ul>
<li><p><strong>势散射</strong>：
本质为中子波与核表面势相互作用，中子既未进入靶核，靶核内能也未发生改变，势散射前后系统能动量守恒，属于弹性散射。</p></li>
<li><p><strong>直接相互作用</strong>：直接相互作用是入射中子直接与靶核内某核子碰撞，使其从核里发射出来，而中子却留在了靶核内的核反应，这个过程需要高能中子才能直接与原子核发生相互作用。如果从靶核里发射出来的核子是质子，这就是直接相互作用的<span
class="math inline">\((n,
p)\)</span>反应。如果从核里发射出来的核子是中子，同时靶核由激发态返回基态放出<span
class="math inline">\(\gamma\)</span>射线，就是直接的非弹性散射过程。实际反应堆中这样的高能中子数非常少，因此堆物理分析时这种直接相互作用方式是不重要的。</p></li>
<li><p><strong>复合核的形成</strong>：复合核的形成分为两个阶段，首先中子被靶核<span
class="math inline">\(_Z ^{A}X\)</span>吸收，形成激发态的复合核<span
class="math inline">\([ _{Z}^{A+1}X^*]\)</span>,
复合核可以在激发态停留一段时间，由于核内的无规律碰撞，激发能在核子间多次交换，使某个核子得到足以逸出系统的能量时，处于激发态的复合核放出一个粒子（或者一个光子）而衰变，并留下一个余核（或反冲核）。</p>
<ul>
<li><span class="math inline">\((n, p)\)</span>：<span
class="math inline">\({}_Z^AX + {}_0^1n \to {}_{Z - 1}^AY +
{}_1^1H\)</span>，复合核放出一个质子而衰变，称为<span
class="math inline">\((n, p)\)</span>反应</li>
<li><span class="math inline">\((n, \alpha)\)</span>：<span
class="math inline">\({}_Z^AX + {}_0^1n \to {}_{Z - 2}^{A - 3}Y +
{}_2^4He\)</span>，复合核放出一个<span
class="math inline">\(\alpha\)</span>粒子而衰变，称为<span
class="math inline">\((n, \alpha)\)</span>反应</li>
<li><span class="math inline">\((n, n)\)</span>：<span
class="math inline">\({}_Z^AX + {}_0^1n \to {}_Z^AX +
{}_0^1n\)</span>，复合核放出一个中子，若余核<span
class="math inline">\(_Z
^{A}X\)</span>直接重新回到基态，则这个过程为<strong>共振弹性散射</strong>，称为<span
class="math inline">\((n, n)\)</span>反应</li>
<li><span class="math inline">\((n, n&#39;)\)</span>：<span
class="math inline">\({}_Z^AX + {}_0^1n \to {}_Z^AX + {}_0^1n +
\gamma\)</span>，复合核放出一个中子，若余核<span
class="math inline">\(_Z ^{A}X\)</span>仍处于激发态，通过放出<span
class="math inline">\(\gamma\)</span>射线回到基态，则这个过程为<strong>共振非弹性散射</strong>，称为<span
class="math inline">\((n, n&#39;)\)</span>反应</li>
<li><span class="math inline">\((n, \gamma)\)</span>：<span
class="math inline">\({}_Z^AX + {}_0^1n \to {}_Z^{A + 1}X +
\gamma\)</span>，复合核放出<span
class="math inline">\(\gamma\)</span>射线而衰变，这个过程为<strong>辐射俘获</strong>称为<span
class="math inline">\((n, \gamma)\)</span>反应</li>
<li><span class="math inline">\((n, f)\)</span>：<span
class="math inline">\({}_Z^AX + {}_0^1n \to {}_{ {Z_1} }^{ {A_1} }Y +
{}_{ {Z_2} }^{ {A_2} }W + \nu
{}_0^1n\)</span>，复合核通过分裂成两个较轻的核而衰变，这个过程为<strong>核裂变</strong>称为<span
class="math inline">\((n, f)\)</span>反应</li>
</ul></li>
</ul>
<p>综上所述，我们可以根据中子和靶核的作用结果的不同，将中子与原子核的作用分为两类：</p>
<ul>
<li><strong>散射</strong>：包括弹性散射（共振弹性散射和势散射）和非弹性散射（共振非弹性散射）</li>
<li><strong>吸收</strong>：包括辐射俘获、核裂变、<span
class="math inline">\((n, p)\)</span>和<span class="math inline">\((n,
\alpha)\)</span>反应等</li>
</ul>
<h3 id="反应堆内的中子散射">反应堆内的中子散射</h3>
<ol type="1">
<li><strong>弹性散射</strong>：分为共振弹性散射和势散射，热中子反应堆中对中子从高能慢化到低能过程起主要作用的是弹性散射</li>
<li><strong>非弹性散射</strong>：非弹性散射的发生要求入射中子的动能必须高于靶核的第一激发态能量，因此非弹性散射具有阈能的特点。</li>
</ol>
<p>对于轻核来说，他的激发态能量在MeV数量级，即使对于<span
class="math inline">\(^{235}{\rm
U}\)</span>这样的重核，中子也必须具有45keV以上的能量才能与之发生非弹性散射。在快中子反应堆中，非弹性散射过程才是重要的。对于热中子反应堆而言，裂变中子的能量在MeV的范围内，因此在高能中子区，仍然会有非弹性散射发生，但是一但中子的能量降到非弹性散射阈能以下后，便主要依靠弹性散射过程来慢化中子了</p>
<h3 id="中子的吸收">中子的吸收</h3>
<h4 id="辐射俘获">辐射俘获</h4>
<p>辐射俘获<span class="math inline">\((n,
\gamma)\)</span>可以在全能区内发生，对于低能中子而言，其与中等质量核和重核作用时易于发生这种反应。反应堆内重要的俘获反应为
<span class="math display">\[
{}_{92}^{238}{\rm U} + {}_0^1n \to {}_{92}^{239}{\rm U} + \gamma
\]</span> <span class="math inline">\({}_{92}^{238}{\rm
U}\)</span>吸收中子后变为<span class="math inline">\({}_{92}^{239}{\rm
U}\)</span>, <span class="math inline">\({} _{92}^{239}{\rm
U}\)</span>经过两次<span
class="math inline">\(\beta^{-}\)</span>衰变后可转变为<span
class="math inline">\({}^{239}{\rm Pu}\)</span>，<span
class="math inline">\({}^{239}{\rm
Pu}\)</span>是一种人工易裂变材料，在自然界里是不存在的，因此这一过程对于核燃料的转换与增值具有重要意义。</p>
<p>辐射俘获中，原先稳定的原子核俘获一个中子后往往会转变为具有放射性的原子核，因此辐射俘获会产生放射性。</p>
<h4 id="n-pn-alpha反应"><span class="math inline">\((n, p)、(n,
\alpha)\)</span>反应</h4>
<p>反应堆运行过程，堆内冷却剂和慢化剂经过<strong>高能中子</strong>照射后，会发生<span
class="math inline">\((n, p)\)</span>核反应： <span
class="math display">\[
_8^{16}{\rm O} + _0^1{\rm n} \to _7^{16}{}{\rm N} + {}_1^1{\rm H}
\]</span> <strong>热中子</strong>也会和<span
class="math inline">\(^{10}{\rm B}\)</span>发生<span
class="math inline">\((n, \alpha)\)</span>吸收反应： <span
class="math display">\[
_5^{10}{\rm B} + _0^1{\rm n} \to _3^7{\rm Li} + {}_2^4{\rm He}
\]</span> 上面的反应经常用来探测堆内热中子，并且<span
class="math inline">\(_5^{10}{\rm
B}\)</span>也广泛用作热中子反应堆的反应性控制材料。</p>
<h4 id="核裂变">核裂变</h4>
<h5 id="易裂变同位素"><strong>易裂变同位素</strong>：</h5>
<p>在各种能量中子作用下均可发生裂变，并且在<strong>低能中子</strong>作用下发生裂变法可能性较大的核素，称为易裂变同位素，常见的有：<span
class="math inline">\({}^{233}{\rm U}\)</span>、<span
class="math inline">\({}^{235}{\rm U}\)</span>、<span
class="math inline">\({}^{239}{\rm Pu}\)</span>、<span
class="math inline">\({}^{241}{\rm Pu}\)</span></p>
<h5 id="可裂变同位素"><strong>可裂变同位素</strong>：</h5>
<p>在能量高于某一阈值的中子作用下才能发生裂变的核素，称为可裂变同位素，常见的有：<span
class="math inline">\({}^{232}{\rm Th}\)</span>、<span
class="math inline">\({}^{238}{\rm U}\)</span>、<span
class="math inline">\({}^{240}{\rm Pu}\)</span></p>
<h1 id="中子截面与中子能谱">中子截面与中子能谱</h1>
<h2 id="中子能谱">中子能谱</h2>
<p>中子数关于能量的分布称为<strong>中子能谱分布</strong>，反应堆中总的中子通量密度为：
<span class="math display">\[
\Phi  = \int_0^\infty  {n\left( E \right)v\left( E \right)dE}  =
\int_0^\infty  {\phi \left( E \right)dE}
\]</span> 我们关注中子能谱分布，实际上关注的就是<span
class="math inline">\(\phi \left( E \right)\)</span>的分布</p>
<h2 id="中子截面">中子截面</h2>
<p>核反应截面的数值取决于入射中子的能量和靶核的性质，对于多数元素，反应截面随入射中子能量<span
class="math inline">\(E\)</span>的变化特性包含三个区域：</p>
<ul>
<li><strong>低能区</strong>：<span class="math inline">\(E \le
1eV\)</span></li>
<li><strong>共振区（中能区）</strong>：<span class="math inline">\(1eV
&lt; E \le {10^5}eV\)</span>，特别是<span class="math inline">\(1 \sim
{10^3}eV\)</span>区域，许多重核元素截面会出现许多共振峰。</li>
<li><strong>快中子区</strong>：<span class="math inline">\(E &gt;
{10^5}eV\)</span></li>
</ul>
<h3 id="微观吸收截面">微观吸收截面</h3>
<p>低能区核素微观吸收截面 <span
class="math inline">\(\sigma_{a}\)</span> 按照 <span
class="math inline">\(1/\sqrt{E}\)</span> 规律变化，即 <span
class="math inline">\(\sigma_{a}(E) \sqrt{E} = {\rm
Const}\)</span>，一般可以通过如下式子计算中子的微观吸收截面</p>
<p><span class="math display">\[
{\sigma _a}\left( E \right) = {\sigma _a}\left( {0.0235} \right)\sqrt
{\frac{ {0.0235} }{E} }
\]</span></p>
<p><span class="math inline">\(0.0253 {\rm eV}\)</span> 为温度为 <span
class="math inline">\(297 {\rm K}\)</span>
时的热中子能量，一般核工程手册中以此为换算基准能量。</p>
<ul>
<li><p>对于轻核来讲，由于其第一激发态能量较高，因此在中子能量在几个
<span class="math inline">\({\rm keV}\)</span> 到 <span
class="math inline">\({\rm MeV}\)</span> 区间内，其吸收截面都近似按照
<span class="math inline">\(1/v\)</span>
律变化，只有对能量比较高的中子(一般在 <span class="math inline">\({\rm
MeV}\)</span> 范围内)才会出现共振吸收，且轻核的共振峰宽而低。</p></li>
<li><p>对于重核和中等质量核，其在中能区共振吸收的影响，吸收截面会偏移
<span class="math inline">\(1/v\)</span>
律，高能区其共振峰开始重叠以致不能分辨。</p></li>
</ul>
<h3 id="微观散射截面">微观散射截面</h3>
<h4 id="非弹性散射截面-sigma_in">非弹性散射截面 <span
class="math inline">\(\sigma_{in}\)</span></h4>
<p>非弹性散射截面有阈能的特点，质量数越大，阈能越低。中子能量小于阈能时，<span
class="math inline">\(\sigma_{in} = 0\)</span></p>
<h4 id="弹性散射截面-sigma_s">弹性散射截面 <span
class="math inline">\(\sigma_{s}\)</span></h4>
<p>多数元素与较低能量中子的散射都是弹性的，<span
class="math inline">\(\sigma_{s}\)</span>
基本为<strong>常数</strong>，一般为几 <span class="math inline">\({\rm
b}\)</span> 。对于轻核、中等核，中子能量从低能区一直到 <span
class="math inline">\({\rm MeV}\)</span> 范围内，<span
class="math inline">\(\sigma_{s}\)</span> 近似为常数。</p>
<p>对于重核来讲，其在共振能区还有共振弹性散射出现。</p>
<h3 id="微观裂变截面-sigma_f">微观裂变截面 <span
class="math inline">\(\sigma_{f}\)</span></h3>
<p>对于易裂变核素的裂变截面</p>
<ul>
<li>热能区：裂变截面值很大，且随中子能量的减小儿增加</li>
<li>共振区：裂变截面出现共振峰，共振能区延伸到几个 <span
class="math inline">\({\rm keV}\)</span></li>
<li>高能区：在 <span class="math inline">\({\rm keV}\)</span> 到 <span
class="math inline">\({\rm MeV}\)</span>
能量范围内，裂变截面随中子能量的增加而下降到几<span
class="math inline">\({\rm b}\)</span></li>
</ul>
<p>根据上述对截面的分析，可以得到各截面之间的关系为</p>
<p><span class="math display">\[
\begin{array}{l}
{\sigma _s} = {\sigma _e} + {\sigma _{in}}\\
{\sigma _a} = {\sigma _\gamma } + {\sigma _f} + {\sigma _{n,\alpha }} +
{\sigma _{n,p}} +  \cdots \\
{\sigma _t} = {\sigma _s} + {\sigma _a}
\end{array}
\]</span></p>
<h1 id="热中子反应堆内中子循环">热中子反应堆内中子循环</h1>
<p>中子循环的五个过程为</p>
<ol type="1">
<li><p><span class="math inline">\(^{238} {\rm U}\)</span>
的快中子增殖：设初始一代有n个中子，在初始裂变中子中，约有 <span
class="math inline">\(60\%\)</span> 的中子能量在 <span
class="math inline">\(^{238} {\rm U}\)</span> 裂变阈能(约 $ {}$
)以上，这部分中子与 <span class="math inline">\(^{238} {\rm U}\)</span>
作用时能够引起 <span class="math inline">\(^{238} {\rm U}\)</span>
裂变而产生快中子，称为 <span class="math inline">\(^{238} {\rm
U}\)</span> 的快中子增殖效应，定义快中子增殖因数 <span
class="math inline">\(\varepsilon\)</span>
为由初始裂变中子所得到的慢化到 <span class="math inline">\(^{238} {\rm
U}\)</span> 裂变阈能以下的平均中子数，则快中子增殖使中子增长至 <span
class="math inline">\(n \varepsilon\)</span> 个。</p></li>
<li><p>慢化过程的共振吸收：由于共振区共振吸收的影响，只有一部分快中子能够慢化到热中子，记逃脱共振俘获概率为
<span class="math inline">\(p\)</span>，则共振俘获后还剩 <span
class="math inline">\(n p \varepsilon\)</span>
个慢化到裂变阈能以下。</p></li>
<li><p>慢化剂一级结构材料等物质的辐射俘获：热中子部分被燃料吸收，部分被其他物质所吸收，记热中子利用系数为
<span class="math inline">\(f\)</span>
表征被燃料吸收的热中子数与热中子总数的比值。</p></li>
<li><p>燃料吸收热中子引起裂变：定义 <span
class="math inline">\(\eta\)</span>
为核燃料每吸收一个热中子所产生的平均裂变中子数，设每次裂变所产生的平均中子数为
<span class="math inline">\(\nu\)</span>，则有</p></li>
</ol>
<p><span class="math display">\[
\eta  = \frac{ { {\Sigma _f} } } { { {\Sigma _a} } }\nu
\]</span></p>
<ol start="5" type="1">
<li>中子的泄露：包括慢化过程中的泄露和热中子扩散过程中的泄露，定义不泄露概率为<span
class="math inline">\(\Lambda\)</span>，则有</li>
</ol>
<p><span class="math display">\[
\Lambda  = {\Lambda _s} {\Lambda _d}
\]</span></p>
<p>最终得到下一代的中子总数为 <span class="math inline">\(n\varepsilon
pf\eta {\Lambda _s}{\Lambda _d}\)</span>，根据有效增殖因数定义</p>
<p><span class="math display">\[
{k_{eff} } = \frac{ {n\varepsilon pf\eta {\Lambda _s}{\Lambda _d} } }
{n} = {k_\infty }\Lambda
\]</span></p>
<p>其中 <span class="math inline">\({k_\infty } = \varepsilon
pf\eta\)</span> 称为四因子公式</p>
<h1 id="中子慢化理论">中子慢化理论</h1>
<p>反应堆内裂变中子具有相当高的能量，其平均值约为2
MeV。这些中子在系统中与原子核发生连续的弹性和非弹性碰撞，使其能量逐渐地降低到引起下一次裂变的平均能量。对于快中子反应堆这一平均能量一般在(0.1
MeV)左右或更高，而对于热中子反应堆，绝大多数裂变中子被慢化到热能区。中子由于散射碰撞而降低速度的过程叫做慢化过程。</p>
<h2 id="中子的弹性散射过程">中子的弹性散射过程</h2>
<p><img src="Elastic-scattering.png" width="50%" height="50%" title="实验室坐标系和质心系弹性散射" alt="错误无法显示"/></p>
<p>实验室坐标系L下中子与核子的碰撞是各项异性的，质心系C中是各向同性的。所以我们在C系中进行研究。</p>
<p>设靶核静止，则在碰撞前质心系中中子和靶核的速度为：</p>
<p><span class="math display">\[
\begin{array}{l}
v_c = v_1 - V_{CM} = \frac{A}{A+1} v_1 \\
V_c = -V_{CM} = - \frac{1}{A+1} v_1
\end{array}
\]</span></p>
<p>在质心系里看，碰撞前后中子和靶核的速度大小不变，只是运动的方向发生了变化。</p>
<p><span class="math display">\[
\begin{array}{l}
v_c&#39; = v_1 - V_{CM} = -\frac{A}{A+1} v_1 \\
V_c&#39; = -V_{CM} = \frac{1}{A+1} v_1
\end{array}
\]</span></p>
<p><img src="L-C-angle.png" width="50%" height="50%" title="L系和C系散射角" alt="错误无法显示"/></p>
<p>换回到实验室坐标系L下，得到碰后中子的速度为</p>
<p><span class="math display">\[
v_1&#39;^2 = V_{CM}^2 + v_c&#39;^2 + 2 v_c&#39; V_{CM} {\rm cos}
\theta_c = \frac{v_1^2 (A^2 + 2A {\rm cos} \theta_c + 1) }{ (A+1)^2 }
\]</span></p>
<p>因此经过一次碰撞前后，中子的能量之比为</p>
<p><span class="math display">\[
\frac{E&#39;}{E} = \frac{v_1&#39;^2 }{v_1^2} = \frac{(A^2 + 2A {\rm cos}
\theta_c + 1) }{ (A+1)^2 }
\]</span></p>
<p>引入参数 $ = ()^2$，碰后能量可以写为</p>
<p><span class="math display">\[
E&#39; = \frac{1}{2} [(1+\alpha) - (1-\alpha){\rm cos} \theta_c]E
\]</span></p>
<ul>
<li><p><span class="math inline">\(\theta = 0^ {\circ}\)</span>时, <span
class="math inline">\(E&#39; \rightarrow E&#39;_{\rm max} =
E\)</span>，碰撞前后中子能量不变</p></li>
<li><p><span class="math inline">\(\theta = 180^ {\circ}\)</span>时,
<span class="math inline">\(E&#39; \rightarrow E&#39;_{\rm min} = \alpha
E\)</span></p></li>
</ul>
<p>因此经过一次碰撞后中子所处的能量区间为 <span
class="math inline">\([\alpha E,
E]\)</span>，一次碰撞的最大能量损失为<span class="math inline">\(\Delta
E_{\rm max} = (1 - \alpha) E\)</span>，为了慢化速度加快，则应使 <span
class="math inline">\(A\)</span>
尽量小，所以应该选择轻核元素作为慢化剂。</p>
<p>根据散射角之间的关系，有</p>
<p><span class="math display">\[
v&#39;_1 {\rm cos} \theta_l = V_{CM} + v&#39;_c {\rm cos} \theta_c
\]</span></p>
<p>联立消去 <span class="math inline">\(\theta_c\)</span>
可得实验室坐标下散射角余弦与碰撞前后能量的关系</p>
<p><span class="math display">\[
{\rm cos} \theta_l = \frac{1}{2} [(A+1) \sqrt{\frac{E&#39;}{E} } - (A-1)
\sqrt{\frac{E}{E&#39;}}]
\]</span></p>
<h3 id="散射后中子的能量分布">散射后中子的能量分布</h3>
<p>根据碰撞后中子散射角分布的概率可以求得碰撞后中子能量 <span
class="math inline">\(E&#39;\)</span> 的分布，定义 <span
class="math inline">\(f(E \rightarrow E&#39;)\)</span>
为<strong>散射函数</strong>， <span class="math inline">\(f(E
\rightarrow E&#39;) dE&#39;\)</span> 表示碰撞前中子能量为 <span
class="math inline">\(E\)</span>，碰撞后中子能量在 <span
class="math inline">\(E&#39;\)</span> 附近 <span
class="math inline">\(dE&#39;\)</span> 的概率，当入射能量 <span
class="math inline">\(E\)</span> 小于几 MeV
时，质心系内中子的散射是各向同性的，因此</p>
<p><span class="math display">\[
f(\theta_c) d\theta_c = \frac{d \Omega_c}{4 \pi} = \frac{1}{2} {\rm
sin}\theta_c d\theta_c
\]</span></p>
<p>进而可以得到散射函数为</p>
<p><span class="math display">\[
f(E \rightarrow E&#39;) dE&#39; = -\frac{d E&#39;}{(1 - \alpha) E} \quad
\quad \alpha E \leq E&#39; \leq E
\]</span></p>
<p>这表明散射后的能量是均匀分布的，并且与碰撞后的能量 <span
class="math inline">\(E&#39;\)</span> 大小无关。</p>
<h3 id="平均对数能降">平均对数能降</h3>
<p>由于中子能量分布范围广，反应堆物理中常用<strong>对数能降</strong>
<span class="math inline">\(u\)</span>
来描述能量变化，取裂变中子平局能量 <span class="math inline">\(E_0 = 2
{\rm MeV}\)</span> 为参考能量，对数能降定义为</p>
<p><span class="math display">\[
u = \ln \frac{ { {E_0}} } { {E} }
\]</span></p>
<p>一次碰撞后对数能量的增量为</p>
<p><span class="math display">\[
\Delta u = u&#39; - u = \ln \frac{ { {E_0}} } { {E&#39;} } -  \ln \frac{
{ {E_0}} } { {E} } =  \ln \frac{ { {E}} } { {E&#39;} }
\]</span></p>
<p>描述中子慢化过程时，通常采用<strong>平均对数能降</strong> <span
class="math inline">\(\xi\)</span></p>
<p><span class="math display">\[
\xi = \overline {\ln E - \ln E&#39;}  = \int_E^{\alpha E} {\left( {\ln E
- \ln E&#39;} \right)f\left( {E \to E&#39;} \right)dE&#39;}  =
\int_E^{\alpha E} {\ln \frac{E}{ {E&#39;} }\frac{ {dE&#39;} }{ {\left(
{1 - \alpha } \right)E}}}
\]</span></p>
<p>积分后得</p>
<p><span class="math display">\[
\xi  = 1 + \frac{\alpha }{ {1 - \alpha } }\ln \alpha  = 1 - \frac{ { {
{\left( {A - 1} \right)}^2} } } { {2A} }\ln \left( {\frac{ {A + 1}} { {A
- 1}} } \right)
\]</span></p>
<p><span class="math inline">\(A &gt; 10\)</span>
时，平均对数能降可近似为</p>
<p><span class="math display">\[
\xi  \approx \frac{2}{ {A + \frac{2}{3}}}
\]</span></p>
<p>因此当质心系中的散射为各向同性时，平均对数能降 <span
class="math inline">\(\xi\)</span> 只与靶核质量数 <span
class="math inline">\(A\)</span> 有关，而与中子的能量无关。</p>
<h3 id="平均散射角余弦">平均散射角余弦</h3>
<p>在L实验室坐标系中，每次碰撞的平均散射角余弦值为</p>
<p><span class="math display">\[
\overline { {\mu _l}}  = \int_0^\pi  {\cos {\theta _l}f\left( { {\theta
_l}} \right)d{\theta _l}}  = \frac{1}{2}\int_0^\pi  {\frac{ {A\cos
{\theta _c} + 1}}{ {\sqrt { {A^2} + 2A\cos {\theta _c} + 1} }}} \sin
{\theta _c}d{\theta _c} = \frac{2}{ {3A}}
\]</span></p>
<p>因此L系中散射各相异性，中子沿原来方向运动的概率较大；靶核质量愈小，散射各相异性概率愈大。
<span class="math inline">\(A \rightarrow 0\)</span> 时，<span
class="math inline">\({\mu _l \rightarrow 0}\)</span>,
系统质心移动到靶核上，L系散射各向同性。</p>
<h3 id="慢化剂选择">慢化剂选择</h3>
<p>慢化剂的选择要考虑以下因素：</p>
<ul>
<li>大的宏观散射截面 <span class="math inline">\(\Sigma_s\)</span></li>
<li>较大的对数能降 <span class="math inline">\(\xi\)</span></li>
<li>较大的慢化比 <span class="math inline">\(\xi \Sigma_s /
\Sigma_a\)</span></li>
</ul>
<p>反应堆常用的慢化剂有：</p>
<ol type="1">
<li>重水，慢化比最大，但是价格昂贵，可以直接用天然铀做核燃料；</li>
<li>轻水，慢化能力大，慢化比小，但是价格低廉，需要用浓缩铀；</li>
<li>石墨，慢化比较大，慢化能力小，需要有庞大的堆芯体积。</li>
</ol>
<h3 id="中子平均寿命">中子平均寿命</h3>
<p>快中子自裂变产生到慢化成热中子，直至最后被俘获所需的平均时间，称为中的的<strong>平均寿命</strong>
<span class="math inline">\(l\)</span></p>
<p>对于热中子反应堆，中子平均寿命主要由扩散时间决定。对于压水堆，中子平均寿命
<span class="math inline">\(l \approx 10^{-4} {\rm
s}\)</span>，对于快中子反应堆，中子平均寿命 <span
class="math inline">\(l \approx 10^{-7} {\rm s}\)</span>.</p>
<p><span class="math display">\[
l = t_s + t_d
\]</span></p>
<p>其中 <span class="math inline">\(t_s\)</span> 为慢化时间, <span
class="math inline">\(t_d\)</span> 为扩散时间</p>
<h4 id="慢化时间">慢化时间</h4>
<p><span class="math inline">\(dt\)</span>
时间间隔内中子与原子核发生弹性散射碰撞次数为 <span
class="math inline">\(n = v dt / \lambda_s (E)\)</span>,
产生的平均对数能降为</p>
<p><span class="math display">\[
du = -\frac{dE}{E} = \frac{\xi v} dt
\]</span></p>
<p>其中 <span class="math inline">\({\lambda_s (E)}\)</span> 是能量为
<span class="math inline">\(E\)</span> 的中子的散射平均自由程,
可以得到中子从 <span class="math inline">\(E_0\)</span> 慢化到 <span
class="math inline">\(E_{th}\)</span> 所用的时间<span
class="math inline">\(t_s\)</span>为</p>
<p><span class="math display">\[
t_s = - \int_{ {E_0}}^{ {E_{th}}} {\frac{ { {\lambda _s}\left( E
\right)}}{ {\xi v}}} \frac{ {dE}}{E} \approx \sqrt 2 \frac{ { { {\bar
\lambda }_s}}}{\xi }\left[ {\frac{1}{ {\sqrt { {E_{th}}} }} - \frac{1}{
{\sqrt { {E_0}} }}} \right]
\]</span></p>
<h4 id="扩散时间">扩散时间</h4>
<p>快中子慢化成热中子后，在被俘获前还会在介质中扩散一段时间 <span
class="math inline">\(t_d\)</span>, 以 <span
class="math inline">\(\lambda_a (E)\)</span>
表示中子的吸收平均自由程，则热中子的平均寿命为</p>
<p><span class="math display">\[
{t_d}\left( E \right) = \frac{ { {\lambda _a}\left( E \right)}}{v} =
\frac{1}{ { {\Sigma _a}\left( E \right)v}}
\]</span></p>
<p>对于吸收截面满足 <span class="math inline">\(1/v\)</span>
律的介质，有 <span class="math inline">\({\Sigma _{a 0}}\left( E \right)
v_0\)</span>，扩散时间可以写为</p>
<p><span class="math display">\[
t_d (E) = \frac{1}{ { {\Sigma _{a0}}\left( E \right)v_0}}
\]</span></p>
<p>其中 <span class="math inline">\({\Sigma _{a 0}}\)</span> 是 <span
class="math inline">\(v_0 = 2200 {\rm m / s}\)</span>
的热中子宏观吸收截面。</p>
<h2 id="中子慢化能谱">中子慢化能谱</h2>
<h3 id="中子慢化方程的建立">中子慢化方程的建立</h3>
<p>在描述中子慢化过程时，通常采用<strong>慢化密度</strong> <span
class="math inline">\(q({\bf r}, E)\)</span> 这个物理量，它表示在 <span
class="math inline">\({\bf r}\)</span> 处单位体积内每秒慢化到能量 <span
class="math inline">\(E\)</span> 以下的中子数。</p>
<p>在 <span class="math inline">\({\bf r}\)</span> 处能量为 <span
class="math inline">\(E&#39;\)</span> 的中子单位体积内每秒慢化到 <span
class="math inline">\(E\)</span> 以下的中子数为</p>
<p><span class="math display">\[
\int_E^0 { {\Sigma _s}\left( { {\bf r},E&#39;} \right)f\left( {E&#39;
\to E} \right)\phi \left( { {\bf r},E&#39;} \right)dE}
\]</span></p>
<p>根据定义慢化密度为 <span class="math inline">\(E &gt; E&#39;\)</span>
的所有能量的中子慢化到 <span class="math inline">\(E\)</span>
以下的数目总和，因此有</p>
<p><span class="math display">\[
q\left( { {\bf r},E} \right) = \int_E^\infty  {dE&#39;\int_E^0 { {\Sigma
_s}\left( { {\bf r},E&#39;} \right)f\left( {E&#39; \to E} \right)\phi
\left( { {\bf r},E&#39;} \right)dE} }
\]</span></p>
<p>在弹性散射的条件下，方程化简为</p>
<p><span class="math display">\[
\begin{array}{l}
\begin{aligned}
q\left( { {\bf r},E} \right) &amp; = \int_E^{E/\alpha }
{dE&#39;\int_{\alpha E&#39;}^E {\frac{ { {\Sigma _s}\left( { {\bf
r},E&#39;} \right)\phi \left( { {\bf r},E&#39;} \right)}} { {\left( {1 -
\alpha } \right)E&#39;}}dE} } \\
&amp; = \int_E^{E/\alpha } { {\Sigma _s}\left( { {\bf r},E&#39;}
\right)\phi \left( { {\bf r},E&#39;} \right)\frac{ {E - \alpha E&#39;}}
{ {\left( {1 - \alpha } \right)E&#39;}}dE&#39;}
\end{aligned}
\end{array}
\]</span></p>
<p>对于<strong>无限均匀介质</strong>，中子通量密度与空间坐标<span
class="math inline">\({\bf r}\)</span> 无关，只能能量 <span
class="math inline">\(E\)</span> 的函数。现考虑能量在 <span
class="math inline">\(E \sim E + dE\)</span>
能量间隔内的中子平衡：<br />
进入 <span class="math inline">\(E \sim E + dE\)</span>
的中子可以分为两部分：</p>
<ol type="1">
<li>由裂变中子源产生的直接处于该能量间隔内的中子 <span
class="math inline">\(S(E) dE\)</span></li>
<li>由于中子与介质原子核散射进入该能量间隔的中子，即由 <span
class="math inline">\(E&#39; &gt; E\)</span> 的能区散射到 <span
class="math inline">\(E \sim E + dE\)</span> 能区内的中子 <span
class="math inline">\(dE\int_\infty ^E { {\Sigma _s}\left( {E&#39;}
\right)f\left( {E&#39; \to E} \right)\phi \left( {E&#39;}
\right)dE&#39;}\)</span></li>
</ol>
<p>离开该能区的中子数为从该能区散射出去和被吸收的中子数 <span
class="math inline">\(\Sigma_t (E) \phi (E) dE\)</span>.</p>
<p>根据中子平衡的稳态条件，进入该能区的中子数等于离开该能区的中子数，得到稳态无限介质内的中子慢化方程为</p>
<p><span class="math display">\[
{\Sigma _t}\left( E \right)\phi \left( E \right) = \int_\infty ^E {
{\Sigma _s}\left( {E&#39;} \right)f\left( {E&#39; \to E} \right)\phi
\left( {E&#39;} \right)dE&#39;}  + S\left( E \right)
\]</span></p>
<p>慢化方程的解即为所要求的中子慢化能谱。</p>
<p>多数情况下，慢化方程式是无法求出解析解的，针对几种特定情况讨论一下慢化方程的解。</p>
<h3 id="无吸收单核素无限介质">无吸收单核素无限介质</h3>
<p>对于只含有一种核素的无吸收性介质,中子源 <span
class="math inline">\(S(E_0)\)</span>
为均匀分布，着重讨论慢化区，此能量区间不存在由裂变反应直接产生的裂变中子源，中子慢化方程写为</p>
<p><span class="math display">\[
{\Sigma _s}\left( E \right)\phi \left( E \right) = \int_E^{E/\alpha } {
\frac{ {\Sigma _s}\left( {E&#39;} \right)\phi \left( {E&#39;} \right)} {
{\left( {1 - \alpha } \right)E&#39;}}dE&#39;}
\]</span></p>
<p>它的渐进解形式为 <span class="math inline">\(\phi (E) = \frac{C}
{E}\)</span>，代入慢化密度表达式得</p>
<p><span class="math display">\[
\begin{array}{l}
\begin{aligned}
q(E) &amp; = \frac{C}{1 - \alpha} \int_E^{E/\alpha } {\frac{\Sigma _s (E
- \alpha E&#39;)} {E&#39;^2} dE&#39;} \\
&amp; = C \Sigma_s (1 + \frac{\alpha}{1 - \alpha} \ln \alpha) \\
&amp; = C \Sigma_s \xi
\end{aligned}
\end{array}
\]</span></p>
<p>得到中子慢化能谱为 <span class="math inline">\(\phi (E) = \frac{q(E)}
{\xi \Sigma_s E}\)</span>，在无吸收稳态条件下慢化密度就是源强度, <span
class="math inline">\(q(E) = S_0\)</span>, 最终得到中子能谱为</p>
<p><span class="math display">\[
\phi (E) = \frac{S_0} {\xi \Sigma_s E}
\]</span></p>
<p>这种形式的能谱称为 <span class="math inline">\(1/E\)</span>
谱或费米谱。</p>
<h3 id="无吸收混合物无限介质情况">无吸收混合物无限介质情况</h3>
<p>对于混合物体系，其宏观散射截面 <span
class="math inline">\(\Sigma_s(E) = \sum_{i} N_i \sigma_{si}\)</span>,
对应漫化方程写为</p>
<p><span class="math display">\[
{\Sigma _t}\left( E \right)\phi \left( E \right) = \int_E ^{E_0}
\frac{N_i \sigma_{si} \phi(E&#39;)} {(1 - \alpha_i) E&#39;} dE&#39;
\]</span></p>
<p>得到中子能谱的解为</p>
<p><span class="math display">\[
\phi (E) = \frac{1} {\sum_{i} {N_i \sigma_{si} \xi_i}} \frac{q(E)} {E}
\]</span></p>
<p>定义混合物的平均对数能降为 <span class="math inline">\(\bar \xi =
\frac {\sum_{i} {N_i \sigma_{si} \xi_i}} {\sum_{i} {N_i
\sigma_{si}}}\)</span>，得到中子慢化能谱为</p>
<p><span class="math display">\[
\phi (E) = \frac{q(E)} {\bar \xi \Sigma_s  E}
\]</span></p>
<h3 id="无限介质弱吸收情况">无限介质弱吸收情况</h3>
<p>无限介质弱吸收情况，认为宏观吸收截面比宏观散射截面小很多，在 <span
class="math inline">\(E - dE\)</span>
的间隔中，慢化密度由于被中子被吸收减小了 <span
class="math inline">\(dq\)</span>，它应该等于在 <span
class="math inline">\(dE\)</span> 内被吸收的中子数，在目前所讨论的弱吸收
<span class="math inline">\(\Sigma_a \ll \Sigma_s\)</span>
情况下，近似地可以认为 <span class="math inline">\(\phi(E)\)</span>
基本上和无吸收时情况相同，因此有</p>
<p><span class="math display">\[
dq = - \Sigma_a \phi(E) dE = - \Sigma_a \frac{q(E)} {\bar \xi \Sigma_s}
\frac{dE}{E}
\]</span></p>
<p>将上式对 <span class="math inline">\(E\)</span> 到 <span
class="math inline">\(E_0\)</span> 积分，同时注意到 <span
class="math inline">\(q(E_0) = S_0\)</span>，因而有</p>
<p><span class="math display">\[
q(E) = S_0 \exp (- \int_E^{E_0} {\frac{\Sigma_a } {\bar \xi \Sigma_s}
\frac{dE&#39;}{E&#39;}})
\]</span></p>
<p>根据逃脱共振俘获概率 <span class="math inline">\(p(E)\)</span>
的定义，有</p>
<p><span class="math display">\[
p(E) = \frac{q(E)}{S_0} = \exp [- \int_E^{E_0} {\frac{\Sigma_a (E) }
{\bar \xi \Sigma_s} \frac{dE&#39;}{E&#39;}}]
\]</span></p>
<h2 id="慢化过程的共振吸收">慢化过程的共振吸收</h2>
<p>中子能量慢化到 <span class="math inline">\(0.1 {\rm MeV}\)</span>
后，许多核素截面会在中能区产生共振峰，使得部分中子在慢化过程中被共振吸收，下面讨论具有强吸收共振峰的均匀介质情形。</p>
<p><img src="Neutron-resonance-absorption.png" width="50%" height="50%" title="宽间距共振峰的共振吸收" alt="Error"/></p>
<h3 id="有效共振积分">有效共振积分</h3>
<p>由慢化剂和吸收剂组成的无限均匀介质，介质内存在均匀分布的中子源，源强为
<span class="math inline">\(S_0\)</span>，放出能量为 <span
class="math inline">\(E_0\)</span>
的中子。共振峰间距足够大以至于前一个共振峰对后一个共振峰的影响可以忽略不计。近似认为到达共振峰
<span class="math inline">\(i\)</span>
的中子能谱为费米谱的形式，并对中子通量密度进行归一化，如认为源强 <span
class="math inline">\(S_0 = \xi \Sigma_s\)</span></p>
<p><span class="math display">\[
\phi (E) = \frac{S_0}{\xi \Sigma_s E} \sim \frac{1}{E}
\]</span></p>
<p>此时共振峰的吸收反应率为 <span class="math display">\[
R = N_A \int_{\Delta E_i} {\sigma_a (E) \phi (E) dE}
\]</span> 其中 <span class="math inline">\(\phi (E)\)</span> 为共振峰
<span class="math inline">\(i\)</span>
内的中子通量密度分布，而在共振峰前的中子通量密度为 <span
class="math inline">\(\phi (E) = 1/E\)</span>，定义共振峰 <span
class="math inline">\(i\)</span> 的<strong>有效共振积分</strong></p>
<p><span class="math display">\[
I_i = \int_{\Delta E_i} {\sigma_a (E) \phi (E) dE}
\]</span> 进而得到中子通过共振峰 <span class="math inline">\(i\)</span>
的被吸收概率为 <span class="math inline">\(N_A I_i / \xi
\Sigma_s\)</span>，进而可以计算逃脱共振俘获概率</p>
<p><span class="math display">\[
p_i = 1 - \frac{N_A I_i} {\xi \Sigma_s} \approx \exp[- \frac{N_A}{\xi
\Sigma_s} I_i]
\]</span></p>
<p>裂变中子从初始能量 <span class="math inline">\(E_0\)</span>
慢化至热中子能量 <span class="math inline">\(E_th\)</span>
的过程中需要通过共振区的所有共振峰，因此热中子反应堆中逃脱共振俘获概率
<span class="math inline">\(p\)</span> 为</p>
<p><span class="math display">\[
p = \prod\limits_i { {p_i}}  = \exp \left[ { - \frac{ { {N_A}}}{ {\xi
{\Sigma _s}}}\sum\limits_i { I_i}} \right] = \exp \left[ { - \frac{ {
{N_A}}}{ {\xi {\Sigma _s}}}I} \right]
\]</span> 其中 <span class="math inline">\(I\)</span>
为整个共振区的有效共振积分，表征共振峰对中子的吸收 <span
class="math display">\[
I = \sum\limits_i { I_i} = \int_{\Delta E} {\sigma_a (E) \phi (E) dE}
\]</span></p>
<h3 id="有效共振积分计算">有效共振积分计算</h3>
<p>要求出有效共振积分首先要根据慢化方程求出共振峰内的中子通量密度 <span
class="math inline">\(\phi (E)\)</span>，考虑无吸收慢化剂 <span
class="math inline">\(M\)</span> 和吸收剂 <span
class="math inline">\(A\)</span> 无限均匀介质，根据中子慢化方程</p>
<p><span class="math display">\[
\Sigma_t (E) \phi(E)  = \int_{E}^{E/ \alpha_M} {\frac {\Sigma_{s, M}
\phi(E&#39;)}{(1 - \alpha_M) E&#39;} dE&#39;} + \int_{E}^{E/ \alpha_A}
{\frac {\Sigma_{s, A} (E&#39;) \phi(E&#39;)}{(1 - \alpha_A) E&#39;}
dE&#39;}
\]</span> 其中 <span class="math inline">\(\Sigma_t (E) = \Sigma_{s, M}
+ \Sigma_{\gamma, A} (E) + \Sigma_{s, A} (E)\)</span>，由于吸收剂 <span
class="math inline">\(A\)</span> 的散射截面 <span
class="math inline">\(\Sigma_{s, A} (E)\)</span> 除了势散射截面 <span
class="math inline">\(\sigma_{p, A}\)</span>
外，还包括共振散射截面，因此它是能量的函数。</p>
<h4 id="窄共振nr近似">窄共振(NR)近似</h4>
<ul>
<li><span class="math inline">\(\overline {\Delta {E_M}}  \gg {\Gamma
_p}, \overline {\Delta {E_A}}  \gg {\Gamma _p}\)</span></li>
</ul>
<p>NR近似下中子通量密度为 <span class="math display">\[
{\phi _{NR}}\left( E \right) = \frac{ { {\Sigma _{s,M}} + {\Sigma
_{p,A}}}}{ { {\Sigma _t}\left( E \right)E}}
\]</span></p>
<p>有效共振积分 <span class="math display">\[
{I_{NR,i}} = \int_{\Delta {E_i}} { {\sigma _{\gamma ,A}}\left( E
\right)\frac{ { {\Sigma _{s,M}} + {\Sigma _{p,A}}}}{ { {\Sigma _t}\left(
E \right)}}\frac{ {dE}}{E}}
\]</span></p>
<h4 id="窄共振无限质量nrim近似">窄共振无限质量(NRIM)近似</h4>
<ul>
<li><span class="math inline">\(\overline {\Delta {E_M}}  \gg {\Gamma
_p}, \overline {\Delta {E_A}}  \ll {\Gamma _p}\)</span></li>
</ul>
<p>NRIM近似下中子通量密度为 <span class="math display">\[
{\phi _{NRIM}}\left( E \right) = \frac{ { {\Sigma _{s,M}}}}{ {\left[ {
{\Sigma _{s,M}} + {\Sigma _{\gamma ,A}}\left( E \right)} \right]E}}
\]</span></p>
<p>有效共振积分 <span class="math display">\[
{I_{NRIM,i}} = \int_{\Delta {E_i}} { {\sigma _{\gamma ,A}}\left( E
\right)\frac{ { {\Sigma _{s,M}}}}{ { {\Sigma _{s,M}} + {\Sigma _{\gamma
,A}}\left( E \right)}}\frac{ {dE}}{E}}
\]</span></p>
<p>中子通量密度和有效共振积分通式为</p>
<p><span class="math display">\[
\begin{array}{l}
\begin{aligned}
{\phi _R}\left( E \right) = \frac{ { {\Sigma _{s,M}} + \lambda {\Sigma
_{p,A}}}}{ {\left( { {\Sigma _{\gamma ,A}} + \lambda {\Sigma _{s,A}} +
{\Sigma _{s,M}}} \right)E}}\\
{I_i} = \int_{\Delta {E_i}} { {\sigma _{\gamma ,A}}\left( E
\right)\frac{ { {\Sigma _{s,M}} + \lambda {\Sigma _{p,A}}}}{ { {\Sigma
_{\gamma ,A}} + \lambda {\Sigma _{s,A}} + {\Sigma _{s,M}}}}\frac{
{dE}}{E}}
\end{aligned}
\end{array}
\]</span> 其中 <span class="math inline">\(\lambda=1\)</span>
对应NR近似，<span class="math inline">\(\lambda=0\)</span>
对应NRIM共振近似。</p>
<h3 id="能量自屏效应">能量自屏效应</h3>
<p><img src="Energy-self-shield.png" width="50%" height="50%" title="共振峰内中子通量密度畸变" alt="Error"/></p>
<p>由于热中子在进入共振中心区域之前会被逐渐吸收，导致到达中心时的中子注量率明显低于燃料棒表面的中子注量率。在共振峰附近能量的中子尤其容易被自屏掉，因为共振截面通常很大。这种局部地方的中子注量率明显低于周围的现象，就被称为自屏效应。</p>
<h2 id="热中子能谱">热中子能谱</h2>
<p>热中子是指中子与所在的介质的原子或分子处于热平衡状态的中子。反应堆物理中通常把某个分解能量
<span class="math inline">\(E_c\)</span> 以下的中子称为热中子， <span
class="math inline">\(E_c\)</span>
称为<strong>分界能</strong>或<strong>缝合能</strong>，对于压水堆通常取
<span class="math inline">\(E_c = 0.625 {\rm eV}\)</span>.</p>
<h3 id="热中子能谱的硬化">热中子能谱的“硬化”</h3>
<ul>
<li>在反应堆中，所有的热中子都是从高能慢化而来，然后与介质达到热平衡，因此较高能区的中子数就较多。</li>
<li>由于介质也要吸收中子，因此必然有一部分中子尚未来得及和周围介质达到热平衡就被介质吸收了。因此低能量的地方，吸收概率要大一些，其结果又造成了能量较低部分的中子份额减少，高能中子的份额较大。</li>
</ul>
<p>热中子的平均能量和最概然能量都要比介质原子核的平均能量和最概然能量高，这一现象称为热中子能谱的<strong>硬化</strong></p>
<p>硬化的结果相当于把介质的麦克斯韦分布谱向右移动，使 <span
class="math inline">\(T_m\)</span> 增大到 <span
class="math inline">\(T_n\)</span>。 <span
class="math inline">\(T_n\)</span>
称为中子温度，中子温度的数值一般要比介质温度高，这个差值与介质的慢化能力相关，近似有以下关系</p>
<p><span class="math display">\[
T_n = T_M (1 + C \frac{\Sigma_a (k T_M)}{\xi \Sigma_s})
\]</span> 对于铀-水栅格，通常取 <span
class="math inline">\(C=1.4\)</span>，如果栅元或介质内的各元素吸收截面服从
<span class="math inline">\(1/v\)</span> 定律，则</p>
<p><span class="math display">\[
\Sigma_a (k T_M) = \Sigma_a (0.0253) \sqrt{\frac{293}{T_M}}
\]</span></p>
<p>通过分析，可以得到热中子反应堆内的能谱分布</p>
<ul>
<li>高能区（能量大于0.1
MeV）,中子能谱近似地可以用裂变中子谱来描述。</li>
<li>在慢化区，中子能量密度的能谱近似按照 <span
class="math inline">\(1/E\)</span> 规律变化。</li>
<li>在热能区，中子能谱可用麦克斯韦谱近似(<span
class="math inline">\(T_M\)</span> 换成 <span
class="math inline">\(T_n\)</span>)。</li>
</ul>
<h3 id="热中子的平均截面">热中子的平均截面</h3>
<p>实际计算中通常把热能区的中子视为一群中子，因此需要求出平均截面</p>
<p><span class="math display">\[
\sigma  = \frac{ {\int_0^{E_c} {\sigma \left( E \right)N\left( E
\right)vdE} }} { \int_0^{E_c} {N\left( E \right)vdE} }
\]</span> 计算可以得到</p>
<p><span class="math display">\[
{ {\bar \sigma }_a} = \frac{ {\sqrt \pi  }}{2}\sqrt {\frac{ {0.0253}}{
{k{T_n}}}} {\sigma _a}\left( {0.0253} \right)
\]</span></p>
<h1 id="中子扩散理论">中子扩散理论</h1>
<p>由于中子与原子核间的无规则碰撞，中子在介质内的运动是一种杂乱无章的具有统计性质的运动，即初始在堆内某一位置具有某种能量及某一运动方向的中子，在稍晚些时候，将运动到堆内的另一位置以另一能量和另一运动方向出现。这一现象称为中子在介质内的<strong>输运</strong>过程。为了对中子输运过程进行研究，首先必须建立描述中子在介质输运过程或中子角密度分布
<span class="math inline">\(n ({\bf r}, E, {\bf \Omega})\)</span>
所满足的基本方程式，然描述中子输运过程的精确方程叫做玻尔兹曼输运方程。即使
是稳态情况，中子输运方程也是一个含有空间位置 <span
class="math inline">\((x, y, z)\)</span>，能量 <span
class="math inline">\(E\)</span> 和运动方向 <span
class="math inline">\((\theta, \varphi)\)</span>
等6个自变量的偏微分-积分方程。</p>
<p>如果中子通量密度的角分布是接近于各向同性的，例如在大型反应堆堆芯的中心部分，那么，可以近似地认为中子通量密度的角分布与运动方向
<span class="math inline">\({\bf \Omega}\)</span>
的依赖性很弱甚至无关，这使同题大大简化。通过这种近似简化得到的方程称为<strong>扩散方程</strong>。</p>
<h2 id="单能中子扩散方程">单能中子扩散方程</h2>
<p>在分子扩散现象中分子扩散是由于分子间的相互碰撞引起的，而在热中子反应堆内中子密度约为
<strong><span class="math inline">\(10^{16} m^{-3}\)</span></strong>
数量级，比起介质的原子核密度 (<strong><span
class="math inline">\(10^{28} m^{-3}\)</span></strong> )
要小得多。因此反应堆内中子之间的相互碰撞可以略去不计，其扩散主要是中子与介质原子核散射碰撞的结果。在中子密度大的地方，中子与原子核碰撞的次数就越多，而每次碰撞以后，中子通常要改变运动方向离开碰撞中心因此与分子的扩散相似，中子总是从中子密度高的地方向密度低的地方扩散。</p>
<h3 id="fick定律">Fick定律</h3>
<p>中子的扩散和气体分子的扩散很相似，它们都从浓度高的区域向浓度底的区域扩散，扩散的速率与粒子的密度的变化梯度成正比，既都服从“斐克扩散定律”。</p>
<p><span class="math display">\[
J = -D \nabla \phi
\]</span></p>
<p><strong>假设</strong></p>
<ol type="1">
<li>介质是无限的、均匀的：只有均匀，散射截面才是常数，才能提到积分号外面；只有无限，才能做无穷积分。</li>
<li>在实验室坐标系中散射是各向同性的：落在单位立体角内的概率为 <span
class="math inline">\(1/4 \pi\)</span></li>
<li>介质的吸收截面很小即 <span class="math inline">\(\Sigma_a \ll
\Sigma_s\)</span> ：只考虑散射截面，会使积分计算方便</li>
<li>中子通量密度是随空间位置缓慢变化的函数：<span
class="math inline">\(\phi(r)\)</span>
才能写出表达式，做一阶泰勒展开</li>
</ol>
<p>具体推导过程参见 <a
href="https://zhuanlan.zhihu.com/p/699561762">中子扩散理论</a></p>
<p>在这个假设下，我们得到了中子的流密度如下：</p>
<ul>
<li>每秒沿 <span class="math inline">\(z\)</span>
轴负方向自上而下穿过单位面积的中子数：<br />
<span class="math display">\[
J_z^{-} = \frac{\phi_0}{4} + \frac{1}{6 \Sigma_s} (\frac{\partial
\phi}{\partial z})
\]</span></li>
<li>每秒沿 <span class="math inline">\(z\)</span>
轴负方向自下而上穿过单位面积的中子数：<br />
<span class="math display">\[
J_z^{+} = \frac{\phi_0}{4} - \frac{1}{6 \Sigma_s} (\frac{\partial
\phi}{\partial z})
\]</span></li>
<li>单位时间内沿着 <span class="math inline">\(z\)</span>
方向穿过单位面积的净中子数 <span class="math inline">\(J_z\)</span> 为
<span class="math display">\[
J_z = J_z^{+} - J_z^{-} = - \frac{\lambda_s}{3} \frac{\partial \phi
({\bf r})}{\partial z}
\]</span></li>
</ul>
<p><span class="math inline">\(J_x\)</span> 和 <span
class="math inline">\(J_y\)</span> 同理可得</p>
<p><span class="math display">\[
J_x = - \frac{\lambda_s}{3} \frac{\partial \phi ({\bf r})}{\partial x}
\]</span></p>
<p><span class="math display">\[
J_y = - \frac{\lambda_s}{3} \frac{\partial \phi ({\bf r})}{\partial y}
\]</span></p>
<p>最终得到中子流密度为 <span class="math display">\[
{\bf J} = J_x \hat{i} + J_y \hat{j} + J_z \hat{k} = -
\frac{\lambda_s}{3} \nabla \phi
\]</span></p>
<p>斐克定律表示：中子流密度J正比于负的中子通量密度梯度，
其比例常数叫作扩散系数，并用 <span class="math inline">\(D\)</span>
表示。推导过程中使用了在实验室坐标系中中子的散射是各向同性的假设，实际计算中应对散射的各向异性进行修正，必须用输运的平均自由程
<span class="math inline">\(\lambda_{tr}\)</span> 代替散射平均自由程
<span class="math inline">\(\lambda_s\)</span>，扩散系数 <span
class="math inline">\(D\)</span> 可写为</p>
<p><span class="math display">\[
\begin{array}{l}
\begin{aligned}
&amp; D = \frac{\lambda_{tr}}{3} \\
&amp; \lambda_{tr} = \frac{\lambda_s}{1 - \bar \mu_0} \\
&amp; \mu_0 = \frac{2}{3A}
\end{aligned}
\end{array}
\]</span></p>
<h3 id="单能中子扩散方程建立">单能中子扩散方程建立</h3>
<p>考虑一定体积V内的中子数守恒 <span class="math display">\[
\frac{d}{dt} \int_{V} {n ({\bf r}, t) dV} = 产生率(S) - 泄露率(L) -
吸收率(A)
\]</span> 其中 <span class="math display">\[
\begin{array}{l}
\begin{aligned}
&amp; L = \int_{S}{ {\bf J}({\bf r}, t) \cdot {\bf n} dS} = \int_{V}
{\nabla \cdot {\bf J} dV} = -\int_{V} {D \nabla^2 \phi}({\bf r}, t)
dV  \\
&amp; S = \int_{V} {S(\bf r, t)} dV  \\
&amp; A = \int_{V} {\Sigma_{a} \phi ({\bf r}, t)} dV
\end{aligned}
\end{array}
\]</span></p>
<p>得到<strong>单能中子的扩散方程</strong>为 <span
class="math display">\[
\frac{1}{v} \frac{\partial \phi ({\bf r}, t)}{partial t} = S ({\bf r},
t) + D \nabla^2 \phi ({\bf r}, t) - \Sigma_a \phi ({\bf r}, t)
\]</span></p>
<p><strong>稳态</strong>情况下单能中子的扩散方程为 <span
class="math display">\[
S ({\bf r}, t) + D \nabla^2 \phi ({\bf r}, t) - \Sigma_a \phi ({\bf r},
t) = 0
\]</span></p>
<h3 id="扩散方程边界条件">扩散方程边界条件</h3>
<ol type="1">
<li>在扩散方程适用的范围内，中子通量密度必须是正的、有限的实数</li>
<li>在两种不同扩散性质的介质交界面上，垂直于分界面的中子流密度相等，中子通量密度相等</li>
<li>介质与真空交界外表面上从真空返回介质的中子流等于零</li>
</ol>
<p>假设从交界面处将中子通量密度的分布曲线按它在交界面处的斜率向真空作直线外推，则<strong>在离开交界面距离
<span class="math inline">\(d\)</span>
处的位置上中子通量密度为零</strong>， 我们有 <span
class="math inline">\(d = \frac{2}{3} \lambda_{th}\)</span>，<span
class="math inline">\(d\)</span> 称为直线外推距离</p>
<h3 id="fick定律适用范围">Fick定律适用范围</h3>
<ol type="1">
<li>在有限的介质内，在距离其表面几个自由程以外的全部区域斐克定律时成立的，而在距真空边界两三个自由程以内区域，它是不适用的。</li>
<li>中子通量密度必须缓慢变化或它的梯度变化不大。
在控制棒附近或两种扩散性质明显不同的介质交界面附近的几个平均自由程内，斐克定律不适用。此外，斐克定律只适用于弱吸收介质。</li>
<li>推导中并没有考虑中子源的贡献，中子流密度的贡献只是来自中子与介质
核的散射碰撞在距强中子源两三个平均自由程的区域内，斐克定律不适用。</li>
</ol>
<h2 id="非增殖介质内中子扩散方程的解">非增殖介质内中子扩散方程的解</h2>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>反应堆物理</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Learning 学习笔记</title>
    <url>/2024/05/08/Deep-Learning/</url>
    <content><![CDATA[<h1 id="pytorch神经网络基础">Pytorch神经网络基础</h1>
<h2 id="自动求导">自动求导</h2>
<span id="more"></span>
<p><img src="Data-flow.png" width="50%" height="50%" title="数据流" alt="错误无法显示"/></p>
<p>在如图所示的数据流下，要计算<span
class="math inline">\(y\)</span>关于<span
class="math inline">\(x\)</span>的梯度，可以采样两种方法</p>
<h3 id="正向传播forward-propagation">正向传播（Forward
Propagation）</h3>
<p>梯度计算方向和数据流方向相同：<span class="math inline">\(\frac{ dy }
{ dx } = \frac{ {dy} } { {db} } \left( {\frac{ {db} } { {da} }\left(
{\frac{ {da} } { {dx} } } \right)}
\right)\)</span>，称为正向传播模式</p>
<h3 id="反向传播backward-propagation">反向传播（Backward
Propagation）</h3>
<p>梯度计算方向和数据流方向相反：<span class="math inline">\(\frac{ dy }
{ dx } = \frac{ {da} } { {dx} } \left( {\frac{ {db} } { {da} }\left(
{\frac{ {dy} } { {db} } } \right)}
\right)\)</span>，称为反向传播模式</p>
<p>Back
Propagation算法是多层神经网络的训练中举足轻重的算法。简单的理解，它的确就是复合函数的链式法则，但其在实际运算中的意义比链式法则要大的多。</p>
<p><img src="Backward-propagation-example.png" width="50%" height="50%" title="反向传播示例" alt="错误无法显示"/></p>
<p>以上图为例子，我们相求<span class="math inline">\(e\)</span>关于<span
class="math inline">\(a\)</span>和<span
class="math inline">\(b\)</span>的导数，那么我们有<span
class="math inline">\(\frac{ {de} }{ {da} } = \frac{ {de} }{ {dc}
}\frac{ {dc} }{ {da} }\)</span> 和 <span class="math inline">\(\frac{
{de} }{ {db} } = \frac{ {de} }{ {dc} }\frac{ {dc} }{ {db} } + \frac{
{de} }{ {dd} }\frac{ {dd} }{ {db} }\)</span></p>
<p>如果采用Forward
Propagation，我们会发现这样做是十分冗余的，因为很多路径被重复访问了。比如图中的a-c-e和b-c-e就都走了路径c-e。对于权值动则数万的深度模型中的神经网络，这样的冗余所导致的计算量是相当大的。</p>
<p>Backward
Propagation算法则机智地避开了这种冗余，它对于每一个路径只访问一次就能求顶点对所有下层节点的偏导值。正如反向传播(BP)算法的名字说的那样，BP算法是反向(自上往下)来寻找路径的，从最上层的节点e开始，初始值为1，以层为单位进行处理。对于e的下一层的所有子节点，将1乘以e到某个节点路径上的偏导值，并将结果“堆放”在该子节点中。等e所在的层按照这样传播完毕后，第二层的每一个节点都“堆放"些值，然后我们针对每个节点，把它里面所有“堆放”的值求和，就得到了顶点e对该节点的偏导。然后将这些第二层的节点各自作为起始顶点，初始值设为顶点e对它们的偏导值，以"层"为单位重复上述传播过程，即可求出顶点e对每一层节点的偏导数。</p>
<p>而神经网络正是需要对每一层求梯度，因此BP算法恰好契合了神经网络的需要。</p>
<h2 id="模型构造">模型构造</h2>
<h3 id="层和块">层和块</h3>
<p>Pytoch中Module是一个很重要的概念，Module可以认为是任何一个层和一个神经网络它都属于Module的一个子类.在PyTorch中，nn.Module类的子类可以像函数一样被调用，这是因为在nn.Module的实现中，__call__方法被重写了，允许你像调用函数一样调用它们。当你调用一个继承自nn.Module的类的实例时，PyTorch会自动调用forward方法，这个方法定义了这个模型的前向传播逻辑。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">X = torch.rand(<span class="number">2</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        self.out = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># nn.Relu()是构造了一个ReLU对象，并不是函数调用，而F.ReLU()是函数调用</span></span><br><span class="line">        <span class="keyword">return</span> self.out(F.relu(self.hidden(X)))</span><br><span class="line"></span><br><span class="line">net1 = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line">net2 = MLP()</span><br><span class="line"><span class="built_in">print</span>(net1(X))</span><br><span class="line"><span class="built_in">print</span>(net2(X))</span><br></pre></td></tr></table></figure>
<p>这个例子中我们通过自定义继承nn.Module这个类来实现了特定功能的函数，我们可以通过继承nn.Module这个类可以比Sequential去更灵活的去定义我们的参数是什么样子以及如何做前向计算。</p>
<p>比如一个混合搭配各种混合块的例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NestMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                                 nn.Linear(<span class="number">64</span>, <span class="number">32</span>), nn.ReLU())</span><br><span class="line">        self.linear = nn.Linear(<span class="number">32</span>, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(self.net(X))</span><br><span class="line"></span><br><span class="line">chimera = nn.Sequential(NestMLP(), nn.Linear(<span class="number">16</span>, <span class="number">20</span>))</span><br><span class="line"><span class="built_in">print</span>(chimera(X))</span><br></pre></td></tr></table></figure>
<p>因此通过这种方法我们可以进行更加灵活的定义。</p>
<h2 id="参数管理">参数管理</h2>
<ul>
<li>访问某一层的参数</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">X = torch.rand(size=(<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].state_dict())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net[<span class="number">2</span>].bias))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.data) <span class="comment"># 访问对应的值</span></span><br><span class="line"><span class="built_in">print</span>(net.state_dict()[<span class="string">&#x27;2.bias&#x27;</span>].data)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.grad) <span class="comment"># 这里还没有做反向计算，所以梯度为None</span></span><br></pre></td></tr></table></figure>
<ul>
<li>一次性访问所有参数</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters()]) <span class="comment"># *[]序列解释包，将这个列表解包成单独的元组</span></span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()])</span><br></pre></td></tr></table></figure>
<h2 id="初始化">初始化</h2>
<p>初始化的目的是让模型在一开始的时候使得每一层的输入输出大小在一个尺度上面，不要然它出现越往后面越大或者越往后面越小的情况，使模型爆掉了。只要初始化开始时不出问题，不同的初始化方法对精度的影响其实差不多。</p>
<h3 id="一般初始化方法">一般初始化方法</h3>
<p>遍历所有Module进行初始化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_normal</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line"></span><br><span class="line">net.apply(init_normal)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.data, net[<span class="number">0</span>].bias.data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_constant</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line"></span><br><span class="line">net.apply(init_constant)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.data, net[<span class="number">0</span>].bias.data)</span><br></pre></td></tr></table></figure>
<h3 id="对某些块应用不同的初始化方法">对某些块应用不同的初始化方法</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">xavier</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_42</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">net[<span class="number">0</span>].apply(xavier)</span><br><span class="line">net[<span class="number">2</span>].apply(init_42)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.data[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data)</span><br></pre></td></tr></table></figure>
<h3 id="自定义初始化">自定义初始化</h3>
<p>也可以自定义初始化方法，如初始化保留绝对值大于等于5的权重</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_init</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">&quot;Init&quot;</span>,</span><br><span class="line">            *[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> m.named_parameters()][<span class="number">0</span>]</span><br><span class="line">        )</span><br><span class="line">        nn.init.uniform_(m.weight, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        m.weight.data *= m.weight.data.<span class="built_in">abs</span>() &gt;= <span class="number">5</span></span><br><span class="line"></span><br><span class="line">net.apply(my_init)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight[:<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<h3 id="更暴力的方法">更暴力的方法</h3>
<p>还可以直接把值拿出来做替换，如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.data[:] += <span class="number">1</span></span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] += <span class="number">42</span></span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<h3 id="参数绑定">参数绑定</h3>
<p>当有一些layer想要sharing某些weight时，可以进行参数绑定，也就是在构建net时其指向同一个类，这是不用网络直接共享权重的一个方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">shared = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), shared, nn.ReLU(), shared, nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line">net[<span class="number">2</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<h2 id="自定义层">自定义层</h2>
<p>本质上讲，自定义层和自定义网络没什么本质区别，因层也是nn.Module的一个子类</p>
<h3
id="构造一个没有任何参数的自定义层">构造一个没有任何参数的自定义层</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CenteredLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> X - X.mean()</span><br><span class="line"></span><br><span class="line">layer = CenteredLayer()</span><br><span class="line"><span class="built_in">print</span>(layer(torch.FloatTensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])))</span><br></pre></td></tr></table></figure>
<p>进而可以将层作为组件合并到构建更复杂的模型中 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">8</span>, <span class="number">128</span>), CenteredLayer())</span><br><span class="line">Y = net(torch.rand(<span class="number">4</span>, <span class="number">8</span>))</span><br><span class="line"><span class="built_in">print</span>(Y)</span><br></pre></td></tr></table></figure></p>
<h3 id="自定义带参数的层">自定义带参数的层</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_units, units</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(in_units, units))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(units, ))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        linear = torch.matmul(X, self.weight.data) + self.bias.data</span><br><span class="line">        <span class="keyword">return</span> F.relu(linear)</span><br><span class="line"></span><br><span class="line">dense = MyLinear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(dense.weight)</span><br><span class="line"><span class="built_in">print</span>(torch.rand(<span class="number">2</span>, <span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<h3 id="使用自定义层构建模型">使用自定义层构建模型</h3>
<p>同样可以使用自定义层构建模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(MyLinear(<span class="number">64</span>, <span class="number">8</span>), MyLinear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(net(torch.rand(<span class="number">2</span>, <span class="number">64</span>)))</span><br></pre></td></tr></table></figure>
<h2 id="读写文件">读写文件</h2>
<h3 id="加载和保存张量">加载和保存张量</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 储存一个张量列表，然后把它们读回内存</span></span><br><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line">y = torch.zeros(<span class="number">4</span>)</span><br><span class="line">torch.save([x, y], <span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line">x2, y2 = torch.load(<span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>((x2, y2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入或读取从字符串映射到张量的字典</span></span><br><span class="line">mydict = &#123;<span class="string">&#x27;x&#x27;</span>: x, <span class="string">&#x27;y&#x27;</span>: y&#125;</span><br><span class="line">torch.save(mydict, <span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line">mydict2 = torch.load(<span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(mydict2)</span><br></pre></td></tr></table></figure>
<h3 id="加载和保存模型参数">加载和保存模型参数</h3>
<p>我们可以通过state_dict()来得到所有的Parameter中字符串到Parameter值的一个映射，并将其保存下来实例化一个模型的备份。</p>
<h4 id="模型参数保存">模型参数保存</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.output(F.relu(self.hidden(x)))</span><br><span class="line"></span><br><span class="line">net = MLP()</span><br><span class="line">X = torch.randn(size=(<span class="number">2</span>, <span class="number">20</span>))</span><br><span class="line">Y = net(X)</span><br><span class="line">torch.save(net.state_dict(), <span class="string">&#x27;mlp.params&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="模型参数读取">模型参数读取</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clone = MLP()</span><br><span class="line">clone.load_state_dict(torch.load(<span class="string">&#x27;mlp.params&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(clone.<span class="built_in">eval</span>())</span><br><span class="line">Y_clone = clone(X)</span><br><span class="line"><span class="built_in">print</span>(Y_clone == Y)</span><br></pre></td></tr></table></figure>
<h2 id="模型训练步骤">模型训练步骤</h2>
<h3 id="基本库导入">基本库导入</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset, Dataset, DataLoader</span><br></pre></td></tr></table></figure>
<h3 id="dataset数据集构建">Dataset数据集构建</h3>
<p>在拿到一个张量数据后，首先要将其整理成Dataset的形式，首先要划分输入数据(features)和输出数据(labels)，然后将其整理为Dataset的形式，Dataset本身不提供数据的批处理或迭代功能。</p>
<h4 id="使用默认dataset形式">使用默认Dataset形式</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset = TensorDataset(features, labels)</span><br></pre></td></tr></table></figure>
<h4 id="使用自定义dataset形式">使用自定义Dataset形式</h4>
<p>除了使用内置的数据集，我们也可以自定义数据集。自定义数据集需要继承Dataset类，并实现__len__和__getitem__两个方法。在实际应用中，self.data
可以是任何类型的数据结构，只要能够按照索引获取样本即可。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, labels</span>):</span><br><span class="line">        self.features = features</span><br><span class="line">        self.labels = labels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.features)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># 返回一个包含特征和标签的元组</span></span><br><span class="line">        <span class="keyword">return</span> self.features[idx], self.labels[idx]</span><br></pre></td></tr></table></figure>
<h4 id="dataset数据查看">Dataset数据查看</h4>
<p>Dataset的访问方法为按照样本的索引访问单个样本，常用的操作为</p>
<ul>
<li><p>访问第<span
class="math inline">\(i\)</span>个样本的features和labels<br />
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset[i]</span><br></pre></td></tr></table></figure></p></li>
<li><p>访问第<span
class="math inline">\(i\)</span>个样本的features或labels<br />
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset[i][<span class="number">0</span>]</span><br><span class="line">dataset[i][<span class="number">1</span>]</span><br></pre></td></tr></table></figure></p></li>
<li><p>访问Dataset前<span
class="math inline">\(m\)</span>个样本的的features和labels<br />
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">m = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataset):</span><br><span class="line">    <span class="keyword">if</span> i &gt;= m:  <span class="comment"># 如果已经打印了100个样本，跳出循环</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Sample <span class="subst">&#123;i&#125;</span>: <span class="subst">&#123;data&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></p></li>
</ul>
<h3 id="数据预处理">数据预处理</h3>
<p>自定义数据集类通常还需要进行数据预处理，例如归一化、编码、格式化等，以确保数据适合模型训练。可通过sklearn中的方法对数据进行标准化处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line"><span class="comment"># fit_transform按数据特征列进行标准化</span></span><br><span class="line">data_normal = scaler.fit_transform(data)</span><br><span class="line"><span class="comment"># transform按数据特征列进行标准化</span></span><br><span class="line">test_data_normal = scaler.transform(testdata)</span><br><span class="line"><span class="comment"># inverse_transform逆标准化还原数据</span></span><br><span class="line">data_row = scaler.inverse_transform(data_normal)</span><br></pre></td></tr></table></figure>
<h3 id="划分训练集测试集验证集">划分训练集、测试集、验证集</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_size = <span class="built_in">int</span>(<span class="number">0.8</span> * <span class="built_in">len</span>(dataset))</span><br><span class="line">val_size = <span class="built_in">int</span>(<span class="number">0.1</span> * <span class="built_in">len</span>(dataset))</span><br><span class="line">test_size = <span class="built_in">len</span>(dataset) - train_size - val_size</span><br><span class="line">train_dataset, val_dataset, test_dataset = \</span><br><span class="line">    torch.utils.data.random_split(dataset, [train_size, val_size, test_size])</span><br></pre></td></tr></table></figure>
<h3 id="dataloader构建">DataLoader构建</h3>
<p>DataLoader提供了一种便捷的方式来以批次的形式访问数据，它在内部实现了数据的迭代，可以按批次返回数据，同时还可以进行数据打乱和多线程数据加载。</p>
<h4
id="dataset数据集转换为dataloader">Dataset数据集转换为DataLoader</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>DataLoader 中，shuffle 参数指定了是否在每个
epoch（数据加载周期）开始时对数据集进行打乱。具体来说，当 shuffle=True
时，整个数据集的顺序会被随机打乱，然后再划分为批次。</p>
<h4 id="dataloader访问方式">DataLoader访问方式</h4>
<ul>
<li><p>迭代访问，DataLoader
本身是一个迭代器，可以直接在它上面进行迭代，以按批次获取数据。每次迭代返回的是一个数据批次，通常是一个包含特征和标签的元组。<br />
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> batch_index, (features, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">    <span class="comment"># 在这里使用 features 和 labels 进行模型训练或评估</span></span><br><span class="line"><span class="keyword">for</span> features, labels <span class="keyword">in</span> data_loader:</span><br></pre></td></tr></table></figure></p></li>
<li><p>按索引访问<br />
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">single_batch = <span class="built_in">next</span>(<span class="built_in">iter</span>(data_loader)) </span><br></pre></td></tr></table></figure></p></li>
<li><p>使用 len() 函数，可以使用内置的 len() 函数来获取 DataLoader
中的批次总数。<br />
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_batches = <span class="built_in">len</span>(data_loader)</span><br></pre></td></tr></table></figure></p></li>
<li><p>结合 Subset
使用，当需要从一个完整的数据集中选择一个子集进行训练或验证时，可以使用
Subset 随机选择或指定一系列索引。<br />
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Subset</span><br><span class="line">indices = [...]  <span class="comment"># 指定的索引列表</span></span><br><span class="line">subset = Subset(full_dataset, indices)</span><br><span class="line">data_loader = DataLoader(subset, ...)</span><br></pre></td></tr></table></figure></p></li>
</ul>
<h3 id="模型训练">模型训练</h3>
<p>在每个迭代周期里，我们将完整遍历一次数据集（train_data），不停地从中获取一个小批量的输入和相应的标签。对于每⼀个小批量，我们会进⾏以下步骤:</p>
<ul>
<li>通过调用net(X)生成预测并计算损失Loss（前向传播）。</li>
<li>通过进行反向传播来计算梯度。</li>
<li>通过调用优化器来更新模型参数。</li>
</ul>
<h4 id="基本参数定义">基本参数定义</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">10</span>,  <span class="number">1e-2</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line"><span class="comment"># nn.MSELoss()默认关键字reduction=&quot;mean&quot;，求均方误差，返回一个标量</span></span><br><span class="line"><span class="comment"># reduction=&quot;none&quot;：求所有对应位置的差的平方，返回的仍然是一个和原来维度一样的tensor。</span></span><br><span class="line"><span class="comment"># reduction=&quot;sum&quot;：求所有对应位置差的平方的和，返回的是一个标量。</span></span><br></pre></td></tr></table></figure>
<h4 id="训练过程">训练过程</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_loader:</span><br><span class="line">        l = loss(net(X), y)</span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step()</span><br><span class="line">    <span class="comment"># 打印训练信息</span></span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span> Loss: <span class="subst">&#123;l.item()&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="模型调参">模型调参</h2>
<h3 id="学习率">学习率</h3>
<p>学习率决定了在每步参数更新中，模型参数有多大程度（或多快、多大步长）的调整。学习率是一个超参数。不同学习率的影响可以用下图表示</p>
<p><img src="learning-rate-effect.png" width="50%" height="50%" title="学习率影响" alt="错误无法显示"/></p>
<p>学习率还会跟优化过程的其他方面相互作用，这个相互作用可能是非线性的。小的batch
size最好搭配小的学习率，因为batch
size越小也可能有噪音，这时候就需要小心翼翼地调整参数。</p>
<h1 id="gpu部署">GPU部署</h1>
<h2 id="gpu-可用性检查">GPU 可用性检查</h2>
<ul>
<li><p>shell中查看GPU使用率 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure></p></li>
<li><p>检查GPU是否可用 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br></pre></td></tr></table></figure></p></li>
</ul>
<h2 id="指定gpu设备">指定GPU设备</h2>
<p>深度学习的所有框架都是默认在CPU上做运算的，使用GPU的话需要先指定GPU</p>
<h3 id="gpu上的张量运算">GPU上的张量运算</h3>
<ul>
<li><p>查看张量所在的设备 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(x.device)</span><br></pre></td></tr></table></figure></p></li>
<li><p>数据在GPU上储存<br />
可以采用两种方法将数据储存在GPU上 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = x.cuda()</span><br><span class="line"><span class="built_in">print</span>(x.device)</span><br><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(X.device)</span><br><span class="line"><span class="built_in">print</span>(x+X)</span><br></pre></td></tr></table></figure>
值得注意的是在GPU上做运算必须要求数据在同一个设备(GPU)上</p></li>
</ul>
<h3 id="神经网络与gpu">神经网络与GPU</h3>
<p>同样可以将NN部署在GPU上</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">net = net.to(device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net(X))</span><br><span class="line"><span class="comment"># 确认模型参数储存在同一个GPU上</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.data.device)</span><br></pre></td></tr></table></figure>
<h1 id="standard-examples">Standard Examples</h1>
<h2 id="dl-model-based-on-gpu">DL Model based on GPU</h2>
<h3 id="查看gpu设备信息并指定所用gpu">查看GPU设备信息并指定所用GPU</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset, Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> mypackage <span class="keyword">import</span> mydl</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">check_gpu_info</span>():</span><br><span class="line">    <span class="string">&#x27;显示 GPU 版本和内存信息&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> torch.cuda.is_available():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;No GPU found. CPU will be used.&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用 nvidia-smi 命令获取 GPU 版本信息</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        result = subprocess.run([<span class="string">&#x27;nvidia-smi&#x27;</span>], capture_output=<span class="literal">True</span>, text=<span class="literal">True</span>, check=<span class="literal">True</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;GPU Version Info:&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(result.stdout)</span><br><span class="line">    <span class="keyword">except</span> subprocess.CalledProcessError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Failed to run nvidia-smi command:&quot;</span>, e)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用 PyTorch API 获取 GPU 内存信息</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nGPU Memory Info:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;GPU Total Numbers: <span class="subst">&#123;torch.cuda.device_count()&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(torch.cuda.device_count()):</span><br><span class="line">        properties = torch.cuda.get_device_properties(i)</span><br><span class="line">        total_memory_gb = properties.total_memory / (<span class="number">1024</span> ** <span class="number">3</span>)</span><br><span class="line">        used_memory_gb = torch.cuda.memory_allocated(i) / (<span class="number">1024</span> ** <span class="number">3</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;GPU <span class="subst">&#123;i&#125;</span>:&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;\tName: <span class="subst">&#123;properties.name&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;\tTotal Memory: <span class="subst">&#123;total_memory_gb:<span class="number">.2</span>f&#125;</span> GB&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;\tUsed Memory: <span class="subst">&#123;used_memory_gb:<span class="number">.2</span>f&#125;</span> GB&quot;</span>)</span><br><span class="line">check_gpu_info()</span><br><span class="line">device = torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;<span class="number">0</span>&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="数据加载">数据加载</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                            Data Preprocessing</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line">filename = <span class="string">rf&#x27;.\Dataset\Traindata(r-u_to_k).xlsx&#x27;</span></span><br><span class="line">data = pd.read_excel(filename, sheet_name=<span class="string">&quot;Sheet1&quot;</span>, header=<span class="literal">None</span>)</span><br><span class="line">data = torch.tensor(data[<span class="number">1</span>:].values.astype(np.float32))</span><br><span class="line">data = data[:, :<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<h3 id="数标准化">数标准化</h3>
<p>数据标准化是为了使得模型更快收敛，通常用sklearn中的方法进行数据标准化，sklearn处理的对象是numpy数组，因此在使用前要注意将数据类型转换为numpy</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                            Data Standardization</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">data = torch.tensor(scaler.fit_transform(data.numpy()), dtype=torch.float32)</span><br></pre></td></tr></table></figure>
<h3 id="构建dataset和dataloader">构建Dataset和Dataloader</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#               Building corresponding Dataset and Dataloader</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment"># Building Dataset</span></span><br><span class="line">dataset_st = TensorDataset(data_st[:, <span class="number">0</span>:<span class="number">2</span>], data_st[:, [<span class="number">2</span>]])</span><br><span class="line">train_size = <span class="built_in">int</span>(<span class="number">0.8</span> * <span class="built_in">len</span>(dataset_st))</span><br><span class="line">val_size = <span class="built_in">int</span>(<span class="number">0.1</span> * <span class="built_in">len</span>(dataset_st))</span><br><span class="line">test_size = <span class="built_in">len</span>(dataset_st) - train_size - val_size</span><br><span class="line">train_dataset_st, val_dataset_st, test_dataset_st = torch.utils.data.random_split(dataset_st, [train_size, val_size, test_size])</span><br><span class="line"><span class="comment"># Building Dataloader</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">train_loader = DataLoader(train_dataset_st, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">val_loader = DataLoader(val_dataset_st, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset_st, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="构建模型并进行初始化">构建模型并进行初始化</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                      Building Model and Inilization</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">2</span>, <span class="number">128</span>), nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">128</span>, <span class="number">32</span>), nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">32</span>, <span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">xavier</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line">net.apply(xavier)</span><br></pre></td></tr></table></figure>
<h3 id="在gpu上进行模型训练">在GPU上进行模型训练</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                            Model Training</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line">num_epochs, lr = <span class="number">50</span>, <span class="number">1e-2</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line">net.to(device=device)</span><br><span class="line">net.train()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line">        l = loss(net(X), y)</span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span> Loss: <span class="subst">&#123;l.item()&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="模型评估与预测">模型评估与预测</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                            Model Evaluation</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line">net.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    y_pred = net(test_dataset_st[:][<span class="number">0</span>].cuda())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pred_test_data_st = torch.cat([test_dataset_st[:][<span class="number">0</span>], y_pred.cpu()], dim=<span class="number">1</span>)</span><br><span class="line">test_data_st = torch.cat([test_dataset_st[:][<span class="number">0</span>], test_dataset_st[:][<span class="number">1</span>]], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="数据可视化">数据可视化</h3>
<p>matplotlib函数基于numpy数组进行处理，当有tensor会将自动转换为numpy格式，但是他只能处理cpu的数据，因此模型预测的数据结果需要移动到cpu上。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                            Figure Plotting</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line">Number = np.arange(<span class="built_in">len</span>(y_pred)) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">1</span>, figsize=(<span class="number">7</span>, <span class="number">5</span>))</span><br><span class="line">ax.scatter(Number,test_data_st[:, <span class="number">2</span>], label=<span class="string">&quot;Ture data&quot;</span>, c=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">ax.scatter(Number, pred_test_data_st[:, <span class="number">2</span>], label=<span class="string">&quot;DL predictions&quot;</span>, c=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&quot;u235(Standarization)&quot;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&quot;keff(Standarization)&quot;</span>)</span><br><span class="line">ax.legend()</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">1</span>, figsize=(<span class="number">7</span>, <span class="number">5</span>))</span><br><span class="line">ax.scatter(Number, scaler.inverse_transform(test_data_st.numpy())[:, <span class="number">2</span>], label=<span class="string">&quot;Ture data&quot;</span>, c=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">ax.scatter(Number, scaler.inverse_transform(pred_test_data_st.numpy())[:, <span class="number">2</span>], label=<span class="string">&quot;DL predictions&quot;</span>, c=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&quot;u235&quot;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&quot;keff&quot;</span>)</span><br><span class="line">ax.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="high-dimensional-linear-regression">High Dimensional Linear
Regression</h2>
<h3 id="数据产生">数据产生</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机种子以获得可重复的结果</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">num_inputs = <span class="number">3</span></span><br><span class="line">num_samples = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成输入数据，这里我们假设每个输入变量的范围是0到10</span></span><br><span class="line">X1 = np.random.uniform(low=<span class="number">0</span>, high=<span class="number">4</span>, size=num_samples)</span><br><span class="line">X2 = np.random.uniform(low=<span class="number">0</span>, high=<span class="number">4</span>, size=num_samples)</span><br><span class="line">X3 = np.random.uniform(low=<span class="number">0</span>, high=<span class="number">4</span>, size=num_samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成噪声项，这里我们假设噪声项是正态分布的</span></span><br><span class="line">epsilon = np.random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=num_samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算输出变量Y，包含非线性项</span></span><br><span class="line">Y = X1**<span class="number">2</span> + <span class="number">5</span>*np.sin(X2) - X3**<span class="number">3</span> + epsilon</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个DataFrame</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;X1&#x27;</span>: X1,</span><br><span class="line">    <span class="string">&#x27;X2&#x27;</span>: X2,</span><br><span class="line">    <span class="string">&#x27;X3&#x27;</span>: X3,</span><br><span class="line">    <span class="string">&#x27;Y&#x27;</span>: Y</span><br><span class="line">&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line">df.to_csv(<span class="string">&#x27;./Dataset/Traindata(r1-r3-u_to_k).csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h3 id="回归模型训练">回归模型训练</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset, Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> mypackage <span class="keyword">import</span> mydl</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                      Building Model and Inilization</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">3</span>, <span class="number">128</span>), nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">128</span>, <span class="number">32</span>), nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">32</span>, <span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line">mydl.init_cnn(net)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                            Data Preprocessing</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line">filename = <span class="string">rf&#x27;./Dataset/Traindata(r1-r3-u_to_k).csv&#x27;</span></span><br><span class="line">data = mydl.read_data(filename, use_header=<span class="number">0</span>)</span><br><span class="line">data= torch.tensor(data.to_numpy(dtype=np.float32))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                            Data Standardization</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line">normalizer = mydl.Normalizer()</span><br><span class="line">normalizer.fit(data)</span><br><span class="line">data_st = normalizer.transform(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#               Building corresponding Dataset and Dataloader</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment"># Dataset Building</span></span><br><span class="line">dataset_st = TensorDataset(data_st[:, <span class="number">0</span>:<span class="number">3</span>], data_st[:, [<span class="number">3</span>]])</span><br><span class="line">train_size = <span class="built_in">int</span>(<span class="number">0.8</span> * <span class="built_in">len</span>(dataset_st))</span><br><span class="line">val_size = <span class="built_in">int</span>(<span class="number">0.1</span> * <span class="built_in">len</span>(dataset_st))</span><br><span class="line">test_size = <span class="built_in">len</span>(dataset_st) - train_size - val_size</span><br><span class="line">train_dataset_st, val_dataset_st, test_dataset_st = \</span><br><span class="line">    torch.utils.data.random_split(dataset_st, [train_size, val_size, test_size])</span><br><span class="line"><span class="comment"># Dataloader Building</span></span><br><span class="line">batch_size = <span class="number">200</span></span><br><span class="line">train_loader = DataLoader(train_dataset_st, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">val_loader = DataLoader(val_dataset_st, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset_st, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                            Model Training</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line">device = mydl.try_gpu()</span><br><span class="line">num_epochs, lr = <span class="number">300</span>, <span class="number">1e-3</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line">net.to(device=device)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line"></span><br><span class="line">Epoch, train_losses, val_losses= [], [], []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    net.train()</span><br><span class="line">    <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line">        l = loss(net(X), y)</span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        train_loss = mydl.evaluate_loss(net, train_loader, loss)</span><br><span class="line">        val_loss = mydl.evaluate_loss(net, test_loader, loss)</span><br><span class="line">        Epoch.append(epoch + <span class="number">1</span>)</span><br><span class="line">        train_losses.append(train_loss)</span><br><span class="line">        val_losses.append(val_loss)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span> Loss: <span class="subst">&#123;l.item()&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    test_inputs = test_dataset_st[:][<span class="number">0</span>].to(device)</span><br><span class="line">    test_targets = test_dataset_st[:][<span class="number">1</span>].to(device)</span><br><span class="line">    test_outputs = net(test_inputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                            Data Anti-normalization</span></span><br><span class="line"><span class="comment">############################################################################### </span></span><br><span class="line">test_targets = test_targets * normalizer.std[<span class="number">3</span>] + normalizer.mean[<span class="number">3</span>]</span><br><span class="line">test_outputs = test_outputs * normalizer.std[<span class="number">3</span>] + normalizer.mean[<span class="number">3</span>]</span><br><span class="line"><span class="built_in">print</span>(normalizer.std, normalizer.mean)</span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment">#                               Figure</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment"># Loss in every Eopch</span></span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">1</span>, figsize=(<span class="number">6</span>, <span class="number">5</span>))</span><br><span class="line">ax.plot(Epoch, train_losses, label=<span class="string">&#x27;Train Loss&#x27;</span>)</span><br><span class="line">ax.plot(Epoch, val_losses, label=<span class="string">&#x27;Validation Loss&#x27;</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;MSE Loss&#x27;</span>)</span><br><span class="line">ax.set_title(<span class="string">&#x27;Loss Curves&#x27;</span>)</span><br><span class="line">ax.set_yscale(<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">ax.legend(framealpha=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prediction Accuracy</span></span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">1</span>, figsize=(<span class="number">6</span>, <span class="number">5</span>))</span><br><span class="line">ax.scatter(test_targets.to(mydl.cpu()), test_outputs.to(mydl.cpu()), s=<span class="number">5</span>, c=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">m = torch.<span class="built_in">max</span>(torch.<span class="built_in">abs</span>(test_targets)).to(mydl.cpu())</span><br><span class="line">ax.plot([-m, m], [-m, m])</span><br><span class="line">ax.set_xlim([-m, m])</span><br><span class="line">ax.set_ylim([-m, m])</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;Target Values&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;Prediction Values&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="数据集类型">数据集类型</h1>
<h2 id="dataframe数据集">DataFrame数据集</h2>
<p>pandas 的 read_excel 函数用于读取Excel文件，并将数据加载到一个
DataFrame 对象中。DataFrame 对象本身不会在数据中显示行号和列号，但它们是
DataFrame 的一部分，打印时会看到对应的行号和列号。</p>
<h1 id="cnn-convolutional-neural-network">CNN (Convolutional Neural
Network)</h1>
<p>适合于计算机视觉的神经⽹络架构基于两个原则：</p>
<ol type="1">
<li><p>平移不变性（translation
invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层
应该对相同的图像区域具有相似的反应。</p></li>
<li><p>局部性（locality）：神经网络的前面几层应该只探索输⼊图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</p></li>
</ol>
<h2 id="什么是卷积">什么是卷积？</h2>
<h3 id="从蝴蝶效应说起">从蝴蝶效应说起</h3>
<div class="group-picture"><div class="group-picture-row"><div class="group-picture-column"><img src="convolution-picture1.png" width="40%" title="卷积说明1" alt="错误无法显示"  /></div><div class="group-picture-column"><img src="convolution-picture2.png" width="40%" title="卷积说明2" alt="错误无法显示" /></div></div></div>
<p>为了更好的理解卷积，考虑这样一个例子，有一只蝴蝶不停地在扇动翅膀，不同时刻扇动翅膀的快慢不同，因此其产生的破坏效果也不同，其破坏效果的影响力我们用左图来描述，并且某时刻产生的破坏效果不会马上消失，而是随时间逐渐衰减，其衰减效果如右图所示。</p>
<p>现在解决一个问题，求一下<span
class="math inline">\(t\)</span>时刻感受到的破坏力？这个问题也很简单，把之前所有时刻对<span
class="math inline">\(t\)</span>时刻的影响加起来就行了，本质上也就是求这个式子:</p>
<p><span class="math display">\[
\int_0^t {f\left( x \right)g\left( {t - x} \right)dx}
\]</span></p>
<p>这也就是我们后面要说的卷积。</p>
<h3 id="卷积卷积-为什么叫卷积">卷积、卷积 为什么叫“卷积”？</h3>
<p>我们给出所谓的卷积的定义，也就是这个式子 <span
class="math display">\[
\int_{ - \infty }^\infty {f\left( \tau \right)g\left( {x - \tau}
\right)d\tau}
\]</span></p>
<p>从左边图可以看出，图中每一条连线都对应着一对<span
class="math inline">\(f(x)\)</span>和<span
class="math inline">\(g(t-x)\)</span>的相乘，把所有的值加起来，就得到了我们所谓的卷积。</p>
<div class="group-picture"><div class="group-picture-row"><div class="group-picture-column"><img src="convolution-picture3.png" width="40%" title="卷积说明3" alt="错误无法显示"  /></div><div class="group-picture-column"><img src="convolution-picture4.png" width="40%" title="卷积说明4" alt="错误无法显示" /></div></div></div>
<p>此时如果我们将<span
class="math inline">\(g(t)\)</span>函数翻转一下，会发现卷积实际上就是将函数翻转后对应位置相乘求和，这也就是为什么叫卷积。</p>
<h3 id="什么是图像的卷积操作">什么是图像的卷积操作</h3>
<p>如果我们把视野放得更广一点，在上面蝴蝶效应的例子中，如果影响力的变化不是随时间改变，而是随着空间距离而改变的，也就是说对<span
class="math inline">\(x\)</span>位置产生影响的是其他很多位置，那么回到开始的问题，什么是图像的卷积操作？图像的卷积操作实际上就是去看图像上其他很多像素点对一个像素点是如何产生影响的，举个例子</p>
<p><img src="convolution-picture5.png" width="90%" title="平滑卷积核操作" alt="错误无法显示" /></p>
<p>可以看到这个例子中，卷积核规定了周围的像素点对当前像素点的影响，当前在经过一个与平滑卷积核进行卷积操作后，对图像进行了平滑，也就是说在这个卷积核下考虑周围像素点对某个像素点影响，遍历整个图片后，得到的结果是每个像素点更平滑。</p>
<div class="group-picture"><div class="group-picture-row"><div class="group-picture-column"><img src="convolution-picture6.png" width="40%" title="卷积说明6" alt="错误无法显示" /></div><div class="group-picture-column"><img src="convolution-picture7.png" width="40%" title="卷积说明7" alt="错误无法显示" /></div><div class="group-picture-column"><img src="convolution-picture8.png" width="40%" title="卷积说明8" alt="错误无法显示" /></div></div></div>
<p>我们现在考虑<span
class="math inline">\(g(m,n)\)</span>这个卷积核下，<span
class="math inline">\((x,y)\)</span>周围的像素点对<span
class="math inline">\((x,y)\)</span>这个像素点的影响效果，根据卷积的定义，可以得到</p>
<p><span class="math display">\[
\begin{array}{l}
\begin{aligned}
f\left( {x,y} \right)g\left( {m,n} \right) &amp;= \sum {f\left( {x,y}
\right)g\left( {m - x,n - y} \right)} \\
&amp; = f\left( {x - 1,y - 1} \right)g\left( {1,1} \right) + f\left(
{x,y - 1} \right)g\left( {0,1} \right) + f\left( {x + 1,y - 1}
\right)g\left( { - 1,1} \right)\\
&amp; + f\left( {x - 1,y} \right)g\left( {1,0} \right) + f\left( {x,y}
\right)g\left( {0,0} \right) + f\left( {x + 1,y} \right)g\left( { - 1,0}
\right)\\
&amp; + f\left( {x - 1,y + 1} \right)g\left( {1, - 1} \right) + f\left(
{x,y + 1} \right)g\left( {0, - 1} \right) + f\left( {x + 1,y + 1}
\right)g\left( { - 1, - 1} \right)
\end{aligned}
\end{array}
\]</span></p>
<p><img src="convolution-picture9.png" width="90%" title="卷积说明8" alt="错误无法显示" /></p>
<p>同样我们发现它仍然是卷着乘的，我们我们将它翻转<span
class="math inline">\(180^\circ\)</span>后会发现是对应位置相乘，实际上后来我们CNN中用的卷积核就是翻转后的结果，它可以直接扣在图像上直接相乘再相加，但它本质上仍然是一个卷积运算。</p>
<h3 id="卷积神经网络与卷积">卷积神经网络与卷积</h3>
<p>卷积神经网络主要是用来干图像识别的。</p>
<h3 id="参考链接">参考链接</h3>
<iframe src="//player.bilibili.com/player.html?aid=418492547&amp;bvid=BV1VV411478E&amp;cid=353587154&amp;p=1&amp;autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true">
</iframe>
<h2 id="再谈全连接层与卷积">再谈全连接层与卷积</h2>
<p>在之前处理图片时，我们将一张图片reshape成一个1D向量来处理，现在我们考虑它的空间信息，对于一个图片它都包含一部分空间信息，因此我们选择用矩阵(宽度,
高度)去描述神经网络的输入<span
class="math inline">\(x\)</span>和输出<span
class="math inline">\(h\)</span>，对应地我们可以将我们的权重变为4D张量，此时有对应的变换关系</p>
<p><span class="math display">\[
{ h_{ij} } = \sum\limits_{k,l} { {w_{ijkl} } {x_{kl} } }
\]</span></p>
<p>接下来我们对<span
class="math inline">\(w\)</span>做一个重新的索引，使得<span
class="math inline">\({v_{ijab} } = {w_{ij(i + a)(j + b)}
}\)</span>，此时有</p>
<p><span class="math display">\[
\begin{equation}
{ h_{ij} } = \sum\limits_{k,l} { {w_{ijkl} } {x_{kl} } }  =
\sum\limits_{a,b} { {v_{ijab} } {x_{(i + a)(j + b)} } }
\label{convolution}
\end{equation}
\]</span></p>
<p>这个式子可以看成<span class="math inline">\((i,
j)\)</span>位置的输出<span class="math inline">\(h\)</span>是由<span
class="math inline">\((i, j)\)</span>位置的周边<span
class="math inline">\((i+a, j+b)\)</span>的一些输入<span
class="math inline">\(x\)</span>在权重<span class="math inline">\({\bf
v}\)</span>下所共同影响而得到的。下面我们根据我们的基本原则引出我们的卷积：</p>
<h3 id="平移不变性">平移不变性</h3>
<p>在方程<span
class="math inline">\(\eqref{convolution}\)</span>中，权重<span
class="math inline">\({\bf
v}\)</span>本质上就是一组识别器，而这时如果<span
class="math inline">\((i, j)\)</span>发生变化(对应平移)，这时权重<span
class="math inline">\(v_{ijab}\)</span>也会发生变化，使得输出结果做出对应的改变，而根据平移不变性的要求，不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应。所以我们并不希望发生平移时（也就是改变<span
class="math inline">\(i, j\)</span>时）输出也随<span
class="math inline">\(i, j\)</span>的变化而改变，因此我们说<span
class="math inline">\({\bf v}\)</span>并不该依赖于<span
class="math inline">\((i, j)\)</span>，进而我们有<span
class="math inline">\(v_{ijab}=v_{ab}\)</span>，故而得到：</p>
<p><span class="math display">\[
{h_{ij} } = \sum\limits_{a,b} { {v_{ab} } {x_{(i + a)(j + b)} } }
\]</span></p>
<p>这就是所谓的二维交叉相关。</p>
<h3 id="全连接层-vs-卷积">全连接层 VS 卷积</h3>
<ul>
<li><p>全连接层最大的问题在于全连接层第一层的权重参数矩阵的行取决于输入的维度，特别是在处理图像问题的时候，如果把每个像素点作为一个维度，这个模型会非常大，容易爆掉。</p></li>
<li><p>相比于全连接层，卷积的优势在于不管它的输入是多大，卷积核的大小总是固定的，这样就极大地降低了模型复杂度。</p></li>
</ul>
<h3 id="局部性">局部性</h3>
<p>局部性的意思是说当我们评估<span
class="math inline">\(h_{ij}\)</span>时，我们不应该使用远离<span
class="math inline">\(x_{ij}\)</span>的参数，因此我们选择当<span
class="math inline">\(|a|, |b| &gt; \Delta\)</span>时，使<span
class="math inline">\(v_{ab}=0\)</span>，此时有： <span
class="math display">\[
{h_{ij} } = \sum\limits_{a =  - \Delta }^\Delta  {\sum\limits_{b =  -
\Delta }^\Delta  { {v_{ab} } {x_{(i + a)(j + b)} } } }
\]</span></p>
<p>因此我们对全连接层使用平移不变性和局部性就得到了我们的卷积层，换句话说就是卷积是一个特殊的全连接层。</p>
<h2 id="卷积层">卷积层</h2>
<p>卷积层本质是将输入和核矩阵进行交叉相关，加上偏移后得到输出，核矩阵和偏移都是可学习的参数，核矩阵的大小是超参数。</p>
<h3 id="二维卷积层">二维卷积层</h3>
<p><img src="2D_convolution.png" width="50%" height="50%" title="二维卷积层示例" alt="错误无法显示"/></p>
<p>在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。当卷积窗口滑动到新一个位置时，包含在该窗口中的部分张量与卷积核张量进行按元素相乘，得到的张量再求和得到⼀个单⼀的标量值，由此我们得出了这⼀位置的输出张量值。</p>
<p>卷积核的宽度和高度大于1，而卷积核只与图像中每个大小完全适合的位置进行互相关运算，这一过程的数学表述可以表示为</p>
<p><span class="math display">\[
{\bf Y} = {\bf X} \star {\bf W} + b
\]</span></p>
<ul>
<li>输入<span class="math inline">\({\bf X}\)</span>: <span
class="math inline">\(n_{h} \times n_{w}\)</span></li>
<li>卷积核<span class="math inline">\({\bf W}\)</span>: <span
class="math inline">\(k_{h} \times k_{w}\)</span></li>
<li>偏差: <span class="math inline">\(b \in \mathbb{R}\)</span></li>
<li>输出<span class="math inline">\({\bf Y}\)</span>: <span
class="math inline">\((n_{h} - k_{h} + 1) \times (n_{w} - k_{w} +
1)\)</span></li>
</ul>
<p>其中<span class="math inline">\(\star\)</span>表示交叉相关运算，<span
class="math inline">\({\bf W}\)</span>和<span
class="math inline">\(b\)</span>是可学习的参数。</p>
<h3 id="交叉相关-vs-卷积">交叉相关 VS 卷积</h3>
<ul>
<li>二维交叉相关</li>
</ul>
<p><span class="math display">\[
{h_{ij} } = \sum\limits_{a,b} { {v_{ab} } {x_{(i + a)(j + b)} } }
\]</span></p>
<ul>
<li>二维卷积</li>
</ul>
<p><span class="math display">\[
{h_{ij} } = \sum\limits_{a,b} { {v_{-a, -b} } {x_{(i + a)(j + b)} } }
\]</span></p>
<p>它们唯一的区别是卷积在索引<span
class="math inline">\(w\)</span>的时候是翻转的，相当于先翻转<span
class="math inline">\(180^\circ\)</span>再做交叉相关操作，这也是为什么称之为“卷积”。由于对称性的存在，在实际使用过程中它们没有任何区别，用二维交叉学出来的东西翻转过来就是用二维卷积学出来的东西。</p>
<h3
id="卷积层的填充padding与步幅stride">卷积层的填充padding与步幅stride</h3>
<p>填充和步幅是卷积层的超参数，填充是在输入周围添加额外的行和列，来控制输出形状的减少量，步幅是每次滑动核窗口时的步长，可以成倍的减少输出形状。</p>
<h4 id="填充">填充</h4>
<p>经过一层卷积操作后，<span class="math inline">\(n_{h} \times
n_{w}\)</span>的输入减小为为<span class="math inline">\((n_{h} - k_{h} +
1) \times (n_{w} - k_{w} +
1)\)</span>的输出，如果想做比较深的神经网络，我们就需要对其进行填充</p>
<p><img src="convolution-fill.png" width="40%" height="50%" title="填充操作" alt="错误无法显示"/></p>
<p>通过在输入的四周添加额外的行和和列，可以使得输出形状保持不变</p>
<ul>
<li>填充<span class="math inline">\(p_h\)</span>行和<span
class="math inline">\(p_w\)</span>列，输出形状为<span
class="math inline">\((n_{h} - k_{h} + p_{h} + 1) \times (n_{w} - k_{w}
+p_{w} + 1)\)</span></li>
<li>为了保持形状不变，通常取<span class="math inline">\(p_{h} = k_{h} -
1\)</span>, <span class="math inline">\(p_{w} = k_{w} - 1\)</span>
<ul>
<li>当<span
class="math inline">\(k_{h}\)</span>为奇数时：在上下两侧填充<span
class="math inline">\(p_{h} / 2\)</span></li>
<li>当<span
class="math inline">\(k_{h}\)</span>为偶数时：在上侧填充<span
class="math inline">\(\lceil p_{h} / 2 \rceil\)</span>, 在下侧填充<span
class="math inline">\(\lfloor p_{h} / 2 \rfloor\)</span></li>
</ul></li>
</ul>
<h4 id="步幅">步幅</h4>
<p>当输入一个比较大的图片时，可以通过调整步幅的大小来减小输出</p>
<ul>
<li>给定高度<span class="math inline">\(s_h\)</span>和宽度<span
class="math inline">\(s_w\)</span>的步幅，输出形状为<span
class="math inline">\(\lfloor (n_{h} - k_{h} + p_{h}) / s_{h} + 1
\rfloor \times \lfloor (n_{w} - k_{w} +p_{w}) / s_{h} + 1
\rfloor\)</span></li>
<li>如果<span class="math inline">\(p_{h} = k_{h} - 1\)</span>, <span
class="math inline">\(p_{w} = k_{w} - 1\)</span>，则输出形状为<span
class="math inline">\(\lfloor (n_{h} - 1) / s_{h} + 1 \rfloor \times
\lfloor (n_{w} - 1) / s_{h} + 1 \rfloor\)</span></li>
<li>如果输入高度<span class="math inline">\(n_{h}\)</span>和宽度<span
class="math inline">\(n_{w}\)</span>可以被步幅整除，则输出形状为<span
class="math inline">\((n_{h}/s_{h}) \times (n_{w}/s_{w})\)</span></li>
</ul>
<h3 id="多输入和多输出通道">多输入和多输出通道</h3>
<p>对于彩色图像来讲可能有RGB三个通道，如果直接将其转换为灰度则会丢失信息。</p>
<h4 id="多输入通道">多输入通道</h4>
<ul>
<li>输入<span class="math inline">\({\bf X}\)</span>: <span
class="math inline">\({c_i} \times {n_h} \times {n_w}\)</span></li>
<li>核<span class="math inline">\({\bf W}\)</span>: <span
class="math inline">\({c_i} \times {k_h} \times {k_w}\)</span></li>
<li>输出<span class="math inline">\({\bf Y}\)</span>: <span
class="math inline">\({m_h} \times {m_w}\)</span></li>
</ul>
<p><span class="math display">\[
{\bf Y} = \sum\limits_{i = 0}^{ {c_i} } { { {\bf X}_{i,:,:} } \star {
{\bf W}_{i,:,:} } }
\]</span></p>
<p>多输入通道每个通道都有一个卷积核，结果是所有通道卷积结果的和。</p>
<h4 id="多输出通道">多输出通道</h4>
<ul>
<li>输入<span class="math inline">\({\bf X}\)</span>: <span
class="math inline">\({c_i} \times {n_h} \times {n_w}\)</span></li>
<li>核<span class="math inline">\({\bf W}\)</span>: <span
class="math inline">\({c_o} \times {c_i} \times {k_h} \times
{k_w}\)</span></li>
<li>输出<span class="math inline">\({\bf Y}\)</span>: <span
class="math inline">\({c_o} \times {m_h} \times {m_w}\)</span></li>
</ul>
<p><span class="math display">\[
{\bf Y}_{i,:,:}= {\bf X} \star { {\bf W}_{i,:,:,:} } \quad {\rm for}
\quad i=1,\ldots,c_o
\]</span></p>
<p>无论有多少输入通道，到目前为止我们只用到了单输出通道，但实际上我们可以有多个三维卷积核，每个核都可以生成一个输出通道。</p>
<h4 id="为什么使用多输入和多输出">为什么使用多输入和多输出</h4>
<ul>
<li>对于每一个输出通道，它都有一个卷积核去识别特定的模式，</li>
</ul>
<p><img src="cat.png" width="80%" height="50%" title="猫图像识别" alt="错误无法显示"/></p>
<ul>
<li>输入通道核识别并组合输入中的模式<br />
当把输出通道的结果传给下一次层的输入时，下一通道会进一步进行特征提取并进行组合，得到一个组合的模式识别。</li>
</ul>
<h4 id="times-1-卷积层"><span class="math inline">\(1 \times 1\)</span>
卷积层</h4>
<p><span class="math inline">\(k_h = k_w =
1\)</span>这个卷积层是一个特殊的卷积层，它不识别空间模式，并不会提取空间信息，而只是用来融合通道。本质上它相当于输入形状为<span
class="math inline">\(n_h n_w \times c_i\)</span>的<span
class="math inline">\({\bf X}\)</span>，权重为<span
class="math inline">\(c_i \times c_o\)</span>的<span
class="math inline">\({\bf K}\)</span>的全连接层。</p>
<h4 id="多输入和多输出通道总结">多输入和多输出通道总结</h4>
<ul>
<li>输入<span class="math inline">\({\bf X}\)</span>: <span
class="math inline">\({c_i} \times {n_h} \times {n_w}\)</span></li>
<li>核<span class="math inline">\({\bf W}\)</span>: <span
class="math inline">\({c_i} \times {k_h} \times {k_w}\)</span></li>
<li>偏差<span class="math inline">\({\bf B}\)</span>: <span
class="math inline">\(c_o \times c_i\)</span></li>
<li>输出<span class="math inline">\({\bf Y}\)</span>: <span
class="math inline">\({m_h} \times {m_w}\)</span></li>
<li>计算复杂度: <span class="math inline">\(O (c_i c_o k_h k_w m_h
m_w)\)</span></li>
</ul>
<h2 id="池化层-pooling">池化层 Pooling</h2>
<p>通常当我们处理图像时，我们希望逐渐降低隐藏表示的空间分辨率、聚集信息，这样随着我们在神经网络中
层叠的上升，每个神经元对其敏感的感受野（输入）就越大。</p>
<p>而的机器学习任务通常会跟全局图像的问题有关（例如：图像是否包含⼀只猫呢？），所以我们最后⼀
层的神经元应该对整个输入的全局敏感。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表
示的目标，同时将卷积图层的所有优势保留在中间层。</p>
<p>池化层最终返回窗口中最大或平均值，它同样有窗口大小、填充和步幅作为超参数，能够缓解卷积层对位置的敏感性。</p>
<p>在Pytorch中默认步幅大小与池化窗口相同。</p>
<h3 id="填充步幅和多通道">填充、步幅和多通道</h3>
<ul>
<li>池化层和卷积层类似，都具体填充和步幅</li>
<li>没有可学习的参数，不需要学kernel</li>
<li>在每个输入通道应用池化层以获得相应的输出通道，它只是处理一下数据</li>
<li>输出通道数 = 输入通道数</li>
</ul>
<h3 id="最大池化层">最大池化层</h3>
<p>输出每个信号中最强的模式信号</p>
<h3 id="平均池化层">平均池化层</h3>
<p>将最大池化层中的“最大”操作替换为“平均”</p>
<h2 id="lenet-经典卷积神经网络">LeNet 经典卷积神经网络</h2>
<p><img src="LeNet.png" width="80%" height="50%" title="LeNet神经网络" alt="错误无法显示"/></p>
<p>LeNet是早期成功的神经网络，先使用卷积层来学习图片空间信息，然后使用全连接层转到类别空间。</p>
<h3 id="lenet-pytorch实现">LeNet Pytorch实现</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Reshape</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    </span><br><span class="line">net = torch.nn.Sequential(</span><br><span class="line">    Reshape(), nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.ReLU(), </span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), </span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.ReLU(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), nn.Flatten(), </span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.ReLU(), </span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.ReLU(), </span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;output shape: \t&#x27;</span>, X.shape)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy_gpu</span>(<span class="params">net, data_iter, device=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">            X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            X = X.to(device)</span><br><span class="line">        y = y.to(device)</span><br><span class="line">        metric.add(d2l.accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">0.1</span>, <span class="number">10</span></span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>
<h2 id="vgg块">VGG块</h2>
<p>VGG块的想法是n个卷积层和1一个池化层把封装成块。</p>
<ul>
<li>VGG使用可重复使用的卷积块来构建深度卷积神经网络</li>
<li>不同卷积块的个数和超参数可以得到不同复杂程度的变种</li>
</ul>
<h2 id="nin网络">NiN网络</h2>
<p>无论是 LeNet 还是 AlexNet
网络，在卷积层输出的最后，其最后都通过一个Flatten层来展平卷积层的输出，然后再加两个全连接层进行分类预测，但实际是这个全连接层是十分占内存的。我们知道一个
<span class="math inline">\(1 \times 1\)</span>
的卷积层可以等效为一个全连接层</p>
<p><img src="1-1-convolution.png" width="80%" title="1乘1卷积" alt="错误无法显示"  />
<img src="NiN-block.png" width="30%" title="NiN架构" alt="错误无法显示" /></p>
<p>通过用 <span class="math inline">\(1 \times 1\)</span>
的卷积层代替全连接层，减少了模型大小，最终得到NiN的架构为</p>
<p><img src="NiN-Networks.png" width="80%" title="NiN Networks" alt="错误无法显示"  /></p>
<ul>
<li>无全连接层</li>
<li>交替使用NiN块和步幅为2的最大池化层逐步减小高宽
<ul>
<li>增大通道数</li>
</ul></li>
<li>最后使用全局平均池化层得到输出
<ul>
<li>其输入通道数是类别数</li>
</ul></li>
<li>NiN块使用卷积层加两个1x1卷积层
<ul>
<li>后者对每个像素增加了非线性性</li>
</ul></li>
<li>NiN使用全局平均池化层来替代VGG和AlexNet中的全连接层
<ul>
<li>不容易过拟合，更少的参数个数</li>
</ul></li>
</ul>
]]></content>
      <categories>
        <category>Math</category>
        <category>Deep-Learning</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>日记本</title>
    <url>/2024/05/03/%E6%97%A5%E8%AE%B0%E6%9C%AC/</url>
    <content><![CDATA[<center>
<b><font face=Kaiti color=BlueGreen>喜欢的事情慢慢做，喜欢的生活努力过</font></b>
</center>
<span id="more"></span>
<h1 id="年">2024年</h1>
<h2 id="may">May</h2>
<h3 id="月3日">5月3日</h3>
<center>
<b><font face=STXingkai color=MediumPurple>在心里种花，人生才不会荒芜</font></b>
</center>
<p>今天去工位做了英语作业，感觉头都要大了，哎我这蹩脚的英语水平，日后还是好好学英语吧。下午弹了一会儿琴，虽然说前半段会弹了，但是感觉是全凭肌肉记忆弹出来的，认谱太慢了，所以还得多练，感觉自己弹得很糊，看来高抬指的练习还是很重要的。</p>
<h3 id="月4日">5月4日</h3>
<center>
<b><font face=STXingkai color=MediumPurple>心生怜悯是我，无能为力也是我</font></b>
</center>
<p>又是阴雨绵绵的一天，果然是江南水乡，要是我现在会弹一首天青色等烟雨就好了（做梦中哈哈哈哈……）</p>
<p>上午看了快船和独行侠的比赛，首先恭喜独行侠，我文子真的好nb，感觉他今年真的在认真打球啊，下半场单打这块真的就是无差别单练地在挑战对方的防线，真的就是打谁都是打，如果今年欧文夺冠，我真不敢想道耐克会怎么样哈哈哈。</p>
<p>晚上再次试着装了一下Office，还是报错，气得我全部卸载干净了，幸运的是这次终于装上了，开心~
一会回去再练会儿琴，感觉一时半会技术是没啥提高了，每天坚持练一点，慢慢来吧！</p>
<h3 id="月6日">5月6日</h3>
<center>
<b><font face=STXingkai color=MediumPurple>允许一切发生，生活不过是见招拆招</font></b>
</center>
<p>没有什么进展，大概了解了一下方法，可是手头什么数据都没有，要做什么也不清楚。明天记得要早起，还有一天的课等着我，现在既然没什么思路的话，明天一天的时间先看看别人之前都做了些什么吧。</p>
<p>上周的目标《天空之城》好得也算实现了，能顺着弹下来，打好基础很重要，今晚回去继续练一下琴。现在去操场跑个步，好几天没动了，也不知道自己现在啥水平，开个三公里，顺顺腿啦~</p>
<h3 id="月7日">5月7日</h3>
<center>
<b><font face=STXingkai color=MediumPurple>抬头一片茫茫月，是我生生不死心</font></b>
</center>
<p>昨天晚上很奇怪，跑步的时候很难受，直接给我干低血糖了，不知道是不是很久没锻炼了，可能也是因为没吃晚饭的原因吧。减肥不是个容易活，今晚再去跑个步。</p>
<p>明天早起把英语作业做一下，再去和别人讨论一下，今晚有时间的话继续弹完中午的练习吧。</p>
<h3 id="月8日">5月8日</h3>
<center>
<b><font face=STXingkai color=MediumPurple>江畔何人初见月，江月何年初照人</font></b>
</center>
<p>上午去交流了一下，感觉K也不知道要做什么，哎，还是靠自己吧，再提醒一下自己，还是物理更重要一些，数学的东西少碰，有库尽量用库，既然他不清楚，那自己就多学点东西自力更生。</p>
<p>今天还发现了一件事情，原来我mi唱不准，我这极差的音准，还好练了一下能唱准了，这个耳朵么，依然不灵敏，这个月坚持练一下，争取能下意识听出旋律走向。</p>
<p>晚上jq给我发了个视频，我安哥点赞了，我以为他已经释怀了，可我现在才发现，他还是他，是啊，所以到底什么叫意难平呢？还记得去年国庆我安哥来找我玩的时候，那时候感觉他俩多么美好呀，现在也不知道发生了什么，我哥既然不愿意说，可能也还是放不下吧。</p>
<p>不得不说，太感性过不了柴米油盐，太理性过不了风花雪月，也许终是柴米油盐断了我们所有的锐气，人情世故染了我们所有的风霜，烟火人间，各有遗憾吧！</p>
<p>希望俺哥赶紧走出来，端午还是想去找你玩的呀</p>
<h3 id="月9日">5月9日</h3>
<center>
<b><font face=STXingkai color=MediumPurple>
城南小陌路又逢春，只见梅花不见人 </font></b>
</center>
<p>看了两篇文献，也算有收获吧，今下午理了一下思路，果然不能按照K的想法来，发现自己之前做了好多无用功。不管怎么样，以后得靠自己了，DL一定要好好学一下，起码比K的思路要强。</p>
<p>今天不知道为什么可喜欢听《你不是真正的快乐了》，解解的演唱会也想去看，奈何我木有钱，等以后有机会去吧。还有一件事，怎么si也唱不准，emm……</p>
<p>现在再去顺个三公里，跑完饿的话就去吃个饭，时间不早了，不知道今晚还有没有时间练琴，走咯~跑步去</p>
<h3 id="月10日">5月10日</h3>
<center>
<b><font face=STXingkai color=MediumPurple> 山与谷难再见，人与人自相逢
</font></b>
</center>
<p>时间不多啦，简单记录一下，先恭喜文子搬回一城，大比分来到一比一。</p>
<p>曲子今天到济南了，时隔一年再回济南，一切应该都感到很熟悉很亲切吧，虽然不知道你此行最大的目的是什么，但没猜错的话今宵肯定是占一部分原因的吧，兜兜转转，俩人终于是一起去NENU了，看你俩今晚这表现，我感觉很有戏，加油哈哈哈哈！</p>
<p>晚上看了个视频，大佬就是大佬，把卷积讲的这么清晰易懂，强烈安利一下，你币有了。</p>
<h3 id="月11日">5月11日</h3>
<center>
<b><font face=STXingkai color=MediumPurple>
趁我还鲜活，不允许任何人熄灭我 </font></b>
</center>
<p>今天太累了，记一下买口饭回宿舍了，整理了一下学习的内容，晚上看见我机哥了，周六晚上还在做实验，果然大家都不容易啊，溜了溜了。</p>
<h3 id="月18日">5月18日</h3>
<center>
<b><font face=STXingkai color=MediumPurple> 如果没有天赋，那就一直重复
</font></b>
</center>
<p>这几天太忙了，没时间记录，先开个头，准备下午出门一趟拿东西，晚上再更~</p>
<p>真的气死，下午去了一趟结果没开门，白跑一趟，还不能改邮寄，我真的裂开。</p>
<p>补一下前两天，达哥前几天还找我问了个问题，可惜啊，我已经不是那个专业了，只能把仅会的和人讲一下，他实验测不对，我这边也无进展，可能这就是当代学生现状吧，祝愿大家以后一切顺利！</p>
<p>帅哥和他朋友也顺利抵达杭州，来时间的太晚了，好多店已经不接了，只能简单的请他吃点，但这家店感觉味道确实还不戳，今日份上海之旅也泡汤了，改天再找机会去吧，晚上去顺顺腿儿，弹弹琴，坚持就是胜利！</p>
<h3 id="月19日">5月19日</h3>
<center>
<b><font face=STXingkai color=MediumPurple>
没有人可以回到过去，但谁都可以从现在开始 </font></b>
</center>
<p>效率低下，不想学习的一天，哎，以后不能这样，你还要毕业呢。</p>
<p>练了好久遇见，真是打脸，八分音符杀我，太难了，收回之前的话，然后发现自己的Fa纯是真唱不准，明天给我练准他呜呜呜~</p>
<p>然后，恭喜我文子进军西决，再拿一次总冠军吧，曾经的北岸花园，如今的嘘声漫天，你并不欠凯尔特人什么，也许只有你知道，之前雷霆主场的嘘声，比不上北岸花园的万分之一，我是真的想看你打决赛，把北岸花园打得鸦雀无声啊</p>
<h3 id="月20日">5月20日</h3>
<center>
<b><font face=STXingkai color=MediumPurple>
所有岌岌无名的日子里，我从未看轻过自己半分 </font></b>
</center>
<p>今天算是干了一天吧，但效率很低，明白了特征值和奇异值的区别还有它们背后的意义，但我不知道以后会不会用得到，但也算是给我蹩脚的线代补了一下知识吧，起码以后看到不会发怵了，也算时值得花一天时间吧。BNN的知识怎么说呢，今天相当于一点没学，明天得先把K那个mcmc的活干一下了，做个二维变量的相关性试试，然后明天问一下别人把数值模拟作业做了。</p>
<p>以后要强制自己看文献了，K既然在瞎搞你只能靠自己了，多看看别人在做什么，不要一直在K给画的框框里。</p>
<p>中午弹了一会儿琴，果然现在水平不配弹遇见，还是好好弹小汤吧。明天还有早八，先去跑步了。</p>
<h3 id="月24日">5月24日</h3>
<center>
<b><font face=STXingkai color=MediumPurple>
逢人不说人间事，便是人间无事人 </font></b>
</center>
<p>补一下昨天的，终于结束这周组会，明天放松收拾一下，再次提醒自己不要拿着锤子到处找钉子，想想自己要做什么，既要埋头赶路，又要仰望星空。</p>
<h3 id="月27日">5月27日</h3>
<center>
<b><font face=STXingkai color=MediumPurple>
不羡鸳鸯不羡仙，凯里欧文在身边 </font></b>
</center>
<p>又是好几天没更，还真是懒了。</p>
<p>首先恭喜独行侠3:0战胜森林狼，但莱弗利受伤让我感觉文子接下来比赛可能会打的很困难，手握赛点，希望下一场就送森林狼回家，决赛之前多休息一会。这口气憋了这么久，今年我们一定要复仇北岸花园！</p>
<p>最近琴有好好弹，起码感觉对键盘更熟悉了一点，加油，坚持就是胜利。</p>
<p>看了下PINN的工作，感觉和我这个方向也没什么太深入的结合，再深挖一下吧，同时加强一下专业知识，不要忘了Physics，今天感觉真是累了，准备回去休了。</p>
<h3 id="月29日">5月29日</h3>
<center>
<b><font face=STXingkai color=MediumPurple> 满地零碎里，我在找自己
</font></b>
</center>
<p>端午计划泡汤，jq接了中考的活，这么久没见，还是很想念的。</p>
<p>今天文子没发挥好，大比分3比1，有点伤心，我今天状态也不好，无精打采，晚上和帅去吃个夜宵，调整一下吧</p>
]]></content>
      <categories>
        <category>生活日常</category>
      </categories>
      <tags>
        <tag>Diary</tag>
        <tag>心情随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>Kalman Filter</title>
    <url>/2024/05/06/Kalman-Filter/</url>
    <content><![CDATA[<h1 id="卡尔曼滤波综述">卡尔曼滤波综述</h1>
<p>卡尔曼滤波尤其适合动态系统。它对于内存要求极低（它仅需要保留系统上一个状态的数据，而不是一段跨度很长的历史数据）。并且它运算很快，这使得它非常适合解决实时问题和应用于嵌入式系统。</p>
<h1 id="g-h-filter">g-h Filter</h1>
<p>当测量数据有噪声时，我们需要根据先验知识和实验测量值的信息来对其真实值进行有效估计</p>
<span id="more"></span>
<p>g-h滤波器通过两个因子来进行真实值的有效估计，g因子用于修正当前测量值和预测值之间的差异，h因子用于修正当前估计变化率和真实变化率之间的差异，在这个模型中当前步下的估计值由当前步下的预测值和当前步下的测量值决定，而当前步下的预测值由上一步下的估计值和估计变化率所决定，具体可以表示如下:
<span class="math display">\[
\begin{array}{l}
\hat{x}_t = x_{t}^{\rm pred} + g (x_{t}^{\rm measure} - x_{t}^{\rm
pred}) \\
\hat{\dot{x}}_t = \hat{\dot{x}}_{t-1} + h \frac{(x_{t}^{\rm measure} -
x_{t}^{\rm pred})}{\Delta t}
\end{array}
\]</span></p>
<p>其中<span class="math inline">\(\hat{x}_t\)</span>, <span
class="math inline">\(\hat{\dot{x}
}_t\)</span>分别为当前步下的预测值和估计变化率，<span
class="math inline">\(x_{t}^{\rm pred}\)</span>由上一步决定: <span
class="math display">\[
x_{t}^{\rm pred} = \hat{x}_{t-1} + \hat{\dot{x}}_{t-1} \cdot \Delta t
\]</span></p>
<p>通过g-h滤波可以从具有噪声的数据中融合先验提供较为准确的估计。</p>
<h1 id="高斯分布">高斯分布</h1>
<h2 id="高斯分布性质">高斯分布性质</h2>
<p>两个高斯函数的乘积仍然是高斯函数, <span class="math inline">\(g(\mu,
\sigma) \sim g_1(\mu_1, \sigma_1) \cdot g_2(\mu_2, \sigma_2)\)</span>,
其中 <span class="math display">\[
\begin{array}{l}
\mu = \frac{\sigma_1^2 \mu_2 + \sigma_2^2 \mu_1}{\sigma_1^2 +
\sigma_2^2} \\
\sigma^2 = \frac{\sigma_1^2 \sigma_2^2}{\sigma_1^2 + \sigma_2^2}
\end{array}
\]</span></p>
<p>两个高斯函数的加和后仍然是高斯函数, <span
class="math inline">\(g(\mu, \sigma) \sim g_1(\mu_1, \sigma_1) +
g_2(\mu_2, \sigma_2)\)</span>, 其中 <span class="math display">\[
\begin{array}{l}
\mu = \mu_1 + \mu_2 \\
\sigma^2 = \sigma_1^2 + \sigma_2^2
\end{array}
\]</span></p>
<h2 id="多维高斯分布">多维高斯分布</h2>
<p>多维高斯分布通常由均值向量 <span class="math inline">\(\mu\)</span>
和协方差矩阵 <span class="math inline">\(\Sigma\)</span> 来描述 <span
class="math display">\[
\begin{array}{l}
VAR(X) = \sigma _x^2 = {\mathbb E} [ { { {\left( {X - \mu } \right)}^2}}
] \\
COV(X,Y) = {\sigma _{xy}} = E\left[ {\left( {X - {\mu _x}} \right)\left(
{Y - {\mu _y}} \right)} \right]
\end{array}
\]</span> 协方差矩阵可以表示为 <span class="math display">\[
\Sigma  = \left[ {\begin{array}{c}
{\sigma _1^2}&amp;{ {\sigma _{12}}}&amp; \cdots &amp;{ {\sigma _{1n}}}\\
{ {\sigma _{21}}}&amp;{\sigma _2^2}&amp; \cdots &amp;{ {\sigma _{2n}}}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
{ {\sigma _{n1}}}&amp;{ {\sigma _{n2}}}&amp; \cdots &amp;{\sigma _n^2}
\end{array}} \right]
\]</span> 这里注意，协方差矩阵是对称的，总是满足 <span
class="math inline">\(\sigma_{xy} = \sigma_{yx}\)</span></p>
<h3 id="pearson相关系数">Pearson相关系数</h3>
<p>Preson相关系数定义为 <span class="math display">\[
\rho_{xy} = \frac{COV(X, Y)}{\sigma_{x} \sigma_{y}}
\]</span></p>
<p>Pearson相关系数取值在 <span class="math inline">\(0 \sim 1\)</span>
之间，描述了不同维度变量之间的相关性，越接近 <span
class="math inline">\(1(-1)\)</span> 表明越呈现正(负)相关，越接近 <span
class="math inline">\(0\)</span> 则表明越不相关。</p>
<h3 id="多维高斯分布性质">多维高斯分布性质</h3>
<p>两个多维高斯函数的乘积仍然是高斯函数，均值向量和协方差矩阵变为 <span
class="math display">\[
\begin{array}{l}
\mu = \Sigma_2 (\Sigma_1 + \Sigma_2)^{-1} \mu_1 + \Sigma_1 (\Sigma_1 +
\Sigma_2)^{-1} \mu_2 \\
\Sigma = \Sigma_1 (\Sigma_1 + \Sigma_2)^{-1} \Sigma_2
\end{array}
\]</span></p>
<h1 id="d-kalman-filter">1D Kalman Filter</h1>
<h2 id="一维卡尔曼滤波">一维卡尔曼滤波</h2>
<h3 id="问题情景">问题情景</h3>
<p>考虑一个情景，我们要测量一只狗的位置 <span
class="math inline">\(x\)</span>, 我们的传感器的测量值 <span
class="math inline">\(z\)</span>
有误差，传感器的测量结果服从高斯分布，狗的运动情况(Process
Model)也存在误差，比如理论上每间隔时间<span class="math inline">\(\Delta
t\)</span>就会移动<span class="math inline">\(\Delta
x\)</span>的距离，但实际上真真实的移动位置是以<span
class="math inline">\(\Delta
x\)</span>为均值的高斯分布，现在我们的问题是根据先验和传感器的测量结果预测狗的真实位置。</p>
<p>实际上这是一个求解后验的问题，传感器的测量值服从高斯分布，在这个问题中相当于似然函数，而我们的Process
Model实际上是根据上一时刻的后验估计值来计算当前时刻的先验，后验为先验和似然的乘积，根据高斯分布的性质我们便可以得到后验的均值和方差为狗的当前位置提供合理的估计。</p>
<p>根据以上分析，我们要求的后验可以写为</p>
<p><span class="math display">\[
\mu  = \left( {\frac{ { { {\bar \sigma }^2}}}{ { { {\bar \sigma }^2} +
\sigma _z^2}}} \right){\mu _z} + \left( {\frac{ {\sigma _z^2}}{ { {
{\bar \sigma }^2} + \sigma _z^2}}} \right)\bar \mu  = {W_1}{\mu _z} +
{W_2}\bar \mu
\]</span></p>
<p>在这里 <span class="math inline">\({\bar \mu}\)</span>
本质为先验，<span class="math inline">\(\mu_z\)</span> 本质为似然，令
<span class="math inline">\(K=W_1\)</span>，则 <span
class="math inline">\(\mu\)</span> 可以写为 <span
class="math display">\[
\mu = {\bar \mu} + K (\mu_z - {\bar \mu})
\]</span> 其中 <span class="math display">\[
K = \frac{ { { {\bar \sigma }^2}}}{ { { {\bar \sigma }^2} + \sigma
_z^2}}
\]</span></p>
<p>上述就是对这个一维问题的卡尔曼滤波实现，可以形象地用下面这个图来进行表示，其中权重<span
class="math inline">\(K\)</span>的引入也表明它的本质是一个 g-h
滤波器</p>
<p><img src="1D-Kalman-Filter.png" width="50%" height="50%" title="一维卡尔曼滤波示意图" alt="Error"/></p>
<h3 id="filterpy-代码实现">Filterpy 代码实现</h3>
<p>Initial state <span class="math inline">\(\mathcal{N}(10,
3)\)</span>, Movement <span class="math inline">\(\mathcal{N}(1,
4)\)</span> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> filterpy.kalman <span class="keyword">as</span> kf</span><br><span class="line">x, P = kf.predict(x=<span class="number">10.</span>, P=<span class="number">3.</span>, u=<span class="number">1.</span>, Q=<span class="number">2.</span>**<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;%.3f&#x27;</span> % x)</span><br><span class="line">x, P = kf.update(x=x, P=P, z=<span class="number">12.</span>, R=<span class="number">3.5</span>**<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;%.3f&#x27;</span> % x, <span class="string">&#x27;%.3f&#x27;</span> % P)</span><br></pre></td></tr></table></figure></p>
<h2 id="一维卡尔曼滤波流程">一维卡尔曼滤波流程</h2>
<p>将对状态的预测值记为 <span class="math inline">\({\bar
x}\)</span>，估计值记为 <span
class="math inline">\(x\)</span>，状态方差预测值记为 <span
class="math inline">\({\bar P}\)</span>，状态方差估计值记为 <span
class="math inline">\(P\)</span>, 过程噪声方差记为 <span
class="math inline">\(Q\)</span>, 测量噪声方差记为 <span
class="math inline">\(R\)</span>, 卡尔曼增益记为 <span
class="math inline">\(K\)</span>，则卡尔曼滤波的工作流程如下：</p>
<ul>
<li><strong>Predict</strong> <span class="math display">\[
\begin{array}{l}
\bar x = x + dx\\
\bar P = P + Q
\end{array}
\]</span></li>
<li><strong>Update</strong> <span class="math display">\[
\begin{array}{l}
y = z - \bar x\\
K = \frac{ {\bar P}}{ {\bar P + R}}\\
x = \bar x + Ky\\
P = (1 - K)\bar P
\end{array}
\]</span></li>
</ul>
<h1 id="多元卡尔曼滤波">多元卡尔曼滤波</h1>
<h2 id="问题情景引入">问题情景引入</h2>
<p><img src="https://picx.zhimg.com/50/v2-3e2ab5a07a120d695c3ff1db7c80a4c4_720w.jpg?source=2c26e567" data-rawwidth="300" data-rawheight="160" data-caption="" data-size="small" data-original-token="v2-3e2ab5a07a120d695c3ff1db7c80a4c4" class="content_image" width="300"/></p>
<p>假设你开发了一款小型机器人，它可以在树林里自主移动，并且这款机器人需要明确自己的位置以便进行导航。</p>
<p>我们可以通过一组状态变量 <span class="math inline">\({\bf x}\)</span>
来描述机器人的状态，包括位置和速度，即 <span class="math display">\[
{\bf x} = \left[ {\begin{array}{c}
x\\
{\dot x}
\end{array}} \right]
\]</span>
我们的机器人同时拥有一个GPS传感器，精度在10m。这已经很好了，但是对我们的机器人来说它需要以远高于10m的这个精度来定位自己的位置。在机器人所处的树林里有很多溪谷和断崖，如果机器人对位置误判了哪怕只是几步远的距离，它就有可能掉到坑里。所以仅靠GPS是不够的。</p>
<p>同时我们可以获取到一些机器人的运动的信息：驱动轮子的电机指令对我们也有用处。如果没有外界干扰，仅仅是朝一个方向前进，那么下一个时刻的位置只是比上一个时刻的位置在该方向上移动了一个固定距离。当然我们无法获取影响运动的所有信息：机器人可能会受到风力影响，轮子可能会打滑，或者碰到了一些特殊的路况；所以轮子转过的距离并不能完全表示机器人移动的距离，这就导致通过轮子转动预测机器人位置不会非常准确。</p>
<p>GPS传感器也会告知我们一些关于机器人状态的信息，但是会包含一些不确定性因素。我们通过轮子转动可以预知机器人是如何运动的，同样也有一定的不准确度。
如果我们综合两者的信息呢？可以得到比只依靠单独一个信息来源更精确的结果么？答案当然是YES，这就是卡尔曼滤波要解决的问题。</p>
<p>在现实中，速度和位置是有关联的。如果已经确定位置的值，那么某些速度值存在的可能性更高。假如我们已知上一个状态的位置值，现在要预测下一个状态的位置值。如果我们的速度值很高，我们移动的距离会远一点。相反，如果速度慢，机器人不会走的很远。
这种关系在跟踪系统状态时很重要，因为它给了我们更多的信息：一个测量值告诉我们另一个测量值可能是什么样子。这就是卡尔曼滤波的目的，我们要尽量从所有不确定信息中提取有价值的信息！</p>
<h2 id="kalman-滤波处理">Kalman 滤波处理</h2>
<p>在这个问题中我们只对物体的状态信息 <span class="math inline">\({\bf
x}\)</span>
感兴趣，但其实关注的是机器人下个时刻出现在哪里也就是物体的位置信息 <span
class="math inline">\(x\)</span>, 并不关心它的速度 <span
class="math inline">\({\dot x}\)</span>, 即速度在这里为隐藏变量。</p>
<p>我们对系统状态的分布建模为高斯分布，所以在 <span
class="math inline">\(k\)</span> 时刻我们需要两个信息：即状态向量 <span
class="math inline">\({\bf x}_{k}\)</span> 和它的协方差矩阵 <span
class="math inline">\({\bf P}_{k}\)</span>，为了得到 <span
class="math inline">\(k\)</span> 时刻的状态信息，我们需要用 <span
class="math inline">\(k-1\)</span>
时刻状态的估计信息来进行预测并结合测量结果进行 <span
class="math inline">\(k\)</span> 时刻状态信息的估计。</p>
<h3 id="prediction-process">Prediction Process</h3>
<h4 id="状态转移矩阵-bf-f-的引入">状态转移矩阵 <span
class="math inline">\({\bf F}\)</span> 的引入</h4>
<p>下一步，我们需要通过k-1时刻的状态来预测k时刻的状态。请注意，我们不知道状态的准确值，但是我们的预测函数并不在乎。它仅仅是对k-1时刻所有可能值的范围进行预测转移，然后得出一个k时刻新值的范围。我们用状态转移矩阵
<span class="math inline">\({\bf F}_k\)</span> 来描述这个转换 <span
class="math display">\[
\begin{array}{l}
{ { {\bar x}_k} = {x_{k - 1}} + { {\dot x}_{k - 1}}\Delta t}\\
{ { {\bar {\dot x}}_k} = { {\dot x}_{k - 1}}}
\end{array}
\]</span> 整理为转移矩阵的形式为 <span class="math display">\[
\begin{array}{l}
{ {\hat {\bf x}}_k} = \left[ {\begin{array}{c}
1&amp;{\Delta t}\\
0&amp;1
\end{array}} \right]{ {\hat {\bf x}}_{k - 1}}\\
= { {\bf F}_k}{ {\hat {\bf x}}_{k - 1}}
\end{array}
\]</span></p>
<p>此时完成了对状态的更新，可以简单预测下个状态，同样也要更新状态对应的协方差矩阵，此时只需每个点进行矩阵
<span class="math inline">\({\bf F}\)</span> 转换，它的协方差矩阵 <span
class="math inline">\(\Sigma\)</span> 变为： <span
class="math display">\[
\begin{array}{l}
Cov(x) = {\bf \Sigma} \\
Cov(Fx) = {\bf F} {\bf \Sigma} {\bf F}^T
\end{array}
\]</span></p>
<h4 id="控制矩阵-bf-b-和控制向量-bf-u-的引入">控制矩阵 <span
class="math inline">\({\bf B}\)</span> 和控制向量 <span
class="math inline">\({\bf u}\)</span> 的引入</h4>
<p>我们并没有考虑到所有影响因素。系统状态的改变并不只依靠上一个系统状态，外界作用力可能会影响系统状态的变化。例如，跟踪一列火车的运动状态，火车驾驶员可能踩了油门使火车提速。同样，在我们机器人例子中，导航软件可能发出一些指令启动或者制动轮子。如果我们知道这些额外的信息，我们可以通过一个向量
<span class="math inline">\({\bf u}_k\)</span>
来描述这些信息，把它添加到我们的预测方程里作为一个修正。假如我们通过发出的指令得到预期的加速度a，上边的运动方程可以变化为：
<span class="math display">\[
\begin{array}{l}
{ { {\bar x}_k} = {x_{k - 1}} + { {\dot x}_{k - 1}}\Delta t} +
\frac{1}{2} a \Delta t^2 \\
{ { {\bar {\dot x}}_k} = { {\dot x}_{k - 1}}} + a \Delta t
\end{array}
\]</span> 写成矩阵形式为 <span class="math display">\[
\begin{array}{l}
{ { {\hat {\bf{x}}}_k} = \left[ {\begin{array}{c}
1&amp;{\Delta t}\\
0&amp;1
\end{array}} \right]{ {\hat {\bf{x}}}_{k - 1}} + \left[
{\begin{array}{c}
{\Delta {t^2}/2}\\
{\Delta t}
\end{array}} \right]a}\\
{ = { {\bf{F}}_k}{ {\hat {\bf{x}}}_{k - 1}} + {\bf B}_k{ {\bf u}_k}}
\end{array}
\]</span> 其中 <span class="math inline">\({\bf B}_k\)</span>
为控制矩阵，<span class="math inline">\({\bf u}_k\)</span> 为控制向量,
(对于没有任何外界动力影响的系统，可以忽略该项)</p>
<h4 id="过程方差矩阵-bf-q-的引入">过程方差矩阵 <span
class="math inline">\({\bf Q}\)</span> 的引入</h4>
<p>如果状态只会根据系统自身特性演变那将不会有任何问题。如果我们可以把所有外界作用力对系统的影响计算清楚那也不会有任何问题。
但是如果有些外力我们无法预测呢？假如我们在跟踪一个四轴飞行器，它会受到风力影响。如果我们在跟踪一个轮式机器人，轮子可能会打滑，或者地面上的突起会使它降速。我们无法跟踪这些因素，并且这些事情发生的时候上述的预测方程可能会失灵。</p>
<p>因此我们可以把“世界”中的这些不确定性统一建模，在预测方程中增加一个不确定项，这样，原始状态中的每一个点可以都会预测转换到一个范围，而不是某个确定的点。可以这样描述：<span
class="math inline">\({\bf x}_{k-1}\)</span>
中的每个点移动到一个符合方差 <span class="math inline">\({\bf
Q}_k\)</span> 的高斯分布里。另一种说法，我们把这些不确定因素描述为方差为
<span class="math inline">\({\bf Q}_k\)</span>
的高斯噪声，这样做的结果是引入噪声 <span class="math inline">\({\bf
Q}_k\)</span>
后。这会产生一个新的高斯分布，方差不同，但是均值相同。只需对 <span
class="math inline">\({\bf Q}_k\)</span>
简单叠加，可以拿到扩展的方差。</p>
<p>这样就得到了完整的预测转换方程 <span class="math display">\[
\begin{array}{l}
{\hat {\bf x}_k} = {\bf F}_k {\hat {\bf x}_{k-1}} + {\bf B}_k {\bf u}_k
\\
{\bf P}_k = {\bf F}_k {\bf P}_{k-1} {\bf F}_k^T + {\bf Q}_k
\end{array}
\]</span></p>
<h3 id="update-process">Update Process</h3>
<p>到这里，我们得到了一个模糊的估计范围，一个通过 <span
class="math inline">\({\bf {\bar x}}_k\)</span> 和 <span
class="math inline">\({\bf {\bar P}}_k\)</span>
描述的范围。如果再结合我们传感器的数据呢？</p>
<p>我们可能还有一些传感器来测量系统的状态。通常来讲传感器的测量变量和我们感兴趣的状态量不一定一致，比如在我们关心的是温度
<span class="math inline">\({\bf
x}\)</span>，但传感器所给的读数可能是电压 <span
class="math inline">\({\bf
z}\)</span>，因此们需要把感兴趣的状态量变换到测量空间中，记测量矩阵为
<span class="math inline">\({\bf H}\)</span>，则变换后的结果为 <span
class="math inline">\({\bf H x}\)</span>，在测量空间下的均值和协方差为
<span class="math display">\[
\begin{array}{l}
{\bf \mu}_k = {\bf  H}_k {\bf {\bar x}}_k \\
{\bf S}_k = {\bf H}_k  {\bf {\bar P}}_k  {\bf  H}_k^T
\end{array}
\]</span>
传感器有自己的精度范围，对于一个真实的位置和速度，传感器的读数受到高斯噪声影响会使读数在某个范围内波动。我们将这种不确定性的方差为描述为
<span class="math inline">\({\bf R}\)</span>， 读数的平均值为 <span
class="math inline">\({\bf z}\)</span>
。这样就得到了完整的测量空间下的转换方程 <span class="math display">\[
\begin{array}{l}
{\bf \mu}_k = {\bf  H}_k {\bf {\bar x}}_k \\
{\bf S}_k = {\bf H}_k  {\bf {\bar P}}_k  {\bf  H}_k^T + {\bf R}_k
\end{array}
\]</span> 下面与一维卡尔曼滤波类似，在执行更新步骤时，首先计算残差 <span
class="math inline">\({\bf y}\)</span> <span class="math display">\[
{\bf y}_k = {\bf z}_k - {\bf H}_k {\bf {\bar x}}_k
\]</span> 计算卡尔曼增益 <span class="math display">\[
\begin{array}{l}
{\bf S}_k = {\bf H}_k  {\bf {\bar P}}_k  {\bf  H}_k^T + {\bf R}_k \\
{\bf K}_k = {\bf {\bar P}}_k {\bf H}_k^{T} {\bf S}_k^{-1}
\end{array}
\]</span> 更新状态量 <span class="math display">\[
\begin{array}{l}
{\bf x}_k = {\bf {\bar x}}_k + {\bf K}_k {\bf y} \\
{\bf P}_k = ({\bf I} - {\bf K}_k {\bf H}_k) {\bf {\bar P}}_k
\end{array}
\]</span> 至此，我们得到了每个状态的更新步骤 <span
class="math inline">\({\bf x}_k\)</span>
是我们最佳的估计值，可以进行持续迭代。</p>
<h2 id="kalamn-filter-算法">Kalamn Filter 算法</h2>
<h3 id="符号定义">符号定义</h3>
<ul>
<li>状态向量 <span class="math inline">\({\bf x}\)</span></li>
<li>状态量的协方差矩阵 <span class="math inline">\({\bf P}\)</span></li>
<li>转移矩阵 <span class="math inline">\({\bf F}\)</span></li>
<li>控制矩阵 <span class="math inline">\({\bf B}\)</span></li>
<li>控制向量 <span class="math inline">\({\bf u}\)</span></li>
<li>过程噪声矩阵 <span class="math inline">\({\bf Q}\)</span></li>
<li>测量值 <span class="math inline">\({\bf z}\)</span></li>
<li>测量噪声矩阵 <span class="math inline">\({\bf R}\)</span></li>
<li>测量矩阵 <span class="math inline">\({\bf H}\)</span></li>
<li>卡尔曼增益矩阵 <span class="math inline">\({\bf K}\)</span></li>
</ul>
<h3 id="算法流程">算法流程</h3>
<h4 id="流程表示方法-1">流程表示方法 1</h4>
<ul>
<li><strong>Predict Step</strong> <span class="math display">\[
\begin{array}{l}
{\bf x} = {\bf Fx} + {\bf Bu} \\
{\bf P} = {\bf FPF}^T + {\bf Q}
\end{array}
\]</span></li>
<li><strong>Update Step</strong> <span class="math display">\[
\begin{array}{l}
{\bf S} = {\bf HPH}^T + {\bf R} \\
{\bf K} = {\bf PH}^T {\bf S}^{-1} \\
{\bf y} = {\bf z} - {\bf Hx} \\
{\bf x} = {\bf x} + {\bf Ky} \\
{\bf P} = ({\bf I} - {\bf KH}) {\bf P}
\end{array}
\]</span></li>
</ul>
<h4 id="流程表示方法-2">流程表示方法 2</h4>
<ul>
<li><strong>Predict Step</strong> <span class="math display">\[
\begin{array}{l}
{\hat {\bf x}}_{k|k-1} = {\bf F}_k {\hat {\bf x}}_{k-1|k-1} + {\bf B}_k
{\bf u}_k \\
{\bf P}_{k|k-1} = {\bf F}_k {\bf P}_{k-1|k-1} {\bf F}_k^T + {\bf Q}_k
\end{array}
\]</span></li>
<li><strong>Update Step</strong> <span class="math display">\[
\begin{array}{l}
{\bf S}_{k} = {\bf H}_k {\bf P}_{k|k-1} {\bf H}_k^T + {\bf R}_k \\
{\bf K}_{k} = {\bf P}_{k|k-1} {\bf H}_k^T {\bf S}_k^{-1} \\
{\bf y}_k = {\bf z}_{k} - {\bf H}_k {\hat {\bf x}}_{k|k-1} \\
{\hat {\bf x}}_{k|k} = {\hat {\bf x}}_{k|k-1} + {\bf K}_{k} {\bf y}_k \\
{\bf P}_{k|k} = ({\bf I} - {\bf K}_k {\bf H}_k) {\bf P}_{k|k-1}
\end{array}
\]</span></li>
</ul>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>数学知识</tag>
        <tag>Kalman Filter</tag>
      </tags>
  </entry>
  <entry>
    <title>学不会的数学</title>
    <url>/2024/05/06/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<h1 id="线性代数">线性代数</h1>
<span id="more"></span>
<h2 id="对称矩阵">对称矩阵</h2>
<p>矩阵乘以自身的转置必为对称矩阵，证明过程为</p>
<p><span class="math display">\[
(A A^{T})^{T} = (A^{T})^{T} A^{T} = A A^{T}
\]</span></p>
<h2 id="矩阵的秩">矩阵的秩</h2>
<p>矩阵的秩（Rank）是指矩阵中线性独立行或列的最大数目。</p>
<h3 id="矩阵秩的性质">矩阵秩的性质</h3>
<ol type="1">
<li>rank(<span class="math inline">\(A\)</span>) = rank(<span
class="math inline">\(A^{T}\)</span>) = rank(<span
class="math inline">\(A A^{T}\)</span>) = rank(<span
class="math inline">\(A^{T} A\)</span>)<br />
用非公式、通俗易懂的语言解释一下行秩等于列秩。直观的解释就是当某个列向量开始与其他列向量线性相关时，说明这个列向量可以用其他向量线性表示，其实也就是它没有跑到其他向量张成的线性空间之外。既然这个列向量可以用其他列向量线性表示，从行向量上来看，这个分量的添加无非相当于给行向量添加了一个没有用的分量，因为行向量不需要这个分量也可以被完全描述，所以这个分量的增加自然也不会改变行向量的秩。正如同一维空间二维空间不需要用三维向量来描述一样，非要用三维向量来描述一维空间也必然有两个维度是多余的，因为他足以被一个维度唯一确定。反之，如果某个列向量与其他向量线性无关，对于行向量来说，没有这个分量必然导致已有分量不能完全描述行向量，所以这个分量的增加必然改变行向量的秩。代数的证明很简单，就是不断地进行初等行变换和列变换，最终可以变换成对角形式，所以行秩必然等于列秩。</li>
</ol>
<h2 id="特征值分解">特征值分解</h2>
<p>如果有向量<span class="math inline">\({\bf x}\)</span>能使得矩阵<span
class="math inline">\({\bf A}\)</span>满足<span
class="math inline">\({\bf Ax} = \lambda {\bf x}\)</span>，那么<span
class="math inline">\(\lambda\)</span>和<span
class="math inline">\(x\)</span>就分别是<span class="math inline">\({\bf
A}\)</span>的特征值与特征向量。</p>
<p>特征值分解的意义在于对于 <span class="math inline">\(A\)</span>
这个线性变换操作来讲，可以找到对应地特征向量，使得 <span
class="math inline">\(A\)</span>
这个线性变换操作当其特征向量时它只是将特征向量 <span
class="math inline">\({\bf x}\)</span> 数乘一个标量(对应的特征值<span
class="math inline">\(\lambda\)</span>)，相当于将 <span
class="math inline">\({\bf x}\)</span> 长度缩放 <span
class="math inline">\(\lambda\)</span>
倍，因此对特征函数构成的空间中的任意一个函数的变换可以简化为各特征函数的加权和。</p>
<h3 id="性质">性质</h3>
<p>实对称矩阵的特征向量一定正交。</p>
<h2 id="奇异值分解-svd">奇异值分解 (SVD)</h2>
<p>如果存在单位正交矩阵(标准正交基矢的转置就是自身的逆矩阵) <span
class="math inline">\(U\)</span> 和 <span
class="math inline">\(V\)</span>，使得 <span class="math inline">\(A = U
\Sigma V^{T}\)</span>，<span class="math inline">\(\Sigma\)</span>
为对角矩阵，对角线上的值被称为奇异值<span
class="math inline">\(U\)</span>和<span
class="math inline">\(V\)</span>中的列分别被称为<span
class="math inline">\(A\)</span>的左奇异向量和右奇异向量。</p>
<p>矩阵的秩（Rank）等于奇异值分解（SVD）中的非零奇异值的数。对于任意<span
class="math inline">\(m \times n\)</span>矩阵<span
class="math inline">\(A\)</span>，其奇异值分解可以表示为。</p>
<p><span class="math display">\[
A = U \Sigma V^{T}
\]</span></p>
<p>其中：</p>
<ul>
<li><p><span class="math inline">\(U\)</span>是一个<span
class="math inline">\(m \times m\)</span>的正交矩阵</p></li>
<li><p><span class="math inline">\(\Sigma\)</span>是一个<span
class="math inline">\(m \times
n\)</span>的对角矩阵，对角线上的非负实数称为奇异值。</p></li>
<li><p><span class="math inline">\(V\)</span>是一个<span
class="math inline">\(n \times n\)</span>的正交矩阵</p></li>
</ul>
<p>奇异值衡量了矩阵<span
class="math inline">\(A\)</span>在其对应的奇异向量方向上的“拉伸”或“压缩”程度。非零奇异值表示了这种变换的显著性，而零奇异值则表示在对应方向上没有显著的变换。</p>
<h2 id="特征值-vs-奇异值">特征值 VS 奇异值</h2>
<h3 id="定义对比">定义对比</h3>
<p>奇异值与特征值都被用于描述矩阵作用于某些向量的标量，都是描述向量模长变化幅度的数值。它们的差异在于：</p>
<ul>
<li>特征向量描述的是矩阵的方向不变作用(invariant action)的向量；</li>
<li>奇异向量描述的是矩阵最大作用(maximum action)的方向向量。</li>
</ul>
<p>这里的“作用”(action)所指的矩阵与向量的乘积得到一个新的向量，几何上相当于对向量进行了旋转和拉伸，就像是对向量施加了一个作用(action)，或者说是变换。</p>
<h2 id="关联">关联</h2>
<p>奇异值是正交矩阵特征值的绝对值。</p>
<h3 id="几何意义对比">几何意义对比</h3>
<p>参考 <a
href="https://zhuanlan.zhihu.com/p/353637184">奇异值与特征值辨析</a></p>
<h2 id="pca与svd">PCA与SVD</h2>
<p>PCA本质是基于SVD来做的，PCA的基本思想是降维度，先给一个结论，奇异值越大表明包含的信息越多，根据SVD我们有</p>
<p><span class="math display">\[
A = U \Sigma V^{T}
\]</span></p>
<p>在实际操作中，A为数据集(<span
class="math inline">\(m=\)</span>样本数，<span
class="math inline">\(n=\)</span>特征数)，我们可以保留几个关键特征数实现降维，此时PCA干的事情可以写为</p>
<p><span class="math display">\[
{A_{m \times n}} \approx {U_{m \times r}}{\Sigma _{r \times r}}V_{r
\times n}^T  \quad \quad r &lt; n
\]</span></p>
<p>能这样做的原因是，在很多情况下，前<span
class="math inline">\(10\%\)</span> 甚至 <span
class="math inline">\(1\%\)</span>
的奇异值的和就占了全部的奇异值之和的<span
class="math inline">\(99\%\)</span>以上了。可以可以用前<span
class="math inline">\(r\)</span>大的奇异值来近似描述矩阵。右边的三个矩阵相乘的结果将会是一个接近于<span
class="math inline">\(A\)</span>的矩阵，在这儿，<span
class="math inline">\(r\)</span>越接近于<span
class="math inline">\(n\)</span>，则相乘的结果越接近于<span
class="math inline">\(A\)</span>。而这三个矩阵的面积之和（在存储观点来说，矩阵面积越小，存储量就越小）要远远小于原始的矩阵<span
class="math inline">\(A\)</span>，我们如果想要压缩空间来表示原矩阵<span
class="math inline">\(A\)</span>，我们存下这里的三个矩阵<span
class="math inline">\(U\)</span>、 <span
class="math inline">\(\Sigma\)</span>、 <span
class="math inline">\(V\)</span>就好了。</p>
<p>因此主成分分析实际上关注的是 <span class="math inline">\({ {\tilde
A}_{m \times r} } = {A_{m \times n} }{V_{n \times r} } = {U_{m \times r}
}{\Sigma _{r \times r} }\)</span> 这个矩阵，它成功将特征数从<span
class="math inline">\(n\)</span>降低到了<span
class="math inline">\(r\)</span>。</p>
<h1 id="不确定性分析">不确定性分析</h1>
<p>不确定性分析的目标在于：将模拟系统输入参数的不确定度传递到计算响应中，量化模拟系统计算响应的不确定度水平和不同响应之间的相关性信息。</p>
<h1 id="敏感性分析">敏感性分析</h1>
<h2 id="sobol指数敏感性分析">Sobol指数敏感性分析</h2>
<p>Sobol指数法是一种全局敏感性分析方法，基本原理是将模型分解为若干由单个参数或参数组合构成的函数，通过方差分解，得到各部分方差对于模型输出的总方差的贡献程度，进一步计算、分析模型中各变量的敏感度以及参数组合间的交互作用。</p>
<p>在 SALib 中，如果使用了 ProblemSpec 对象并调用了 analyze_sobol
方法进行敏感度分析，采样情况通常是在调用 sample
方法时确定的。ProblemSpec
对象会存储采样结果，并且可以使用它来进行后续的分析。</p>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>数学知识</tag>
      </tags>
  </entry>
  <entry>
    <title>Memo - 反应堆物理备忘录</title>
    <url>/2024/05/06/Nuclear-reactor/</url>
    <content><![CDATA[<h1 id="基本知识补充">基本知识补充</h1>
<span id="more"></span>
<h2 id="核燃料">核燃料</h2>
<ul>
<li><strong>UOX燃料</strong> ：UOX代表低浓缩铀氧化物（Uranium
Oxide），它是一种常见的核燃料形式，UOX燃料通常使用天然铀或低浓缩铀（<span
class="math inline">\(^{235}{\rm U}\)</span>含量在3-5%之间）制成。</li>
<li><strong>MOX燃料</strong>：MOX代表混合氧化物（Mixed
Oxide），它混合了铀和钚，是由<span class="math inline">\({\rm
UO}_{2}\)</span>和<span class="math inline">\({\rm
PuO}_{2}\)</span>构成的氧化铀钚燃料。MOX燃料通常由浓缩铀和从核废料中回收的钚混合而成。</li>
</ul>
<p>UOX和MOX燃料都是核反应堆的燃料形式，但MOX燃料相比UOX燃料含有更高比例的<span
class="math inline">\(^{235}{\rm U}\)</span>以及<span
class="math inline">\(^{239}{\rm Pu}\)</span></p>
<h1 id="基本概念常识补充">基本概念常识补充</h1>
<h1 id="常见缩写">常见缩写</h1>
<h2 id="几种反应堆型">几种反应堆型</h2>
<ul>
<li><strong>LMFBR</strong>: Liquid Metal Fast Breeder Reactors
液态金属快增殖反应堆</li>
<li><strong>BWR</strong>: Boiling Water Reactor 沸水反应堆</li>
<li><strong>PWR</strong>: Pressurized Water Reactor 压水反应堆</li>
<li><strong>CANDU</strong>: 加拿大重水铀反应堆</li>
</ul>
<h2 id="几个实验室缩写">几个实验室缩写</h2>
<ul>
<li><strong>ORNL</strong>：Oak Ridge National Laboratory
美国橡树岭国家实验室</li>
<li><strong>ANL</strong>：Argonne National Laboratory
美国阿贡国家实验室</li>
<li><strong>JAEA</strong>：Japaen Atomic Engery Agency
日本原子能机构</li>
<li><strong>OECD/NEA</strong>：Organization for Economic Co-operation
and Development Nuclear Energy Agency
国际经济合作与发展组织核能机构</li>
<li><strong>BNL</strong>：Brookhaven National Laboratory
鲁克海文国家实验室</li>
<li><strong>LANL</strong>：Los Alamos National Laboratory
洛斯阿拉莫斯国家实验室</li>
</ul>
<h2 id="几个核数据库">几个核数据库</h2>
<ul>
<li><strong>ENDF</strong></li>
</ul>
<h1 id="专业名词解释">专业名词解释</h1>
<h2 id="eoi">EOI</h2>
<p>"EOI" 代表 "End Of
Irradiation"，即“辐照结束”，用来指代燃料组件或单个燃料棒在反应堆中辐照的结束点。在EOI时刻，燃料已经达到了其设计寿命的终点，此时会进行停堆，并开始后续的冷却、运输和处理过程。</p>
<p>在核燃料的背景下，EOI是重要的时间节点:</p>
<ul>
<li><p>它标志着燃料从反应堆中移出，开始冷却期。</p></li>
<li><p>它影响燃料的放射性和热输出，因为随着时间的推移，短寿命的放射性核素会衰变。</p></li>
<li><p>它决定了燃料中裂变产物和转换材料的库存量，这些材料的组成是评估燃料特性和后续处理选项的关键。</p></li>
</ul>
<h1 id="核反应堆计算程序">核反应堆计算程序</h1>
<h2 id="origen-ornl-isotope-generation-and-depletion-code">ORIGEN (ORNL
Isotope Generation and Depletion code)</h2>
<h3 id="origen-2-version">ORIGEN-2 Version</h3>
<p><strong>ORIGEN-2</strong>是一个著名的零维燃耗计算程序，模拟核燃料在反应堆中因中子辐照而发生的燃耗过程，可以预测燃料在辐照过程中的同位素组成变化。</p>
<p>它不考虑空间维度（即没有考虑燃料棒或燃料组件内部的几何结构和空间分布），而是集中在燃料的化学和核物理特性上，特别是关注燃料中同位素的组成随时间的变化。由于零维模型没有考虑空间异质性，因此它们不能提供空间上燃耗分布的细节。对于需要考虑空间效应的复杂分析，可能需要使用更高维度的燃耗模型。</p>
<h3 id="origen-apr-version">ORIGEN-APR Version</h3>
<p><strong>ORIGEN-APR</strong>（ORIGEN-Advanced Physics and
Reactor）是从ORIGEN系列代码发展而来，这些代码最初被设计用于计算核材料中同位素的组成和特性。</p>
<p>ORIGEN-APR特别关注于提供更高级的物理模型和更精确的燃耗计算，包括以下特点：</p>
<ul>
<li>更精确的物理模型：ORIGEN-APR包含了更精确的核数据和更详细的物理过程描述，如中子俘获、裂变、衰变等。</li>
<li>多维燃耗模拟：与简单的零维模型不同，ORIGEN-APR能够进行<strong>一维或多维燃耗模拟</strong>，这意味着它可以模拟燃料棒内部或整个反应堆芯的燃耗分布。</li>
<li>先进的反应堆物理：该程序可以模拟不同类型的反应堆，包括压水堆（PWR）、沸水堆（BWR）、重水堆（CANDU）、快中子堆（如LMFBR）等，并考虑了各自的中子能谱特性。</li>
<li>考虑冷却时间：ORIGEN-APR能够考虑燃料辐照结束后的冷却时间对同位素组成的影响，这对于核材料的管理和处置非常重要。</li>
</ul>
<h1 id="常见反应堆">常见反应堆</h1>
<h2 id="vver">VVER</h2>
<p>参见<a
href="https://www.bilibili.com/read/cv25679314/">VVER反应堆简介</a></p>
<p>水-水高能反应堆（Water-water energetic
reactor），简称WWER或VVER，为苏联于上世纪70年代研发的压水反应堆，与美国的PWR相比，特点为采用卧式蒸汽发生器、六边形燃料组件、无底部穿孔压力容器和高容量稳压器。</p>
<h2 id="组件示意图">组件示意图</h2>
<h2 id="反应堆参数">反应堆参数</h2>
<ul>
<li><strong>参考文献</strong><br />
</li>
</ul>
<ol type="1">
<li>Bilodid, Y., Fridman, E., &amp; Lötsch, T. X2 VVER-1000 benchmark
revision: Fresh HZP core state and the reference Monte Carlo solution.
Annals of Nuclear Energy, <strong>144</strong>, 107558 (2020).</li>
<li>Hossain, M. I., Mollah, A. S., Akter, Y., &amp; Fardin, M. Z.
Neutronic calculations for the VVER-1000 MOX core computational
benchmark using the OpenMC code. Nuclear Energy and Technology,
<strong>9</strong>, 215 (2023).</li>
<li>Khuwaileh, B. A., Al-Shabi, M., &amp; Assad, M. E. H. Artificial
Neural Network based Particle Swarm Optimization solution approach for
the inverse depletion of used nuclear fuel. Annals of Nuclear Energy,
<strong>157</strong>, 108256 (2021).</li>
</ol>
]]></content>
      <categories>
        <category>Physics</category>
      </categories>
      <tags>
        <tag>反应堆物理</tag>
      </tags>
  </entry>
  <entry>
    <title>Openmc</title>
    <url>/2024/06/14/Openmc/</url>
    <content><![CDATA[<h1 id="基本概念与原理">基本概念与原理：</h1>
<h2 id="粒子权重">粒子权重</h2>
<ol type="1">
<li><p>粒子权重允许每个粒子在模拟中代表多个实际粒子。通过这种方式，可以用较少的粒子数获得足够的统计数据，从而减少计算量和模拟时间。</p></li>
<li><p>在模拟开始时，所有粒子通常被赋予相同的权重，例如，每个粒子的权重都设置为1。</p></li>
<li><p>在模拟过程中，根据特定的准则或算法，粒子的权重可能会被增加或减少。例如，如果某个区域的统计数据不足，可以增加该区域粒子的权重，使其在后续步骤中具有更大的影响力。
权重累积：</p></li>
<li><p>粒子在经历散射、裂变等事件后，可能会生成新的粒子（如裂变产生的次级中子）。这些新粒子可能会继承或分配特定的权重。
权重归一化：</p></li>
<li><p>为了确保模拟结果的准确性，所有粒子的权重在每个批次结束时通常会被归一化，以保证它们的总和等于1。这样，粒子的权重反映了它们在模拟中的相对重要性。</p></li>
</ol>
<h1 id="openmc-数据后处理">Openmc 数据后处理</h1>
<span id="more"></span>
<h2 id="tally-统计">Tally 统计</h2>
<p>Tally（计数器）用于在模拟过程中记录特定的物理量，例如中子通量、裂变率、吸收率等。</p>
<p>在蒙特卡洛模拟中，任何物理的统计形式都可以写为</p>
<p><span class="math display">\[
X = \int {d {\bf r}} \int {d {\bf \Omega}} \int {dE} f({\bf r}, {\bf
\Omega}, E) \psi({\bf r}, {\bf \Omega}, E)
\]</span></p>
<p>Tally需要与Filter一起使用，来确在不同区域、方位角、表面、能量范围的计数。</p>
<p>通常tally.sum
给出的是一个累积值，即在模拟的所有批次中，所有批次中子通量的总和。</p>
<h3 id="tally统计物理量">Tally统计物理量</h3>
<p>一般来讲Tally数据有三个维度:一个用Filter组合，一个用于核素Nuclide，一个用于指定的Scores。</p>
<ol type="1">
<li><strong>flux</strong><br />
flux统计的是单位面积（体积）上单位时间内通过的中子数。通量计数可以应用于不同的统计对象，例如：
<ol type="1">
<li><strong>材料（Materials）：</strong>
如果使用材料过滤器，通量计数将统计特定材料内部的中子通量。</li>
<li><strong>单元格（Cells）：</strong>
如果使用网格过滤器（MeshFilter），通量计数将统计网格上每个单元格内的中子通量。</li>
<li><strong>表面（Surfaces）：</strong>
如果使用表面过滤器，通量计数将统计通过特定表面的中子通量。</li>
</ol></li>
<li><strong>fission</strong><br />
fission统计的是在指定的几何区域（由 Tally
的过滤器定义，例如单元格、表面或材料）内发生的裂变事件总数。</li>
</ol>
<h1 id="openmc.settings">Openmc.Settings</h1>
<h2 id="settings.batches">settings.batches</h2>
<p>在 openmc
蒙特卡洛模拟中，每个批次的模拟通常是独立的，并且可以并行执行。以下是关于批次模拟的一些要点：</p>
<ul>
<li>独立性：每个批次的模拟都是从上一个批次结束时的状态开始的。这意味着每个批次的模拟结果是基于前一个批次结束时粒子的状态和分布进行的。</li>
<li>非活跃批次：在模拟的开始阶段，可以设置一定数量的惰性批次，这些批次的统计数据不会被用于最终结果的计算。惰性批次有助于让模拟达到稳态条件，减少初始条件对结果的影响。</li>
<li>活跃批次：在惰性批次之后，模拟进入活跃阶段，此时每个批次的结果将被用于最终的统计分析。</li>
<li>批次依赖：尽管每个批次的模拟是独立的，但模拟的统计结果依赖于所有批次的累积。在模拟结束后，所有批次的结果会被合并以产生最终的统计数据。</li>
<li>状态点文件：在每个批次结束时，openmc
可以保存一个状态点文件，该文件包含了当前批次结束时的模拟状态。这些状态点文件可以用于分析、验证或作为未来模拟的起点。</li>
<li>重启模拟：如果需要，可以使用之前保存的状态点文件来重启模拟，从特定的批次继续进行模拟计算。</li>
<li>并行性：openmc
支持并行计算，可以同时运行多个批次。这种并行性是通过在多个处理器或计算节点上分配不同的批次来实现的，从而加快模拟的总体速度。</li>
</ul>
]]></content>
      <categories>
        <category>软件学习</category>
      </categories>
      <tags>
        <tag>反应堆物理</tag>
        <tag>Openmc</tag>
      </tags>
  </entry>
  <entry>
    <title>练琴日常</title>
    <url>/2024/04/30/Piano/</url>
    <content><![CDATA[<h1 id="艰难足迹">艰难足迹</h1>
<ol type="1">
<li>小汤1，已结束</li>
<li>天空之城，已会弹</li>
<li>小汤2， 已结束</li>
<li>拜厄，艰难进行中……</li>
<li>三和弦节奏与转位练习，艰难进行中……</li>
</ol>
<span id="more"></span>
<h1 id="前行路线">前行路线</h1>
<h2 id="天空之城弹奏">天空之城弹奏</h2>
<p><strong>目标：</strong> 《天空之城》</p>
<p>希望合手不要帕金森~</p>
<ul>
<li><strong>庆祝：</strong> 2024.05.05 五一假期终于成功了</li>
</ul>
<h2 id="小汤2">小汤2</h2>
<ul>
<li><p>时间点：2024.05.18 刚到一半，遥遥无期哦</p></li>
<li><p>时间点：2024.06.20
在我的不屑努力下，终于磕磕巴巴的弹完了小汤2</p></li>
</ul>
<h2 id="遇见">《遇见》</h2>
<p>感觉遇见还是挺简单的，争取这个月拿下吧。</p>
<ul>
<li>时间点：2024.06.21 搁浅中，我可能还不配弹这个</li>
</ul>
<h2 id="三和弦转位练习">三和弦转位练习</h2>
<ul>
<li>时间点：2024.06.21 转位形式不太熟，还不能随便转位，呜呜呜……</li>
</ul>
<h2 id="和弦走向">6415和弦走向</h2>
<ul>
<li>里程碑： 2024.06.10 会简单地弹唱《晴天》</li>
</ul>
<h1 id="即兴伴奏">即兴伴奏</h1>
<h2 id="三和弦">三和弦</h2>
<ul>
<li><strong>大三度</strong>：两个全音相加为大三度</li>
<li><strong>小三度</strong>：一个全音加一个半音</li>
<li><strong>大三和弦</strong>：根音和三音是大三度，三音和五音是小三度</li>
<li><strong>小三和弦</strong>：根音和三音是小三度，三音和五音是大三度</li>
<li><strong>减三和弦</strong>：根音和三音是小三度，三音和五音是小三度</li>
<li><strong>增三和弦</strong>：根音和三音是大三度，三音和五音是大三度，听起来有种扩张的感觉</li>
</ul>
<p><strong>Tips</strong></p>
<ol type="1">
<li>C大调中一共有7个基本三和弦 1 2m 3m 4 5 6m 7dim</li>
</ol>
<h2 id="七和弦">七和弦</h2>
<h3 id="大七度和小七度">大七度和小七度</h3>
<ul>
<li><strong>大七度</strong>：两音之间只包含一个半音就是大七度。比如:do到si七度两音之间只包含一个半音，所以是大七度，re到升do七度两音之间只包含一个半音，所以是大七度。</li>
<li><strong>小七度</strong>：小七度:两音之间包含两个半音就是小七度。比如:do
降si七度两音之间包含两个半音，所以是小七度，re到do七度两音之间包含两个半音，所以是小七度。听起来紧张一点</li>
</ul>
<p><strong>Tips</strong></p>
<ol type="1">
<li>C大调中只有do-xi和fa-mi是大七度，其余都是小七度</li>
<li>小二度的转位就是大七度</li>
</ol>
<h3 id="五种七和弦">五种七和弦</h3>
<ul>
<li><p><strong>大七和弦（大大七和弦）</strong>：根音和三音是大三度、三音和五音是小三度、五音和七音是大三度。根音和七音是大七度。记和弦口诀，大小大，故叫大七。和弦标记是
<span class="math inline">\({ {\rm C}_{maj7}}\)</span></p></li>
<li><p><strong>属七和弦（大小七和弦）</strong>：根音和三音是大三度，三音和五音是小三度，五音和七音也是小三度。根音和七音是小七度。记和弦口诀，大小小，故叫大小七。和弦标记是用根音音名加个7表示。</p></li>
<li><p><strong>小七和弦（小小七和弦）</strong>：根音和三音是小三度，三音和五音是大三度，五音和七音是小三度，根音和七音是小七度。记和弦口诀:小大小，故叫小小七和弦。和弦标记是用根音音名加个m7表示。</p></li>
<li><p><strong>半减七和弦（减小七和弦、小七降五）</strong>：半减七和弦:也就是减小七和弦，也叫小七降五。根音和三音是小三度，三音和五音是小三度五音和七音是大三度。根音和七音是小七度。记和弦口诀:小小大，故叫半减七和弦。和弦标记是用根音音名加个m7b5表示。</p></li>
<li><p><strong>减七和弦（减减七和弦）</strong>：根音和三音是小三度，三音和五音是小三度，五音和七音也是小三度。根音和七音是减七度。记和弦口诀:小小小，故叫减七和弦。和弦标记是用根音音名加个dim7表示。</p></li>
</ul>
<p><strong>Tips</strong></p>
<ol type="1">
<li>大三和弦基础上根音加大七度就是大七和弦，大三和弦基础上根音加小七度就是属七和弦</li>
<li>如何找大七和弦：纯八度，降一个半音就是大七和弦。如1351，1降半音就是大七度1357</li>
<li>如何找属七和弦：纯八度，降一个全音就是小七和弦。如1351，1降全音就是大七度135降7</li>
<li>属七和弦一般都需要被解决，这个东西用来转调非常好用</li>
<li>如何找小七和弦：属七和弦基础上三音降半音就是小七和弦</li>
<li>C大调中<br />
</li>
</ol>
<ul>
<li>大七和弦为： 1357, 4613</li>
<li>小七和弦为：2461, 3572, 6135</li>
<li>属七和弦为：5724</li>
<li>半减七和弦为：7246</li>
</ul>
<div style="text-align: right; font-style: italic;">
<p>发布日期：<span style="font-weight: bold;">2024年05月18日</span></p>
</div>
]]></content>
      <categories>
        <category>生活日常</category>
      </categories>
      <tags>
        <tag>Piano</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World! 一个建于21世纪的小站，存活于互联网边缘~</title>
    <url>/2024/04/29/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a
href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a
href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<span id="more"></span>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
</search>
